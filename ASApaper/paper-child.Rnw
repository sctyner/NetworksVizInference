\newcommand{\st}[1]{{\color{orange} #1}}
\newcommand{\hh}[1]{{\color{magenta} #1}}

<<setup-ch-2, echo = FALSE, message = FALSE, warning = FALSE>>=
options(replace.assign=TRUE,width=70, digits=3)
require(knitr)
opts_chunk$set(fig.path='figure/', cache.path='cache/', fig.align='center', fig.pos='h', out.width='.99\\textwidth', par=TRUE, cache=FALSE, concordance=TRUE, autodep=TRUE, message=F, warning=F, echo = FALSE, dev="cairo_pdf", fig.width = 6, fig.height = 6)#, root.dir = "~/Desktop/Dissertation/SAOM-removing-blindfold/")
@

<<pkgs>>=
library(tidyverse)
library(xtable)
library(RSienaTest)
library(RSiena)
library(geomnet)
library(GGally)
library(vinference)
library(lme4)
library(extrafont)
loadfonts(quiet = T)
load("../Data/senate/senateSienaNoHRC.rda")
ThemeNoNet <- theme_bw() %+replace% 
            theme(plot.title = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0, 
                                            family="Times New Roman"), 
                  axis.title.x = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0,
                                            family="Times New Roman"),
                  axis.title.y = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 90,
                                            family="Times New Roman"),
                  axis.text.x.top = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0,
                                            family="Times New Roman"), 
                  axis.text.x.bottom = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0,
                                            family="Times New Roman"), 
                  axis.text.y.left = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0,
                                            family="Times New Roman"),
                  axis.text.y.right = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0,
                                            family="Times New Roman"),
                  strip.text.x = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0, 
                                            family="Times New Roman",
                                            margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.text.y =element_text(size = 12,
                                            face = 'plain', 
                                            angle = 90,
                                            family="Times New Roman",
                                            margin = margin(t = 3, r = 3, b = 3, l = 3, unit = "pt")),
                  strip.background = element_rect(colour = "black", fill = "white")
                    )
ThemeNet <- theme_net() %+replace% 
            theme(plot.title = element_text(size = 12,
                                            face = 'plain', angle = 0, family = "Times New Roman"), 
                  strip.text.x = element_text(size = 12,
                                            face = 'plain', angle = 0, family = "Times New Roman", 
                                            margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.text.y =element_text(size = 12,
                                            face = 'plain', angle = 0, family = "Times New Roman",margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  panel.border = element_rect(fill = NA, color = 'black'),
                  strip.background = element_rect(colour = "black", fill = "white"))
@

\section{Introduction}
% \st{
% \begin{enumerate}
% \item significance testing: what is it and why is it important
% \item goodness-of-fit testing: what is it and why is it important
% \item difficulty of these two for sna, why do vis inf
% \end{enumerate}
% }
\st{Three} of the most important pieces of statistical modeling are significance testing of model parameters, goodness-of-fit tests, and power of a test. In the first, the data are usually assumed to come from a simple model under the null hypothesis, and additional parameters are tested whether they significantly contribute to explaining variability in the data. In the second, the model of interest is examined to determine how well it fits the data. \st{In the final, the ability of the  hypothesis test in question to detect the difference between the null and alternative hypothesis is determined.}  All three of these aspects of statistical modeling increase greatly in difficulty as the complexity of the model of interest increases. 
%The more complicated the model, the harder it is to determine fit or to include or exclude a parameter. 

Some particularly complicated sets of models are those designed to model network change. A \textit{network} is any set of things, such as people, computers, or neurons, that are connected in some way, through social relations, internet connection, or electrical impulses in the brain. We refer to the ``things" in the network as \textit{nodes}, or \textit{actors} in a social network, and the connections as \textit{edges}, or \textit{ties} in a social network. 
Dependencies inherent to the data make network objects particularly difficult to model. Even more challenging is the situation when we go beyond single instances of a network and consider the dynamics of network change between observed instances. This type of modelling for dynamic networks is often performed on social network data, such as friendship networks among students or the spread of HIV in \st{drug users sharing needles.} %sexually active communities.% of men sleeping with other men. X ST a bit less jarring hopefully. X 
These models lack the asymptotics required to perform many well-known goodness-of-fit tests, and the maximum likelihood estimation of parameters is so difficult that it can make significance testing difficult as well \citep{goldenberg09}. 
\hh{is the previous sentence a quote? I'm not sure that I follow.} \st{it's a paraphrase... shall I make a quote? Also I definitely botched a word. Hopefully makes more sense now? }

We propose new methods for significance and goodness-of-fit testing and power calculation of these for a set of social network models, stochastic actor-oriented models for dynamic network data \citep{saompaper}. Specifically, we are using \textit{visual inference} in place of traditional statistical methods for social network models, such as Wald tests for significance of parameters and in- and outdegree distribution metrics for determining goodness-of-fit. Visual inference, introduced by \citet{Bujaetal}, allows us to look at the entire dataset simulated from a network model as opposed to a (set of) usually one-dimensional metric(s) derived from the network such as outdegree, or a $p$-value for a single parameter in the model. %In addition, we use visual inference to better understand some parameters in a social network model, by looking at how different values for parameters affect the networks simulated from these models.   

The paper is outlined as follows: Section~\ref{vi} gives a basic overview of visual inference and the lineup protocol. Section~\ref{sec:saom} provides an introduction to the our models of interest, stochastic actor-oriented models. Section~\ref{sec:expersetup} details how we define significance testing and goodness of fit procedures for SAOMs through visual inference, and Section~\ref{sec:res} details the results of a visual inference survey of Amazon Mechanical Turk workers. We close with a discussion in Section~\ref{sec:concl}.  

\hh{where do the results come in?}\st{whoops, forgot to updat this with the reorg. }

\section{Visual Inference}\label{vi}

Data visualizations are an important component of data analysis, providing a mechanism for discovering patterns in data. Pioneering research by \citet{gelman:2004}, \citet{Bujaetal} and \citet{majumder:2011} provide methods to quantify the significance of discoveries made from visualizations. \citet{Bujaetal} introduced two protocols, the Rorschach and the lineup protocol, which bridge the gulf between traditional statistical inference and exploratory data analysis. %The Rorschach protocol consists of a set of $m$ (usually, $m=20$) plots (called the {\it null plots}) rendered from data that is consistent with a given null model. The Rorschach protocol helps to understand the extent of randomness in the null model. 
Here, we use the lineup protocol. Under this protocol, a plot of the observed data is placed randomly among a set of $m-1$ null plots (where $m =20$, usually), and human observers are then asked to examine the lineup and to identify the most different plot. If an observer identifies the data plot, this is quantifiable evidence against the null hypothesis. \hh{Since an observer has a chance of 1 in $m$ to pick the data plot from the lineup by simply guessing, i.e. in a situation where the data plot is virtually indistinguishable from the null plots,} \st{the evidence grows in strength with the number of independent observers identifying the data plot.}

The lineup protocol places a plot firmly in the framework of hypothesis tests: a plot of the data is considered to be the test statistic, which is compared against the sampling distribution under the null hypothesis represented by the null plots. Obviously, the null generating mechanism, i.e.\ the method of obtaining the data for null plots, is crucial for both the lineup and the Rorschach protocol, as the null hypothesis directly affects the choice of null generating method. Null generating methods are typically based on (a) simulation, if the null hypothesis allows us to directly specify a parametric model, (b) sampling, as for example in the case of large data sets, or (c) permutation of the original data \citep[see e.g.\ ][]{Good05}, which allows for non-parametric testing that preserves marginal distributions  while ensuring independence in higher dimensions. The model of interest here allows us to simulate directly from a parameteric model for dynamic social network data. 
%In the experimental data that we analyzed the null generating methods used were permutation methods and direct simulation from a null model.

The lineup protocol was formally tested in a head-to-head comparison with the equivalent conventional test in \citet{majumder:2011}. The experiment utilized human subjects from Amazon's Mechanical Turk \citep{turk} and used simulation to control conditions. The results suggest that  visual inference is comparable to conventional tests in a controlled conventional setting. This provides support for its appropriateness for testing in real exploratory situations where no conventional test exists. %Interestingly, the power of a visual test increases with the number of observers engaged to evaluate lineups, and the pattern in results suggests that the power will provide results consistent with practical significance \citep{kirk:1996}.

\section{Stochastic Actor-Oriented Models}\label{sec:saom}

%To model family that we apply to the Senate data is the Stochastic Actor-Oriented Model (SAOM) family. 
Stochastic Actor-Oriented Models (SAOMs) are a family of models for dynamic network data \citep{saompaper} that incorporate both network structure and node-level information to describe how a network observed on two or more occasions changes over time. The two titular properties of SAOMs, stochasticity and actor-orientation, are crucial to understanding networks as they exist naturally: social networks are ever-changing as relationships decay or grow in seemingly random ways, and most actors in them have characteristics that could affect how they change their ties to other nodes in the network. 
% Talk about SAOMs
These unique properties allow for the fitting of some very complicated models to inherently complex data, so it can be exceedingly difficult to interpret parameters and their corresponding estimates. The sheer amount of possible parameters to include in the model combined with the difficulty of interpretation make parameter selection and goodness-of-fit testing burdensome as well.

Broadly, a SAOM takes network structure and node covariate information into account in two ways and models the network changes as a continuous time Markov chain (CTMC). First, the rate of change between states is dictated by a rate function that describes \textit{how often} changes in the network occur, and secondly, the objective function describes \textit{what} those state changes are. As in many other network models, the variables of interest,  are the binary edges of the network. Let $x_{ij}$ denote the edge between nodes $i$ and $j$, where $i,j \in \{1, 2, \dots, n=\text{the number of nodes}\}$. 
$x_{ij}$ is modelled as a binary variable, i.e.
  \begin{equation}\label{eq:edgevars}
  x_{ij} =
  \begin{cases}
                                   1 & \text{if an edge from $i$ to $j$ exists} \\
                                   0 & \text{otherwise}
  \end{cases}
  \end{equation}
Edges are treated as \textit{directed}, i.e. in general $x_{ij} \neq x_{ji}$, and self-referencing edges or loops are not allowed, i.e.  $x_{ii} = 0$ for all $i$. 
Assume, the network is observed $M$ times at time points  $t_1 < t_2 < ... < t_M$, then the  entire network at time point $t_m$ is denoted as $x(t_m)$. In sections~\ref{rf} and~\ref{of} we discuss the rate and objective functions of a SOAM in more depth.  Additional details on SOAMs can be found in \citet{saompaper, snijders01, snijders2010, snijdersetall:2007, snijders:2010, snijders:2017},

\subsection{Rate Function}\label{rf}

All changes in SAOMs are treated as changes made by the nodes, or \textit{actors}, in the network, i.e.\ each actor, $i$, gets a chance to make a change according to the rate function, typically denoted $\lambda_i$, which dictates when relationships between nodes in the network can change. In general, the rate function can take the network structure e.g. outdegree of node $i$, and the node covariates into account, but we use the simple rate function, which is constant over all nodes in a given time period. We denote the rate from $t_{m}$ to $t_{m+1}$ as $\alpha_m$ for $m = 1, \dots, M-1$. Using this notation, the waiting time to the next chance for actor $i$ to make a change is exponentially distributed with expected value $\alpha_m^{-1}$. Since the rate is the same for all actors, the waiting time for \textit{any} actor to get the chance to change is exponentially distributed with expected value $(n\alpha_m)^{-1}$.

\subsection{Objective Function}\label{of}

After actor $i$ has been given the opportunity to change, it probabilistically chooses one of its current ties, $x_{ij}$, to change. The probability that actor $i$ changes its current tie to actor $j$ is determined by the \textit{objective function} of the model and a random component, $U$, \st{which can be thought of as encompassing any other factors that may be influencing the changes the node makes not accounted for by the parameters in the model.} \hh{XXX can you paraphrase the purpose of U in half a sentence?}
%The random component is assumed to have log-Weibull distribution with location parameter $\mu = 0$ and scale parameter $\sigma = 0$ (see \citet{modelsSnijders}), so the probability density function of $U$ is
%\begin{equation}\label{eq:logweibull}
%g(u) = \exp\{-(u + \exp\{-u\})\}.
%\end{equation}
%The objective function, $f_i$, is the driving force of a SAOM. 
Actor $i$ is aiming to maximize the objective function $f_i$ given the current state of the network, $x$ and the node-level covariates, $\mathbf{Z}$, given as:% which has the form
\begin{equation}\label{eq:objective}
f_i(x, \boldsymbol{\beta}, \mathbf{Z}) = \sum_{k = 1}^K \beta_k s_{ik}(x, \mathbf{Z}),
\end{equation}
where $\boldsymbol{\beta} = (\beta_1, \dots, \beta_K)$ are additional model parameters, each associated with some network statistics, $s_{ik}(x, \mathbf{Z})$, $s_{ik}(x, \mathbf{Z})$, calculated with respect to actor $i$. Network statistics range from the simple outdegree, $s_i(x) = \sum_{i\neq j} x_{ij}$, to the more complicated {\it transitive triplets jumping to different covariate}, $s_i(x, \mathbf{Z}) = \sum_{i \neq j \neq h} x_{ij}x_{ih}x_{hj} \cdot \mathbb{I}(z_i = z_h \neq z_j)$. %, plus many more. At last count, in the software we use to fit these models to network data, 
Version 1.2-3 of \texttt{RSiena} \citep{RSiena}, the software used to fit the models here, provides  over 80 possible effects that can be included in the objective function. We discuss these statistics in more detail in Section~\ref{sec:models}. 

Objective function $f_{i}(x, \boldsymbol{\beta}, \mathbf{Z})$ and  random component $U$ are combined to form the \textit{transition probability}, $p_{ij}$, of the network changing from its current state $x$ to the state with changed tie $x_{ij}$, denoted as $x(i \leadsto j)$:
\begin{equation}\label{eq:transprob}
p_{ij} = \dfrac{\exp\{f_i(x(i \leadsto j), \boldsymbol{\beta}, \mathbf{Z})\}}{\sum_{h} \exp\{f_i(x(i \leadsto h), \boldsymbol{\beta}, \mathbf{Z})\}}
\end{equation}

This probability dictates which edge change is made by the acting node. The acting node can also choose to \textit{not} change at all. This occurs when the numerator, as calculated for the current state of the network, is larger than for any changes $x(i \leadsto j)$ that could be made.  

\hh{XXX how is the probability computed in the case that no change is made? Why is the notation $j \equiv i$ used for no change?}\st{that's more for the computation. I'll adjust accordingly}

According to \citet{RSienamanual}, at least two parameters must be included in the objective function: the density and the reciprocity. We denote the density, or out-degree, parameter by $\beta_1$ and the associated statistic as $s_{i1}(x) = \sum_{j} x_{ij}$. Similarly, we denote the reciprocity parameter by $\beta_2$ and the associated statistic as $s_{i2}(x) = \sum_{j} x_{ij}x_{ji}$. We refer to the model with only these two parameters in the objective function as M1. 


\subsection{Example Data}\label{sec:senatedata}

The data we use are collaboration networks in the United States Senate during the $111^{th}$ through $114^{th}$ Congresses, overlapping with Barack Obama's presidency. These senates began on January 6, 2009 and ended on January 3, 2017\footnote{Details of how this data can be downloaded are provided by FranÃ§ois Briatte at \url{https://github.com/briatte/congress}}. There are three legislative ways that senators can show support for legislation: they can author a bill, cosponsor a bill, and vote for a bill. We use cosponsorship as a metric because it results in a network that is unimodal (all nodes are senators) and directed. In this network, ties are directed from senator $i$ to senator $j$ when senator $i$ signs on as a cosponsor to the bill that senator $j$ authored. There are many hundreds of ties between senators when they are connected in this way, so we simplify the network by computing a single value for each senator-senator collaboration called the \textit{weighted propensity to cosponsor} (WPC). This value is defined in \citet{senate} as 

\begin{equation}\label{eq:sen1}
    WPC_{ij} = \dfrac{\sum\limits_{k=1}^{n_j} \frac{Y_{ij(k)}}{c_{j(k)}}}{\sum\limits_{k=1}^{n_j} \frac{1}{c_{j(k)}}}
\end{equation}

where $n_j$ is the number of bills in a congressional session authored by senator $j$, $c_{j(k)}$ is the number of cosponsors on senator $j$'s $k^{th}$ bill, where $k \in \{1,\dots, n_j\}$, and $Y_{ij(k)}$ is a binary variable that is 1 if senator $i$ cosponsored senator $j$'s $k^{th}$ bill, and is 0 otherwise. This measure ranges in value from 0 to 1, where $WPC_{ij} = 1$ if senator $i$ is a cosponsor on every one of senator $j$'s bills and $WPC_{ij} = 0$ if senator $i$ is never a cosponsor any of senator $j$'s bills. Because SAOMs require binary edges, we construct the edges as follows: 
 \begin{equation}\label{eq:edgewpc}
  x_{ij} =
\begin{cases}
                                   1 & WPC_{ij} > 0.25 \\
                                   0 & WPC_{ij} \leq 0.25
\end{cases}
\end{equation}
For each of the four senate sessions, we have the WPC value between any two senators in the session, the party affiliation of each senator, the number of bills they authored in each session, and their gender. We explored each of these covariates in the model to determine if they affect the overall network structure and how ties are formed between senators. The node-link diagram representations of the data we use for modelling are shown in Figure~\ref{fig:senateAll}. We have labelled some of the nodes in these networks whose names will be familiar to US readers, because they are leaders in their party or they have run for president. The size of the nodes represent how many bills the senator authored in a session, the color represents party affiliation, and the shape represent gender. In each of the four sessions, there is one very large connected component tying many of the prominent senators together, with many smaller groups of two to ten senators surrounding the larger component. In each senate, the structure changes slightly as new senators arrive or come to prominence.

<<senateAll, fig.cap="The four senate collaboration networks that we use as our example data to visually assess the SAOM effects. Color represents party, shape represents gender, and size represents number of bills authored in a session. The Frucherman-Reingold layout is shown. ">>=
seobama <- read_csv("data/senateobamapres_gsw_25.csv")
to_label <- c("Joseph R. Biden Jr.", "John S. McCain", "Ted Cruz","Marco Rubio",
  "Lindsey O. Graham","Rand Paul", "Bernard Sanders", "Jim Webb", "Mitch McConnell",
  "Harry M. Reid", "Hillary Rodham Clinton", "Amy Jean Klobuchar", "Elizabeth Warren")
seobama$label <- as.factor(ifelse(seobama$source %in% to_label, seobama$source, ""))
levels(seobama$label) <- c("", "Amy Klobuchar", "Bernie Sanders", "Elizabeth Warren",
                           "Harry Reid", "Hillary Clinton", "Jim Webb", "John McCain", 
                           "Joe Biden","Lindsey Graham", "Marco Rubio", "Mitch McConnell", 
                           "Rand Paul", "Ted Cruz")
se111clint <- filter(seobama, senate == 111)
se111noclint <- filter(seobama, senate == 111)
se111noclint$target[which(se111noclint$target == "Hillary Rodham Clinton")] <- NA
seobama2 <- seobama %>% filter(senate != 111) %>% bind_rows(se111noclint)

set.seed(56049382)
ggplot(data = seobama2) + 
  geom_net(directed = T, labelon=T, arrowsize = .25, singletons= T, fiteach = T, linewidth = .25, layout.alg = 'fruchtermanreingold', fontsize = 2, repel=T, 
           aes(from_id = source, to_id = target, color = party, 
               label = label,shape = sex,size = n_au)) + 
  ThemeNet + 
  scale_color_manual(values = c("royalblue", "forestgreen","firebrick")) + 
  scale_shape_manual(values = c(17,16)) + 
  scale_size_continuous(name = "Bills\nauthored", range = c(.75, 3)) + 
  theme(legend.position = 'bottom', strip.text = element_text(size = 8)) + 
  facet_wrap(~senate, nrow = 2, labeller = "label_both") + 
  xlim(c(0,1.05)) + 
  ylim(c(0,1.05))
@

For Senate 111, for instance, we see Hillary Clinton, serving out her second term in the senate until she became Secretary of State. She is isolated in Figure~\ref{fig:senateAll}, but in actuality, she had many cosponsors on two pieces of legislation she authored in that short time, as is shown in Figure~\ref{fig:senateClinton}. We chose to remove Clinton and her edges from the network because they make the overall structure look so different from the other three senates, showing that the pattern is not typical of a senate in any other year. We suspect that because Hillary Clinton had just been appointed Secretary of State, the cosponsorships were largely symbolic, so the $111^{th}$ Senate without Hillary Clinton is more typical than the $111^{th}$ Senate with her. 

<<senateClinton, fig.cap="We removed Hillary Clinton's ties from the network because she had abnormally high collaboration with senators during the time she was in the 111th senate and before she left office to become Secretary of State.", fig.height=3>>=
se111clint$Clinton <- 'Yes'
se111noclint$Clinton <- 'No'
clintonSenate <- rbind(se111clint, se111noclint)
clintonSenate$Clinton <- as.factor(clintonSenate$Clinton)
clintonSenate$Clinton <- ordered(clintonSenate$Clinton, levels = c("Yes", "No"))
clintonSenate %>% filter(!(source %in% c("Roland Burris", "Bernard Sanders", "Frank R. Lautenberg", "Mary L. Landrieu") & is.na(target))) %>%
ggplot() + 
  geom_net(directed = T, labelon=F, arrowsize = .3, singletons= F, fiteach = T, arrowgap = .01, layout.alg = 'fruchtermanreingold',
           aes(from_id = source, to_id = target, color = party, linewidth = gsw, shape = sex,size = n_au)) + 
  scale_size_continuous(name = "Bills\nauthored", range = c(.75, 3)) + 
  ThemeNet + 
  scale_shape_manual(values = c(17,16)) + 
  scale_color_manual(values = c("royalblue", "forestgreen","firebrick")) + 
  theme(legend.position = 'bottom') + 
  facet_wrap(~Clinton, nrow = 1, labeller = 'label_both')
@

In legislative cosponsorship networks, it is well known that party affiliation and reciprocity of relationships are major influences on structure \citep{legnet}. We focus on these two covariates when choosing which SAO models to fit to the data. 

\subsection{Models of Interest}\label{sec:models}

In addition to considering already well-known effects in legislative networks for application of our significance and goodness-of-fit methods, we first fit many other possible models and selected a few significant effects. To determine the effects that we would move forward with, we followed this procedure: 

\begin{enumerate}
\item Define the simple effects structure of the data: the rate parameters and the outdegree and reciprocity parameters. 
\item Add each additional possible evaluation effect in \texttt{RSiena} one-at-a-time to the model structure, as determined by the effects documentation function \citep{RSiena}.
\item Fit each model to the data and check for convergence.
    \begin{enumerate}
    \item If the model converged, move to 4.
    \item If the model did not converge, use the previous fitted values as starting values and repeat 5 times or until convergence, whichever comes first.
    \end{enumerate}
\item Test the added parameter for significance using a Wald-type test.
\item Report out the estimate of the additional parameter, its standard error, Wald $p$ value, and convergence criterion.
\end{enumerate}
 
After completing the procedure for all model effects, we selected effects whose estimates converged, had a Wald $p$-value of less than 0.10, and seemed to have a reasonable interpretation for our data according to well-known properties of legislative networks \citep{legnet}.

The parameters we use for the remainder of the paper are detailed in Table~\ref{tab:effects}.  The most significant effect was the jumping transitive triplet (JTT) parameter for the party covariate, which was estimated to be about -6 with a standard error of 0.11, resulting in a $p$-value of less than 0.0001. This estimate of the parameter associated with this statistic relies on the number of transitive closures formed between two senators from different parties. The negative estimate is an indication that forming transitive ties between two people from different parties is discouraged, which tracks with the divisive nature of American politics, where party affilitation is dominant. Another significant effect was the same JTT parameter for the sex covariate, with an estimate of about 3 with a standard error of 0.89. The covariate-related similarity score-weighted transitive triplets parameter estimate for the number of bills authored by a senator was also significant. This effect was estimated at about 10 with standard error of 3.9, and the high positive effect suggests senators tend to collaborate with other senators who author about the same number of bills they do. This tendency of senators to cosponsor bills written by senators who are similarly ``prolific" corresponds to another well-known property of the U.S. Senate structure: the tendency of senators to be either ``workhorses" or ``showhorses". Senators known as workhorses author many pieces of legislation in a session, and largley stay out of the public arena. The showhorse senators, on the other hand, author relatively few pieces of legislation, and tend to appear on television, radio, and other media a great deal. Finally, we found the same party transitive triplet effect was also significant, with a fitted value of 1.3 and standard error of 0.7, meaning that transitive relationships between senators tend to form when they are from the same party. 

<<geteffects, eval = FALSE>>=
# used for table below
initeff <- read_csv("data/sigEffsSenate912.csv")
initeff %>% filter(shortName == "sameX")
#density & -- & $\sum_j x_{ij}$ & & & \\
#reciprocity & -- & $\sum_j x_{ij}x_{ji}$ & & & \\
@

\begin{table}
\scalebox{0.8}{
\begin{tabular}{c|p{2cm}|p{2.1cm}|c|l|p{1.75cm}|p{2cm}}
$\beta_k$ & {\bf Effect name} & {\bf Interaction Variable} & {\bf Formula} & {\bf Picture} & {\bf Initial estimate} & {\bf Wald $p$-value} \\
\hline 
$\beta_3$ & jumping transitive triplet & party & $s_{i3}(x, \mathbf{p}) = \sum_{j\neq h} x_{ij}x_{ih}x_{hj}\cdot \mathbb{I}(p_i = p_h \neq p_j)$ & \includegraphics[width=.6in]{img/jttp.png} & -5.884 & $<0.0001$\\
$\beta_4$ & jumping transitive triplet & sex & $s_{i4}(x, \mathbf{s}) = \sum_{j\neq h} x_{ij}x_{ih}x_{hj}\cdot \mathbb{I}(s_i = s_h \neq s_j)$ & \includegraphics[width=.6in]{img/jtts.png}  & 3.335 & 0.0002 \\
$\beta_5$ & similarity transitive triplet & bills & $s_{i5}(x, \mathbf{b}) = \sum_{j} x_{ij}x_{ih}x_{hj}\cdot (sim^b_{ij} - \overline{sim}^b)^*$ & \includegraphics[width=1in]{img/simttb.png} & 9.821 & 0.0128 \\
$\beta_6$ & same transtive triplet & party & $s_{i6}(x, \mathbf{p}) =\sum_{j} x_{ij}x_{ih}x_{hj}\cdot \mathbb{I}(p_i = p_j)$ & \includegraphics[width=1in]{img/samettp.png}  & 1.306 & 0.0642 %\\
%$\beta_7$ & same & party & $s_{i7}(x, \mathbf{p}) = \sum_j x_{ij}\mathbb{I}(p_i = p_j)$ & \includegraphics[width=.6in]{img/samep.png}  & 0.363 & 0.0074
\end{tabular}
}
\caption{\label{tab:effects} The additional effects we used in the SAOMs fit to the senate data. * - $sim^b_{ij} = \frac{\max_{hk}|b_h - b_k| - |b_i - b_j|}{\max_{hk}|b_h - b_k|}$ is the similarity score between two senators based on the number of bills authored, and $\overline{sim}^b = \frac{1}{n(n-1)}\sum_{i\neq j} sim^b_{ij}$ is the average bill similarity score between any two senators.}
\end{table}

We examine a total of six models, each identified by its objective function: 
\begin{enumerate}
\item Model M1: $f_{i}(x, \boldsymbol{\beta}) = \beta_1 s_{i1}(x) + \beta_2 s_{i2}(x)$
\item Model M2: $f_{i}(x, \boldsymbol{\beta}, \mathbf{p}) = \beta_1 s_{i1}(x) + \beta_2 s_{i2}(x) + \beta_3 s_{i3}(x, \mathbf{p})$
\item Model M3: $f_{i}(x, \boldsymbol{\beta}, \mathbf{s}) = \beta_1 s_{i1}(x) + \beta_2 s_{i2}(x) + \beta_4 s_{i4}(x, \mathbf{s})$
\item Model M4: $f_{i}(x, \boldsymbol{\beta}, \mathbf{b}) = \beta_1 s_{i1}(x) + \beta_2 s_{i2}(x) + \beta_5 s_{i5}(x, \mathbf{b})$
\item Model M5: $f_{i}(x, \boldsymbol{\beta}, \mathbf{p}) = \beta_1 s_{i1}(x) + \beta_2 s_{i2}(x) + \beta_6 s_{i6}(x, \mathbf{p})$
\item Model M6: $f_{i}(x, \boldsymbol{\beta}, \mathbf{p}, \mathbf{b}, \mathbf{s}) = \beta_1 s_{i1}(x) + \beta_2 s_{i2}(x) + \beta_4 s_{i4}(x, \mathbf{s}) + \beta_5 s_{i5}(x, \mathbf{b}) + \beta_6 s_{i6}(x, \mathbf{p})$
\end{enumerate}

We fit models M1 through M6 in \texttt{RSiena} using Markov Chain Monte Carlo (MCMC) methods to approximate the method of moments estimates of the parameters. Because the estimation is done through MCMC simulation, we fit each model to the data 1,000 times to get a better estimate of the true value of $\boldsymbol{\beta}$. From the simulations that converged, which made up over 90\% of the fits for each model, we computed the mean of the 1,000 estimates of each parameter to get final estimates of  $\hat{\boldsymbol{\beta}}$ for each model, which are given in Table~\ref{tab:fittedvalues}. 

<<modelestimates>>=
load("data/allModelMeans.RDS")
modelMeanEsts %>% unnest(ests) %>% mutate(param = "rate") -> modelmeans
modelmeans$param[c(1,6,12, 18,24, 30)] <- "alpha1"
modelmeans$param[c(1,6,12, 18,24, 30)+1] <- "alpha2"
modelmeans$param[c(1,6,12, 18,24, 30)+2] <- "alpha3"
modelmeans$param[c(1,6,12, 18,24, 30)+3] <- "beta1"
modelmeans$param[c(1,6,12, 18,24, 30)+4] <- "beta2"
modelmeans$param[modelmeans$param == "rate"] <- c("beta3", 'beta4', 'samep', 'beta6', 'beta5')
modelmeans %>% filter(model != "samep") %>% spread(param, ests) -> modelmeans
M6 <- c(2.4405048,2.4594403,2.2098176,-4.9232775,4.8916183,2.3743720,0.2047038,6.9661589)
@

\begin{table}
\centering
\begin{tabular}{|l|ccccccccc|}
\hline
Model & $\alpha_1$ & $\alpha_2$ & $\alpha_3$ & $\beta_1$ & $\beta_2$ & $\beta_3$ & $\beta_4$ & $\beta_5$ & $\beta_6$ \\
\hline
\hline
M1 & \Sexpr{round(modelmeans$alpha1[1], 3)} & \Sexpr{round(modelmeans$alpha2[1], 3)} & \Sexpr{round(modelmeans$alpha3[1], 3)} & \Sexpr{round(modelmeans$beta1[1], 3)} & \Sexpr{round(modelmeans$beta2[1], 3)} & -- & -- & -- & --\\
M2 & \Sexpr{round(modelmeans$alpha1[2], 3)} & \Sexpr{round(modelmeans$alpha2[2], 3)} & \Sexpr{round(modelmeans$alpha3[2], 3)} & \Sexpr{round(modelmeans$beta1[2], 3)} & \Sexpr{round(modelmeans$beta2[2], 3)} & \Sexpr{round(modelmeans$beta3[2], 3)} & -- & -- & --\\ 
M3 & \Sexpr{round(modelmeans$alpha1[3], 3)} & \Sexpr{round(modelmeans$alpha2[3], 3)} & \Sexpr{round(modelmeans$alpha3[3], 3)} & \Sexpr{round(modelmeans$beta1[3], 3)} & \Sexpr{round(modelmeans$beta2[3], 3)} & -- & \Sexpr{round(modelmeans$beta4[3], 3)} & -- & --\\
M4 & \Sexpr{round(modelmeans$alpha1[5], 3)} & \Sexpr{round(modelmeans$alpha2[5], 3)} & \Sexpr{round(modelmeans$alpha3[5], 3)} & \Sexpr{round(modelmeans$beta1[5], 3)}& \Sexpr{round(modelmeans$beta2[5], 3)} & -- & -- & \Sexpr{round(modelmeans$beta5[5], 3)} & -- \\
M5 & \Sexpr{round(modelmeans$alpha1[4], 3)} & \Sexpr{round(modelmeans$alpha2[4], 3)} & \Sexpr{round(modelmeans$alpha3[4], 3)} & \Sexpr{round(modelmeans$beta1[4], 3)} & \Sexpr{round(modelmeans$beta2[4], 3)} & -- & -- & -- & \Sexpr{round(modelmeans$beta6[4], 3)}\\
M6 & \Sexpr{round(M6[1], 3)} & \Sexpr{round(M6[2], 3)} & \Sexpr{round(M6[3], 3)} & \Sexpr{round(M6[4], 3)} & \Sexpr{round(M6[5], 3)} & -- & \Sexpr{round(M6[6], 3)} & \Sexpr{round(M6[8], 3)} & \Sexpr{round(M6[7], 3)} \\
\hline
\end{tabular}
\caption{\label{tab:fittedvalues} The final estimates from repeated estimation of models M1 through M6.}
\end{table}

We want to explore the role of each of these parameters in the objective functions for each model. So, we use the estimates given in Table~\ref{tab:fittedvalues} to simulate from models M1 through M6. We discuss the simulation procedure and how we use the simulations in Section~\ref{sec:expersetup}.  

\section{Experiment Set-Up}\label{sec:expersetup}

We want to explore three different aspects of the SAOM models using the lineup protocol: (1) significance of parameters, (2) goodness-of-fit of a model, and (3) visual detection of parameters. 
Each one of these situations requires a different setup, which we describe in detail, but we make ue of the lineup protocol for all of these aspects. 

In each lineup, we include plots from two models: the null model and an alternative model. The definition of the null and alternative model varies with the aspect of the SAOMs we are exploring. 
%For each lineup, we consider two models: model M1 is the null model, and another model, from M2-M5, which is chosen to be the alternative model. 

 Typically, a lineup shows sets of 20 plots at a time c.f.~\citet{loy:2015, vanderplas:2016}, but we determined that not enough structure could be shown in each plot for 20 node-link diagrams. We chose to expose our participants to only six plots at a time in order to show the node-link diagrams in more detail and to lower cognitive load for participants.
To construct a lineup, we simulate five networks from the null model and one network from the alternative model.  An example of a lineup like those shown to our participants is given on the right side of the image  in Figure~\ref{fig:shiny}.  In this lineup, model M4 is the alternative model, and model M1 is the null model. 

To simulate lineups from the models, we set the parameters to the values given in Table~\ref{tab:fittedvalues} for all parameters within the respective models, with the exception of $\beta_5$. For $\beta_5$, twice the estimated value was used. More detail on why we use twice the fitted value is provided in Section~\ref{sec:setupvp}.\footnote{If you would like to explore the kinds of lineups we use in further detail, please visit \url{https://sctyner.shinyapps.io/saom_lineup_creation/}}. To get the simulations, we used the \texttt{siena07} function in \texttt{RSiena} \citep{RSiena}. 

\begin{figure}
\includegraphics[width=.9\textwidth]{img/shinyappscreen.png}
\caption{\label{fig:shiny} A screen shot of the web application we created to design our lineup experiment. More details about this application are given in Section~\ref{sec:setupvp}. In the lineup, M4 is the alternative model with $\beta_5$ set to twice its estimated value given in Table~\ref{tab:fittedvalues}. One plot simulated from this model is placed at random among five observations simulated from the null model, M1. Participants of the study are asked to identify the most different plot.  %The other two tabs in the right side of the screen tell you which plot comes from the alternative model, and allow you to view and download the data that is plotted in the lineup.
}
\end{figure}

A series of these lineups is shown to independent observers recruited through Amazon Mechanical Turk for feedback (more details on the Turk setup in Section~\ref{sec:res}). 

\subsection{Significance Testing}\label{sec:setupst}

In the significance testing protocol, a parameter of interest is selected to test, say $\beta_k$. The hypotheses we use to generate lineups are: 

\begin{equation}\label{eq:sig}
H_0: \beta_k = 0 \ \ \ \text{ versus } \ \ \ H_A: \beta_k \neq 0
\end{equation}
%\begin{description}
%\item $H_0$: $\beta_k = 0 $
%\item $H_A$: $\beta_k \neq 0$
%\end{description}

Under the null hypothesis, we assume that the model that generated the network data is $M_1$, the simplest model presented in Section~\ref{sec:models}. Thus, the five null plots in the lineup are simulations from M1 with $\beta_1, \beta_2$ set to the estimates given in Table~\ref{tab:fittedvalues} for M1. The alternate model is the model with $\beta_1, \beta_2$, and $\beta_k$ in the objective function, with the remaining plot simulated from the appropriate model. 

The lineup generated under this scenario is shown to a number of independent viewers. If an observer picks out the alternative data plot, that is evidence in favor of the null hypothesis, while picking one of the null plots is evidence against the null hypothesis. The significance tests that we perform in our experiment are for $\beta_3$ and $\beta_4$, making the alternative models M2 and M3, respectively. 

\subsection{Goodness-of-Fit}

For the goodness-of-fit tests, we compare one model of interest, say M\textit{i} to the data. The hypothesis we use to generate lineups are 
\begin{description}
\item $H_0$: The data come from model M\textit{i}
\item $H_A$: The data come from some other, unknown model
\end{description}

To generate the null plots, we simulate five networks from model M\textit{i} using the corresponding parameters in Table~\ref{tab:fittedvalues}. We pick a wave to focus on, wave two, which is the first simulated network, and among these five plots, we place a node-link diagram of the true second wave data. We cannot show the data more than once to each participant, so we examine several different models in our Amazon Mechanical Turk experiment, each participant never seeing the true data wave twice. The models we chose for goodness-of-fit testing are M2, M3, M4, and M6. 

\subsection{Visual Power}\label{sec:setupvp}

%Visual detection does not fit into the typical statistical testing framework as easily as the significance and goodness-of-fit tests we perform do. 
Through visual inference, we want to determine at which point an effect becomes noticeable in a SAOM. By \textit{noticeable}, we mean that the inclusion of the effect alters the appearance of networks simulated from a model to a degree that viewers are able to reliably pick out a node-link diagram rendered from data simulated from a model with the effect from a lineup of plots without the effect. \st{This is a way to determine the power of the visual test.}  We explore all parameters in the objective function, $\beta_1, \dots, \beta_6$ in this way. 

In model M1, with only two parameters in the objective function, we varied both the density and reciprocity parameter values one at a time. In models M2 through M5, we vary the additional parameter, $\beta_3$ through $\beta_6$. Thus, we have six different parameters of interest to us: $\beta_1, \dots, \beta_6$. We want to determine how the size of these parameters affects the overall structure of the network data simulated from the models M1 through M5, so we also vary the value of the parameters in both negative and positive directions. 

%Because the traditional way to visualize a network is a node-link diagram, we will examine the effect of different parameter values using node-link visualizations. 

<<hypothesis, fig.height=2, fig.cap="We hypothesize that as the parameter value of interest increases in absolute value, more viewers of the lineup will pick the alternative data out of a lineup. Note that the significance test we construct in Section~\\ref{sec:setupst} is just one point on the line below, represented by the vertical red line.">>=
x <- seq(-10, 10, .05)
N <- length(x)
qplot(x = 1:N, y = pt(x, 1), geom = 'line') + geom_line(aes(x = -(1:N), pt(x,1))) + 
    scale_x_continuous(name = "Parameter Value", labels = c("very negative", "negative", "0", "positive", "very positive")) + 
  #geom_hline(yintercept = 1/6) +
    geom_vline(xintercept = 150, color = 'red', linetype = 'dotdash') + 
    scale_y_continuous(name = "% detecting plot from\nthe alternative model", labels = paste0(c(0,25,50,75,100), "%"))
@

To determine the threshold at which an effect becomes noticeable, we examine six different levels of the effect, three negative and three positive ones. Figure~\ref{fig:hypothesis} shows a sketch of what the detection probability by participants' looks like hypothetically with varying effect size: the higher in absolute value the parameter is, the more likely participants are to choose the alternative model out of the lineup. To determine the exact values of the six levels we want to test for each effect, we started with the estimates of the parameter at hand (see  Section~\ref{sec:models}), and used small negative and positive factors to determine at what point \textit{we} noticed the effect of the parameter in simulations from the changed models.  


For this we constructed an online application that created the lineup protocol for us to be the guinea pigs of our own experiment \citep{theqs}. A screen shot of the app we created with the \texttt{shiny} package is shown in Figure~\ref{fig:shiny} \citep{shiny}. On the left side of the screen, the user\footnote{Please visit \url{https://sctyner.shinyapps.io/saom_lineup_creation/} to create lineups constructed from the models we present for this data for yourself.} can input the information necessary for creating a lineup of the models M1 through M6 for the data in Section~\ref{sec:senatedata}: first, choose to simulate only one plot from the specified model (analogous to changing the alternative model in the lineup protocol) or to simulated $M-1$ plots from the specified model (analogous to changing the null model in the lineup protocol); the model of interest; the wave of the data to examine; if model M1 is selected, whether to alter the density or the reciprocity parameter; the size of the lineup; the amount by which to multiply the effect selected; a random seed for replicability; and a layout algorithm to use for the node-link diagrams. There is also a checkbox if the user wishes the nodes to be colored by the size of the connected component to which they belong. The plots that appear that are \textit{not} from the model specified using the other options are simulations from model M1 with the estimates of the rate parameters, $\beta_1$ and $\beta_2$ given in Table~\ref{tab:fittedvalues}. 

Using the ``Picking Lineups" web application, we settled on six parameter values to test for each of our six effects, $\beta_1, \dots \beta_6$. The complete details of the parameters tested using the lineup protocol is given in Table~\ref{tab:experdetail}. In the case of both $\beta_4$ and $\beta_6$, we could not determine any values for negative effects that made the data simulated from M3 and M5 look different than null model simulations from model M1. Therefore we decided that the lesser experienced participants in our experiment would also not be able to. Instead of testing the negative values of these effects, we are examining a different scenario: we placed 5 simulations from positive values of the parameter with one simulation from model M1 in a lineup. We refer to this later on as the ``reverse" lineup scenario. We used the reverse scenario to determine if the perception of the effect size is symmetric: if an effect is noticed $x$\% of the time at value $\beta_k = \beta_{k_0}$ when one simulation from the corresponding model is placed among five null plots from model M1, then when five simulations from the model with $\beta_k = \beta_{k_0}$ are put in a lineup with one simulation from model M1, the plot from the simpler model should be noticed about $x$\% of the time as well. 

<<getexperdetail, results='asis'>>=
turk22 <- read_csv("data/turk22-sig.csv")
modelData_sig <- turk22 %>% 
  mutate(type2 = ifelse(sign == 0, sign(initialEst), sign),
         type2 = ifelse(type == "one", type2, -1)) %>% 
  filter(pic_id != 3132) 
modelData_sig %>% group_by(test_param, type2, difficulty, size) %>%
  count() %>% ungroup() %>%
  spread(difficulty, size) -> step1
step1 %>% select(1:4) %>% filter(!is.na(`1`)) -> step2a
step1 %>% select(c(1:3,5)) %>% filter(!is.na(`2`)) -> step2b
step1 %>% select(c(1:3,6)) %>% filter(!is.na(`3`)) -> step2c
experdet <- step2a %>% mutate(easy = `1`, medium = step2b$`2`, hard = step2c$`3`) %>% select(-c(`1`, n))
experdet$test_param <- as.factor(experdet$test_param)
levels(experdet$test_param) <- paste0("beta", c(1,3,4,2,6,5))
experdet$test_param <- as.character(experdet$test_param)
names(experdet) <- c("Parameter", "Lineup Type", "Easy Value", "Medium Value", "Hard Value")

print(xtable::xtable(experdet, caption = "All conditions in our MTurk experiment. Note that for $\\beta_4$ and $\\beta_6$, the negative type experiments are the reverse lineups, where 5 plots were simulated from the model with the parameter value given in the table, while 1 plot was simulated from M1.", label = "tab:experdetail", digits = c(0,0,0,3,4,3)))
@

\section{Experiment Results}\label{sec:res}

We recruited 250 participants for our experiment through Amazon Mechanical Turk. Each participant was peresented with some brief training material before beginning the experiment. After agreeing to participate, the participants were shown two trial plots, one where the data plot was the most different from the others due to its complex structure, while the other trial included a data plot that was most different from the others due to its very simple structure. Only when participants were able to correctly identify the data plot from the trial lineups, they were allowed to begin the experiment. Each participant was randomly assigned 13 lineups to look at. The were asked to select one or more plots that they perceived as ``most different" from the others, and provide a reasoning for their choice. They could select from ``Most simple overall structure," ``Most complex overall structure," or ``Other" and provide their own text description of their reasoning. 
\st{Twelve of the 13 lineups that the participants saw were used for the significance testing and the visual power methods discussed in Sections~\ref{sec:setupst} and \ref{sec:setupvp}. The six parameters, $\beta_1, \dots, \beta_6$ were set to three different values according to how difficult we thought picking the data plot from the lineup would be for our participants, and were also set to be less than or greater than the initial estimate, creating the lineup type variable.}\hh{what about the first 12 plots? 6 parameters, 3 levels, 2 directions}
One of the 13 lineups the participants saw was the true data from the 112th senate shown in Section~\ref{sec:senatedata}. Each participant only saw the data one time in order to avoid bias. Upon completion of the 13 lineups, each participant was paid \$1.75.   

\subsection{Significance Testing}\label{sec:sigtest}

<<sigtestWald, eval = FALSE, echo = FALSE>>=
SenBasic <- getEffects(senateSiena)
Senjtt_p <- includeEffects(SenBasic, "jumpXTransTrip", include = TRUE, type = "eval", interaction1 = "party", character = TRUE)
myalg <- sienaAlgorithmCreate( projname = Sys.time() , n3 = 1000)
fits <- siena07(myalg, data = senateSiena, effects = Senjtt_p, returnDeps = TRUE,
                                      batch=TRUE, verbose = FALSE, silent = TRUE)
thet <- fits$theta
sig <- fits$covtheta
Wald.RSiena(A = c(0,0,1), fits)
th <- c(0,0,1) %*% thet
covmat <- c(0,0,1) %*% sig %*% c(0,0,1)
csq <- drop(th %*% solve(covmat) %*% th)
1 - pchisq(csq, 1)
@

For a SAOM, there are two ways a conventional significance test of the parameters can be performed. In \texttt{RSiena}, there are $t$-type tests and Wald-type test for a single parameter and for multiple parameters. The $t$-type test statistic is simply the parameter estimate divided by its standard error, and compared to a standard normal distribution. The Wald-type test statistic for a single parameter, $\beta_k$ is 
\begin{equation}\label{eq:wald1}
\frac{(\hat{\beta_k})^2}{var(\hat{\beta_k})} \sim \chi^2_1,
\end{equation}
which is compared to a Chi-square distribution with one degree of freedom. Testing the significance of multiple parameters depends on the hypothesis we wish to test, and a $P \times K$ matrix, $A$, must be appropriately designed to test the $P$ hypotheses of interest. The null hypothesis is that $A\boldsymbol{\beta} = \mathbf{0}$, and the test statistic is    
\begin{equation}\label{eq:wald2}
(A\boldsymbol{\hat{\beta}})' \hat{\Sigma}^{-1} A\boldsymbol{\hat{\beta}} \sim \chi^2_p,
\end{equation}
where $\hat{\Sigma}$ is the estimated covariance matrix of $\boldsymbol{\beta}$. This statistic is then compared to a Chi-square distribution with $P$ degrees of freedom. 

All of the parameters we test for significance using the lineup protocol, $\beta_3$ and $\beta_4$, were determined to be statistically significant using Equation~\ref{eq:wald1}. The results from the significance tests we performed using the lineup protocol are given in Table~\ref{tab:sigtesting}. \hh{XXX how do we interpret these  visual results? Include the six lineups in an appendix and refer to them from here.} \st{XXX should be taken care of in next paragraph now.}

<<sigtesting, results='asis'>>=
turk22_sig <- turk22 %>% 
  mutate(type2 = ifelse(sign == 0, sign(initialEst), sign),
         type2 = ifelse(type == "one", type2, -1)) %>% 
  filter(param_value == 1)
summ_sig <- turk22_sig %>% group_by(pic_id, test_param) %>% 
  summarize(npick = sum(datapick), n = n(), 
            pv = map2_dbl(npick, n, pV, m=6, scenario=3))
names(summ_sig) <- c("Lineup ID", "parameter", "# Alt. Model Picks", "Total Views", "p-value")
summ_sig$parameter <- as.factor(summ_sig$parameter)
levels(summ_sig$parameter) <- c("beta3", "beta4")
print(xtable::xtable(summ_sig, caption = "Experiment results for the two parameters for which we performed significance tests. There were three lineups for each parameter, so there are three results for each plot.", label = "tab:sigtesting", digits = c(0,0,0,0,0,5)), include.rownames = FALSE)
@

We see that the $p$-values from visual inference, which are calculated using the \texttt{vinference} \texttt{R} package by \citet{vinference}), are highly variable. This variability is introduced through the null plots generated from M1, since not all simulations look alike. In addition, the necessarily small number of null plots do not give the viewer as complete of a view of the null model as the usual 19 null plots would. The results of the significance tests for $\beta_3$ and $\beta_4$ are not clear cut. Thus, unlike the Wald-type tests described at the beginning of this section, there is no way to flat out reject or to fail to reject the null hypothesis that the parameter value is 0. We include all of the lineups shown to our participants in the appendix. 

\subsection{Goodness-of-Fit Testing}

Goodness-of-fit testing for network models is notoriously difficult. Most network models, other than the most simple, lack the asymptotics required to develop the goodness-of-fit methods required \citep{goldenberg09}. Some methods have been developed based on what Snijders et al call ``auxiliary statistics" such as the indegree or outdegree distribution on the nodes. In \texttt{RSiena}, the \texttt{sienaGOF} function performs goodness-of-fit testing as follows:
\begin{enumerate}
\item Auxiliary statistics are computed on the observed data ($\mathbf{u}_d$) and on $N$ simulated observations from the model ($\mathbf{u}_1 \dots \mathbf{u}_N$). (Usually, $N=1000$) 
\item The mean vector, $\overline{\mathbf{u}}$ and covariance matrix, $\mathbf{S}$ of the statistics on the simulations from the model are computed, and the Mahalanobis distance, $d_M(\mathbf{u})$ from the observed statistics to the distribution of the simulated statistics is computed:
\begin{equation}\label{eq:mahal}
d_M(\mathbf{u}) = \sqrt{(\mathbf{u} - \overline{\mathbf{u}})' S^{-1} (\mathbf{u} - \overline{\mathbf{u}})}
\end{equation}
\item The Mahalanobis distance for each of the $N$ simulations is calculated and $d_M(\mathbf{u}_d)$ is compared to this distribution of distances.
\item An empirical $p$-value is found by computing the proportion of simulated distances found in step 4 that are as large or larger than $d_M(\mathbf{u}_d)$. A SAOM is thus considered a good fit to the data if $p$ is large. A plot comparing the data to the simulations is also considered, and a similar plot is shown in Figure~\ref{fig:gofsiena}
\end{enumerate}

<<gofsiena, fig.height=3, fig.cap="An example of what a goodness-of-fit plot from \\texttt{RSiena} looks like. The overlaid boxplots and violin plots show the distribution of each of the outdegree values on the simulated networks, and the red points and lines are the observed data values.", cache = TRUE>>=
load("data/ansnullpaper.rda")
library(lattice)
gof1 <- sienaGOF(ansnull, OutdegreeDistribution, varName ="friendship", join = FALSE)
sims1 <- data.frame(gof1$`Period 1`$Simulations)
dat1 <- gof1$`Period 1`$Observations
dat1 %>% data.frame() %>% gather(outdegree, val) %>%
  mutate(outdegree = parse_number(outdegree)-1) -> dat1
p1 <- sims1 %>%
  mutate(sim = row_number()) %>%
  gather(outdegree, val, X1:X9) %>%
  mutate(outdegree = parse_number(outdegree) - 1) %>%
  filter(outdegree <= 6) %>%
  ggplot(aes(x = outdegree, y = val)) + 
  geom_boxplot(aes(group = outdegree), size = .5, outlier.shape = "x", outlier.size = 5) + 
  geom_violin(aes(group = outdegree), bw = 2, fill = NA) + 
  geom_point(data = dat1, color = 'red') + 
  geom_line(data= dat1, color = 'red') + 
  geom_text(data = dat1, aes(label = val), hjust = -.5) + 
  labs(x = "Outdegree (p = 0.154)", y = "Statistic", 
       title = "Goodness-of-Fit: Outdegree distribution period 1") + 
  ThemeNoNet
p1
@

The \texttt{RSiena} software also provides a Rao score-type test for goodness-of-fit for assessing one or more parameters, the test statistic of which is compared to a Chi-square distribution with $P$ degrees of freedom, where $P$ has the same definition as in Section~\ref{sec:sigtest}. For full detail on the score-type test, see \citet{scoretest}. 

These methods are similar in that they are both restriced: the \texttt{sienaGOF} method only considers one measure on the data and simulations from the model, while the score-type tests only consider subsets of parameters, ``nuisance parameters" in \citet{scoretest}, not the entire set of parameters. By using visual inference instead of more traditional statistical methods, we hope to perform a more holistic goodness-of-fit test. 

Using the lineup protocol, we show each Amazon Mechanical Turk worker the data once, in a lineup with five other plots of simulated data from one of the models we chose. We examined four different models, M2, M3, M4, and M6, and examined three repetitions of each, for a total of 12 goodness-of-fit lineups. In each lineup, the ``null model" is one of the four models and the ``alternative" model is the true, unknown model that generated the senate network data. The hypotheses for our goodness-of-fit tests are: 
\begin{description}
\item $H_0$:  The senate network data come from (or could have come from) the null model.
\item $H_A$: The senate network data do not come from the null model. 
\end{description}
If a lineup viewer picks out the data among the five simulations from the null model, it is evidence in favor of the alternative hypothesis. On the contrary, if the lineup viewer picks one of the null plots, that is evidence against the alternative hypothsis. Because the size of the lineups is small, the probability of picking the data by chance is high, $\frac{1}{6}$, but if \textit{many} independent viewers pick out the data from the nulls, the evidence in favor of the alternative hypothesis becomes stronger. Results from our MTurk goodness-of-fit plots are provided in Table~\ref{tab:gofstats}. 

<<goftab, cache = TRUE, results='asis'>>=
turk22_gof <- read_csv("data/turk22-gof.csv")
turk22_gof_stats <- turk22_gof %>% group_by(pic_id) %>% 
  summarize(npickdata = sum(datapick), total = n(), 
          pvinf = map2_dbl(npickdata, total, pV, m = 6, scenario = 3))
turk22_gof_stats %>% separate(pic_id, into = c("discard", "model", "rep"), sep = c(2,3)) -> print_gof_stats
print_gof_stats %>% select(-discard) %>% mutate(model = paste0("M", (as.integer(model)-1)))-> print_gof_stats
names(print_gof_stats) <- c("Model", "Replicate", "Data Picks", "Total Viewers", "p-value")
print_gof_stats$`p-value` <- ifelse(print_gof_stats$`p-value` < .0001, "< 0.0001", as.character(round(print_gof_stats$`p-value`,4)))
print(xtable::xtable(print_gof_stats, label = "tab:gofstats", caption = "An overview of the results from our 12 goodness-of-fit lineup tests.", digits = 4), caption.placement = "bottom", include.rownames = FALSE)
@

The $p$-values were calculated using the \texttt{vinference} package by \citet{vinference}. This package contains methods to calculate \textit{Visual distributions} for lineup experiment data.  The distribution depends on the number of evaluations of a plot, $K$, the size of the lineup, $m$, and the lineup scenario, which here is that each lineup containing the same data and the same set of null plots is shown to $K$ independent observers. \st{The visual inference family of distributions is similar to the binomial distribution, but takes the dependency among the $m$ plots in a signle lineup shown to multiple viewers into account. } Using these $p$-values, all but one lineup results in a rejection of the null hypothesis at Type-I error rate of $\alpha = 0.05$. The lineup that resulted in a failure to reject the null hypothesis is shown in Figure~\ref{fig:failgof}. The null model in this lineup is M4, and the senate data is shown in panel number $3^2 - 7$. However, the panel most participants chose was number four, and the most common reasoning for that choice was that it had the most simple structure. 

<<failgof, fig.height=4, fig.cap="The goodness-of-fit lineup that failed to reject the null hypothesis. The null model for this lineup is M4. Only 7 of 20 viewers of this lineup selected the data plot as the most different from the others.">>=
load("data/se112adjmat.RDS")
wave2 <- data.frame(se112adj)
senators <- data.frame(id = colnames(wave2), number = 1:155)
wave2$from <- colnames(wave2)
wave2 <- wave2 %>% gather(to, val, Alan.Stuart.Franken:William.Cowan) %>% filter(val > 0)
wave2 <- merge(wave2, senators, by.x = "from", by.y = "id", all = T)
wave2$from <- senators$number[match(wave2$from, senators$id)]
wave2$to <- senators$number[match(wave2$to, senators$id)]
wave2$from <- paste0("V", wave2$from)
wave2$to <- ifelse(is.na(wave2$to), NA, paste0("V", wave2$to))
wave2 <- wave2 %>% mutate(sim = 1001, model = "data", wave = 1) %>% select(-c(val, number))
dat <- read_csv(paste0("data/jtts_gof_9_2.csv"))
datplot <- unique(dat$ord[dat$sim == 1001])
dat <- dat %>% filter(sim != 1001) %>% 
      bind_rows(wave2) %>% 
      mutate(ord = ifelse(is.na(ord), datplot, ord))
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size = .5) + 
        theme_net() + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

The smallest $p$-value for one of the goodness-of-fit lineups was for the third replicate of the null model M4. This contradicts our previous finding that the only lineup to fail to reject the null was also when the null model was M4. The plot of this lineup is shown in Figure~\ref{fig:mostsiggof}. In the remaining replicate of M4 as the null model, 13 of 16 viewers identified the data plot, corresponding to a $p$-values of less than $0.0001$, just like the third replicate. This variability in results is similar to the variability we found in Section~\ref{sec:sigtest}. This variability is again introduced through the plots simulated from null model, and does not provide us with a clear cut decision resulting the hypothesis test. For model M4, we can neither reject nor fail to reject the null hypothesis that the data come from model M4. This is evidence that the goodness-of-fit of network models cannot always be determined by one dimensional derived features, such as $p$-value shown on the $x$-axis in Figure~\ref{fig:gofsiena}.   %This leads us to believe that the high $p$-value observed in the second replicate is more of a fluke. %This demonstrates some of the problems inherent with network models, such as model degeneracy, as discussed in \citet{degeneracy}. Network models 
%Furthermore, for the first lineup replicate with model M5 as the null model, only 9 of 21 participants were able to pick out the data, but the Visual distribution $p$-value is less than 0.05. %This is much higher than the corresponding Binomial probability of $Pr(X = 9|n = 21, p = \frac{1}{6}) = \binom{21}{9}(\frac{1}{6})^9(\frac{5}{6})^{21-9} \approx 0.003$, but we are not sure that having fewer than half of our participants identify the data plot is a truly significant result. We could move the threshold from $\alpha = 0.05$ to $\alpha = 0.01$ or $\alpha = 0.001$, but more work needs to be done to determine which, if any of these, is appropriate.  
For the other models for which we tested goodness-of-fit, however, we do have significant evidence from all three replicates to reject the null hypothesis that the null model generated the data. All of the goodness-of-fit lineups are provided in the appendix. 

\hh{XXX we need to go throught the logic of the argument below.}\st{ XXX I reformulated the discussion per our convo today.  }



<<mostsiggof, fig.height = 4, fig.cap="The lineup resulting in the smallest $p$-value rejecting the null hypothesis. Surprisingly, this another repetition for M4 as the null model.">>=
dat <- read_csv(paste0("data/jtts_gof_9_3.csv"))
datplot <- unique(dat$ord[dat$sim == 1001])
dat <- dat %>% filter(sim != 1001) %>% 
      bind_rows(wave2) %>% 
      mutate(ord = ifelse(is.na(ord), datplot, ord))
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size = .5) + 
        theme_net() + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

We believe this goodness-of-fit testing method holds promise for the future of social network analysis. The participants in our experiments are very good overall at picking out the data when it is noticeably different from the null plots in the lineups. In addition, as in replicate three for null model M4, when the null plots contain similarly sized structures as the data plot, our participants have a hard time distinguishing the data. We believe that running these tests multiple times using several different sets of null models to adequately explore the possible structures generated by the models is a step in the right direction for a more comprehensive goodness-of-fit test for network models. 

\subsection{Visual Power}

A summary of the results from our experiment is shown in Figure~\ref{fig:predictglmm}. On the $x$ axis, we plot the value of the parameter of interest, and on the $y$ axis, the proportion of times the plot of data simulated from the alternative model was picked out for each lineup. The results are split into groups based on the value of the parameter and the lineup type. We can see clear patterns in nearly all of the groups: as the parameter value approaches 0, fewer participants identified the alternative plot. 

<<readturk22, fig.height=6, fig.cap="A summary of some results from the Amazon mechanical turk study. On the $x$-axis, the value of the parameter changed in the model of interest, and on the $y$-axis, the proprotion of times turkers identified the different data plot. A simple linear regression is fit to each group. The pattern as the parameter value approaches zero is clear in all conditions.", eval = FALSE>>=
turk22 %>% mutate(type2 = ifelse(sign == 0, sign(initialEst), sign),
                       type2 = ifelse(type == "one", type2, -1)) %>% 
  group_by(pic_id, size, test_param, sign, type2,alt_model) %>% 
  summarize(datapick = mean(datapick)) %>%
  filter(pic_id != 3132) %>% 
  ggplot(aes(x = size, y = datapick, colour=factor(type2))) + 
  geom_point() +
  scale_color_brewer(palette = "Dark2", name = "Lineup Type") + 
  facet_wrap(~test_param, scales="free", labeller = 'label_both') +
  geom_smooth(se=FALSE, method="lm") + 
  ThemeNoNet + 
  theme(legend.position = 'bottom')
@

\hh{this is the power of the visual test XXX}
We futher explore this relationship between identification of the alternative data in the lineup and the parameter, effect size, and lineuptype with a generalized linear mixed model \st{that provides us with an estimate of the power of the visual test.} The response variable, $Y_{i\ell m}$, is binary, indicating whether participant $m$ picked the alternative data plot in lineup $\ell$. The covariate $x_{\ell 1}$ takes on the value -1 or 1 according to the lineup type code in Table~\ref{tab:experdetail}. In most cases, -1 indicates the parameter value is less than original estimate, while 1 indicates the parameter value is at or above the original estimate. The continuous covariate $x_{\ell 2}$ is the centered and scaled size of the effect of interest from which the alternative data were simulated, the values of which are labeled ``easy", ``medium", and ``hard" in Table~\ref{tab:experdetail} according to how difficult we thought the Turk participants would find each lineup. In Equation~\ref{eq:glmm}, $i \in \{1, 2, 3, 4, 5\}$ corresponding to the effects $\beta_3, \beta_4, \beta_2, \beta_6, \beta_5$, respectively. We include random effects in the model for each lineup, $\delta_{\ell}$ and each participant, $\epsilon_m$, and fit a hierarchical model as follows:
\begin{align}
\begin{split}
Y_{i\ell m} &\sim \text{Bernoulli}(\pi_{i\ell m}) \\
\text{logit}(\pi_{i\ell m}) & = \mu + \alpha_{i} + \theta\mathbb{I}(x_{\ell 1} = 1) + \gamma x_{\ell 2} + \\ & (\alpha\theta)_i \mathbb{I}(x_{\ell 1} = 1) + (\alpha\gamma)_i x_{\ell 2} + (\theta\gamma)\mathbb{I}(x_{\ell 1} = 1)x_{\ell 2} + \\ & (\alpha\theta\gamma)_i\mathbb{I}(x_{\ell 1} = 1) x_{\ell 2} + \delta_{\ell} + \epsilon_{m} \label{eq:glmm} \\
\delta_{\ell} & \sim N(0, \sigma^2_{\delta}) \\
\epsilon_{m} & \sim N(0, \sigma^2_{\epsilon})
\end{split}
\end{align}
The results of fitting this model using \texttt{glmer} from the \texttt{lme4} package are summarized in Table~\ref{tab:glmmests} \citep{lme4}. The baseline condition for this model against which all other experimental conditions are compared is the first condition in Table~\ref{tab:experdetail}, for parameter $\beta_1$ and lineup type -1. The expected value of the link function for this scenario with $\beta_1 = 0$ is $\mu$, and the expected probability that a new observer will pick out the data plot in a new lineup generated from this scenario is $\frac{\exp(\mu)}{1+\exp{\mu}}$. Similarly, for a new lineup generated from the tenth condition in in Table~\ref{tab:experdetail}, with $\beta_6 = 1$, and lineup type 1, the expected value of the link function is $\mu + \alpha_{4} + \theta + \gamma + (\alpha\theta)_4 + (\alpha\gamma)_4  + (\theta\gamma) + (\alpha\theta\gamma)_4$. The corresponding expected probability that a new observer viewing a new lineup from this scenario is: 
\begin{equation}
\frac{\exp\{\mu + \alpha_{4} + \theta + \gamma + (\alpha\theta)_4 + (\alpha\gamma)_4  + (\theta\gamma) + (\alpha\theta\gamma)_4\}}{1 + \exp\{\mu + \alpha_{4} + \theta + \gamma + (\alpha\theta)_4 + (\alpha\gamma)_4  + (\theta\gamma) + (\alpha\theta\gamma)_4\}}
\end{equation}

<<glmmres>>=
load("data/finalglmm.RDA")
mod <- model0randomscalesize
res <- summary(mod)[[10]]
ests <- as.numeric(round(res[,1], 4))
odds <- round(exp(ests), 4)
se <- as.numeric(round(res[,2], 4))
pval <- as.numeric(round(res[,4],4))
@

\begin{table}
\centering
\begin{tabular}{lcccc}
Parameter & Estimate & Std Error & $p$-value & Odds Multiplier \\
$\mu$ & \Sexpr{ests[1]} & \Sexpr{se[1]} & \Sexpr{pval[1]} & \Sexpr{odds[1]}\\
$\alpha_1$ & \Sexpr{ests[2]} & \Sexpr{se[2]} & \Sexpr{pval[2]} & \Sexpr{odds[2]} \\
$\alpha_2$ & \Sexpr{ests[3]} & \Sexpr{se[3]} & \Sexpr{pval[3]} & \Sexpr{odds[3]}  \\
$\alpha_3$  & \Sexpr{ests[4]} & \Sexpr{se[4]} & \Sexpr{pval[4]} & \Sexpr{odds[4]} \\
$\alpha_4$ & \Sexpr{ests[5]} & \Sexpr{se[5]} & \Sexpr{pval[5]} & \Sexpr{odds[5]}  \\
$\alpha_5$  & \Sexpr{ests[6]} & \Sexpr{se[6]} & \Sexpr{pval[6]} & \Sexpr{odds[6]} \\
$\theta$  & \Sexpr{ests[7]} & \Sexpr{se[7]} & \Sexpr{pval[7]} & \Sexpr{odds[7]} \\
$\gamma$  & \Sexpr{ests[8]} & \Sexpr{se[8]} & \Sexpr{pval[8]} & \Sexpr{odds[8]} \\ 
$(\alpha\theta)_1$  & \Sexpr{ests[9]} & \Sexpr{se[9]} & \Sexpr{pval[9]} & \Sexpr{odds[9]} \\
$(\alpha\theta)_2$  & \Sexpr{ests[10]} & \Sexpr{se[10]} & \Sexpr{pval[10]} & \Sexpr{odds[10]} \\
$(\alpha\theta)_3$  & \Sexpr{ests[11]} & \Sexpr{se[11]} & \Sexpr{pval[11]} & \Sexpr{odds[11]} \\
$(\alpha\theta)_4$  & \Sexpr{ests[12]} & \Sexpr{se[12]} & \Sexpr{pval[12]} & \Sexpr{odds[12]} \\
$(\alpha\theta)_5$  & \Sexpr{ests[13]} & \Sexpr{se[13]} & \Sexpr{pval[13]} & \Sexpr{odds[13]} \\
$(\alpha\gamma)_1$  & \Sexpr{ests[14]} & \Sexpr{se[14]} & \Sexpr{pval[14]} & \Sexpr{odds[14]} \\
$(\alpha\gamma)_2$  & \Sexpr{ests[15]} & \Sexpr{se[15]} & \Sexpr{pval[15]} & \Sexpr{odds[15]} \\
$(\alpha\gamma)_3$  & \Sexpr{ests[16]} & \Sexpr{se[16]} & \Sexpr{pval[16]} & \Sexpr{odds[16]} \\
$(\alpha\gamma)_4$  & \Sexpr{ests[17]} & \Sexpr{se[17]} & \Sexpr{pval[17]} & \Sexpr{odds[17]} \\
$(\alpha\gamma)_5$  & \Sexpr{ests[18]} & \Sexpr{se[18]} & \Sexpr{pval[18]} & \Sexpr{odds[18]} \\
$\theta\gamma$  & \Sexpr{ests[19]} & \Sexpr{se[19]} & \Sexpr{pval[19]} & \Sexpr{odds[19]} \\
$(\alpha\theta\gamma)_1$ & \Sexpr{ests[20]} & \Sexpr{se[20]} & \Sexpr{pval[20]} & \Sexpr{odds[20]}  \\
$(\alpha\theta\gamma)_2$  & \Sexpr{ests[21]} & \Sexpr{se[21]} & \Sexpr{pval[21]} & \Sexpr{odds[21]} \\
$(\alpha\theta\gamma)_3$  & \Sexpr{ests[22]} & \Sexpr{se[22]} & \Sexpr{pval[22]} & \Sexpr{odds[22]} \\
$(\alpha\theta\gamma)_4$  & \Sexpr{ests[23]} & \Sexpr{se[23]} & \Sexpr{pval[23]} & \Sexpr{odds[23]} \\
$(\alpha\theta\gamma)_5$  & \Sexpr{ests[24]} & \Sexpr{se[24]} & \Sexpr{pval[24]} & \Sexpr{odds[24]} \\ 
$\sigma^2_{\delta}$ & 0.5638 \\
$\sigma^2_{\epsilon}$ & 0.3416
\end{tabular}
\caption{\label{tab:glmmests}Summary of the results from fitting the model given in Equation~\ref{eq:glmm}}
\end{table}

<<predictglmm, fig.height=6, fig.cap='Predictions from our generalized linear mixed effects model given in Equation~\ref{eq:glmm}. The lines show the expected probability of detecting the alternative data in a lineup of size 6 for new observers of new lineups is plotted on the $y$-axis, and the size of the parameter of interest is on the $x$-axis. The proportions detected by our Turk participants for each lineup group are shown by the points, with the probability of picking out the data plot at random shown by a horizontal line at 1/6. The lineup marked as ``outlier" was removed from modeling. The panel for the reciprocity parameter, $\\beta_2$ is also presented in Figure~\\ref{fig:recipzoom} in more detail.'>>=
newdata <- read_csv("data/newdata_pred_glmm.csv")
newdata$param <- newdata$test_param
newdata$param <- as.factor(newdata$test_param)
levels(newdata$param) <- c("beta[1]", "beta[3]", "beta[4]", "beta[2]", "beta[6]", "beta[5]")
newdata$param <-as.character(newdata$param)
plotdata <- turk22 %>% mutate(type2 = ifelse(sign == 0, sign(initialEst), sign),
                       type2 = ifelse(type == "one", type2, -1)) %>% 
  group_by(pic_id, size, test_param, sign, type2,alt_model) %>% summarize(
    datapick = mean(datapick)
)
plotdata$param <- plotdata$test_param
plotdata$param <- as.factor(plotdata$param)
levels(plotdata$param) <- c("beta[1]", "beta[3]", "beta[4]", "beta[2]", "beta[6]", "beta[5]")
plotdata$param <- as.character(plotdata$param)
labdat <- data_frame(test_param = "jttp", param = "beta[3]", type2 = -1, x = -5, y = .9, label = "jttp_neg_hard_2")

ggplot() + 
  geom_line(data = newdata, aes(x = size, y = predictfinal, color = as.factor(type2))) + 
  geom_point(data = plotdata, aes(x = size, y = datapick, color = as.factor(type2))) + 
  #geom_text(data = plotdata, aes(x = size, y = datapick, color = as.factor(type2), label = pic_id)) +
  geom_text(data = labdat, aes(x = x, y=y, label = label, color = as.factor(type2)),
            show.legend = F) + 
  geom_hline(yintercept = 1/6, linetype='dashed') +
  scale_color_brewer(palette = "Dark2", name = "Lineup Type") + 
  facet_wrap(~param, scales = 'free_x', labeller = label_parsed) + 
  labs(x = "Parameter value", y = "Expected probability of detection in a lineup of size 6") + 
  ThemeNoNet + 
  theme(legend.position = 'bottom')


#vals <- predict(model1, type = "response")
#modelData_sig$prediction <- vals
#ggplot(data = modelData_sig) + 
#  geom_point(aes(x = size, y = prediction, color = as.factor(type2))) + 
#  facet_wrap(~test_param, scales = "free")
@

<<recipzoom, fig.height=2.5, fig.cap='The top middle panel of Figure~\\ref{fig:predictglmm} expanded to show greater detail. The square root of the estimate is shown on the $x$-axis. For this parameter, as its value approaches zero, the probability of identifying the alternate data model decreases, then increases, which is noticeably different from the pattern exhibited by the others. Again, a horizontal line is drawn at 1/6, the chance of selecting the data plot at random.'>>=
ggplot() + 
  geom_line(data = newdata %>% filter(test_param == "recip"), aes(x = sqrt(size), y = predictfinal, color = as.factor(type2))) + 
  geom_point(data = plotdata %>% filter(test_param == "recip"), aes(x = sqrt(size), y = datapick, color = as.factor(type2))) + 
  scale_color_brewer(palette = "Dark2", name = "Lineup Type") + 
  geom_hline(yintercept = 1/6, linetype='dashed') + 
  facet_wrap(~type2, scales = 'free_x') + 
  labs(x = "Square root of parameter value (recip)", y = "Expected probability of detection\nin a lineup of size 6") + 
  ThemeNoNet + 
  theme(legend.position = 'bottom')
@

In Figure~\ref{fig:predictglmm}, we see a clear trend in all parameters except $\beta_1$ and $\beta_2$ that as the parameter value approaches zero from either side, the probability of picking the data plot in a lineup of size six descreases. For $\beta_3$ and $\beta_5$, the slope of the fitted line is much steeper for positive values of the parameter than form negative values, meaning that our participants perceived differences more often for postitive parameter values than for negative parameter values. This finding is similar to that of \citet{corrviz}, who found that people detect positive correlations sooner and better than negative correlations. 

<<beta41, fig.height = 4, fig.cap='In our experiment, 52.8\\% of viewers of this plot selected the plot from the alternative model, M4. The ``reverse" of this lineup is given in Figure~\\ref{fig:beta4neg1}, where 41.4\\% of viewers selected the plot from the alternative model, M1. Here, the alternative plot is $\\sqrt{25} - 3$.'>>=
dat <- read_csv("data/jtts_pos_easy_3.csv")
# "answer" is 2
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size = .5) + 
        theme_net() + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

<<beta4neg1,fig.height=4, fig.cap='In our experiment, 41.4\\% of viewers of this plot selected the plot from the alternative model, M1. The ``reverse" of this lineup is given in Figure~\\ref{fig:beta41}, where 52.8\\% of viewers selected the plot from the alternative model, M1. Here, the alternative plot is $\\sqrt{25} - 1$.' >>=
dat <- read_csv("data/jtts_neg_med_2.csv")
# 'answer' is 4
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size = .5) + 
        theme_net() + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

For $\beta_4$ and $\beta_6$, where one plot simulated from M1 was placed among five plots from the corresponding model, we see that the predictions for the reverse lineup type (-1), are less than the standard lineup type (1) for all values of the parameter that we have. This contradicts our hypothesis for this scenario, which was that these two scenarios would perform similarly. One of the lineups for the $\beta_4 = 6.681$, lineup type 1 scenario is given in Figure~\ref{fig:beta41}, and a corresponding lineup for the lineup type -1 scenario is given in Figure~\ref{fig:beta4neg1}. For identical values of the parameter, viewers had a harder time identifying the different plot when they were selected the most ``simple" structure, detecting M1 in five plots from the more complicated model, than they did identifying the most ``complex" structure, the plot from the more complicated model, from the five plots from M1.  



\section{Discussion}\label{sec:concl}

By using visual inference methods, we have developed new ways to perform significance and goodness-of-fit testing for a complicated and intractable set of statistical models for social network data. These new methods can be used to supplement traditional methods and check our assumptions about network models. The traditional methods only look at one piece or one measure of a network model, but our methods look at the models holistically for a broader sense of what it means for a parameter to be significant or a model to be a good fit. By looking at an entire network simulated from a SAOM side-by-side with other instances of networks simulated from another model, instead of one-dimensional derived features, we develop an idea of the model in terms of the data itself, instead of in terms of statistical summaries of the data.

We have found the visual power of some effects in the object function of a SAOM for this particular senate data example,  and we have shown that, for the same effects, there is a lot of variability in results from significance and goodness of fit tests. Because the visual tests we performed show a great deal of variability, we can see that the decisions with respect to the significance of a parameter or the goodness of fit of a model to data are not as cut-and-dried as the traditional methods would have us believe. 

These results do not come without limitations. In visual inference, the null plots are supposed to play the role of good representatives of the null model. Here, the number of null plots is reduced to five, which increases the variability seen from a single lineup dramatically, and can unfortunately lead to different conclusions for the same lineup scenario. Furthermore, these results do not generalize yet. The lineups shown are made for only one test case, and it is not clear whether the power results transfer, nor is it clear to what degree if they do transfer, to other situations with different number of actors and different edge density. 

We hope to apply these methods further for different types of network data and different types of network models. We accept the limitations of this type of network data visualization, in that even in small instances, the cognitive load of looking at a lineup is very high for the average observer. We would therefore like to explore larger datasets, different layout algorithms, and different ways of visualizing network data, such as adjacency matrix visualizations, through visual inference to see if similar patterns emerge. 

\hh{Best results: visual detection/ power analysis, a lot of variability in goodness of fit and significance tests. BUT: visualizations show that a decision of significance/ goodness of fit are not as clear cut as the traditional tests want us to believe.

Limitations: 
\begin{itemize}
\item number of null plots: null plots are representatives of the null distribution of all possible node-link diagrams of data sampled from the null model. Here, the number of null plots is  reduced to five, which increases the variability seen from a single lineup dramatically and unfortunately leads to different conclusions.
\item ability to generalize: the lineups are made for only one test case - it is not clear, whether and how far, power results transfer to other situations with different number of actors and different edge density.
\end{itemize}}

% Talk about visual inference
%We propose to attack some of the aforementioned difficulties with SAOMs by using a technique known as \textit{visual inference}. This technique was created by \citet{Bujaetal} to provide well-defined, statistical rigor to the usual exploratory data analyses and model diagnostics that are typically performed by visualizing the data as opposed to looking at it raw or gathering numerical summaries of it. In visual inference there are \textit{null plots} and \textit{data plots}: the null plots are visualizations of data simulated from the model according to the null hypothesis, while the data plots are visualizations of data simulated from the model according to an alternative hypothesis. These two data sources are visualized side-by-side using small multiples using what Buja et al called the \textit{lineup protocol}. The idea behind the lineup protocol is the police lineup, where witnesses to crimes are brought to the police station to observe a group of people, one of whom is suspected to have committed the crime, and are asked to identify the perpetrator of the crime. If the witness identifies the suspect in the lineup, that is taken as evidence the suspect is guilty, whereas if the witness does not identify the suspect, that is taken as evidence the suspect is not guilty. In the visual inference lineup protocol, the \textit{suspect} is the data plot among $M-1$ null plots. If the "witness" can pick the data plot out of the lineup, that is taken as evidence that the null model should be rejected, whereas failing to pick out the data plot is taken as evidence that the null model should not be rejected. In our application of the lineup protocol to SAOMs, we include various effects of varying sizes in the SAOMs we fit to some data and try to \textit{see} those effects in node-link diagram corresponding to simulations from the various models.

%-Talk about political networks-

%-Talk about goodness-of-fit-

%-Talk about significance testings- 

