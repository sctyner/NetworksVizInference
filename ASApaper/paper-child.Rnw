% \newcommand{\st}[1]{{\color{orange} #1}}
% \newcommand{\hh}[1]{{\color{magenta} #1}}

<<setup-ch-2, echo = FALSE, message = FALSE, warning = FALSE>>=
options(replace.assign=TRUE,width=70, digits=3)
require(knitr)
opts_chunk$set(fig.path='figure/', cache.path='cache/', fig.align='center', fig.pos='h', out.width='.99\\textwidth', par=TRUE, cache=FALSE, concordance=TRUE, autodep=TRUE, message=F, warning=F, echo = FALSE, dev="cairo_pdf", fig.width = 6, fig.height = 6)#, root.dir = "~/Desktop/Dissertation/SAOM-removing-blindfold/")
@

<<pkgs>>=
library(tidyverse)
library(xtable)
library(RSienaTest)
library(RSiena)
library(geomnet)
library(GGally)
library(vinference)
library(lme4)
library(extrafont)
loadfonts(quiet = T)
load("../Data/senate/senateSienaNoHRC.rda")
ThemeNoNet <- theme_bw() %+replace% 
            theme(plot.title = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0, 
                                            family="Times New Roman"), 
                  axis.title.x = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0,
                                            family="Times New Roman"),
                  axis.title.y = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 90,
                                            family="Times New Roman"),
                  axis.text.x = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0,
                                            family="Times New Roman"), 
                  # axis.text.x.top = element_text(size = 12,
                  #                           face = 'plain', 
                  #                           angle = 0,
                  #                           family="Times New Roman"), 
                  # axis.text.x.bottom = element_text(size = 12,
                  #                           face = 'plain', 
                  #                           angle = 0,
                  #                           family="Times New Roman"), 
                  axis.text.y = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0,
                                            family="Times New Roman"),
                  # axis.text.y.left = element_text(size = 12,
                  #                           face = 'plain', 
                  #                           angle = 0,
                  #                           family="Times New Roman"),
                  # axis.text.y.right = element_text(size = 12,
                  #                           face = 'plain', 
                  #                           angle = 0,
                  #                           family="Times New Roman"),
                  strip.text.x = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0, 
                                            family="Times New Roman",
                                            margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.text.y =element_text(size = 12,
                                            face = 'plain', 
                                            angle = 90,
                                            family="Times New Roman",
                                            margin = margin(t = 3, r = 3, b = 3, l = 3, unit = "pt")),
                  strip.background = element_rect(colour = "black", fill = "white")
                    )
ThemeNet <- theme_net() %+replace% 
            theme(plot.title = element_text(size = 12,
                                            face = 'plain', angle = 0, family = "Times New Roman"), 
                  strip.text.x = element_text(size = 12,
                                            face = 'plain', angle = 0, family = "Times New Roman", 
                                            margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.text.y =element_text(size = 12,
                                            face = 'plain', angle = 0, family = "Times New Roman",margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  panel.border = element_rect(fill = NA, color = 'black'),
                  strip.background = element_rect(colour = "black", fill = "white"))
turk22 <- read_csv("data/turk22-sig.csv")
@


\section{Background} \label{sec:bg}
% \st{
% \begin{enumerate}
% \item significance testing: what is it and why is it important
% \item goodness-of-fit testing: what is it and why is it important
% \item difficulty of these two for sna, why do vis inf
% \end{enumerate}
% }
When selecting and fitting statistical models, there are typically three key assessments of interest: significance tests of parameters, goodness-of-fit tests, and power calculations. With significance testing, the null hypothesis assumes that the data come from a simple model nested within the model of interest. We then conduct significance tests of one or more additional parameters to determine how much of the variability in the data they explain. For goodness-of-fit tests, we examine one or more models of interest to assess how well these models explain the data. Power then quantifies the ability of the hypothesis test to detect the difference between the null and alternative hypothesis.  All three of these elements of statistical modeling are vital to ensure that we can draw valid conclusions from a model. 
%The more complicated the model, the harder it is to determine fit or to include or exclude a parameter. 

The more complex the model, however, the harder it typically is to assess significance, power, and goodness-of-fit. One particularly complicated family of models are those designed to model networks. A \textit{network} is any set of things, such as people, or computers, that are connected in some way, through social relations or the Internet. In a social network, people are \textit{nodes} or \textit{actors} connected by relationships, which are \textit{edges} or \textit{ties}. We use nodes and edges interchangeably with actors and ties, respectively. Models for networks are particularly complex, as 
dependencies inherent to network data make them  difficult to model. %Even more challenging is the situation 
This difficulty increases further when studying \textit{dynamic networks}, the same set of nodes and their changing relationships observed at many points in time, because of the added temporal dimension. Yet, researchers are often interested in modeling dynamic social networks, such as friendship networks among students or collaboration networks between legislators. %
Even some of the simplest network models, however, lack the asymptotics required to perform traditional goodness-of-fit tests \citep{holland81}. In addition, direct maximum likelihood estimation of model parameters is frequently impossible due to the intractability of the models \citep{hummeletal}. 
%\hh{is the previous sentence a quote? I'm not sure that I follow.} %\st{it's a paraphrase... shall I make a quote? Also I definitely botched a word. Hopefully makes more sense now? }
% stop 4pm on 1/28/19

In order to circumvent some of these difficulties, we propose a new approach for significance testing of parameters, goodness-of-fit testing, and power calculations %of these 
for one family social network models: continuous time Markov-chain (CTMC) models for dynamic network data, as described in \citet{saompaper}. We use \textit{visual inference} \st{to complement} %in place of 
traditional statistical methods for social network models, such as $t$-tests for significance of parameters or outdegree distributions for goodness-of-fit tests. Visual inference (VI), introduced by \citet{Bujaetal}, allows us to look at the \textit{entire} dataset simulated from a network model, whereas traditional methods use one-dimensional metrics derived from the network or a $p$-value for a parameter in the model. By using VI to supplement traditional statistical tests, we gain insight into the role of the parameters in these CTMC models, and we gain the ability to assess the fit of the CTMCs to dynamic network data.%In addition, we use visual inference to better understand some parameters in a social network model, by looking at how different values for parameters affect the networks simulated from these models.   

The paper is outlined as follows: Section~\ref{subsec:saom} provides an introduction to the CTMC family of models. Section~\ref{subsec:vi} gives a basic overview of visual inference and the lineup protocol. In Section~\ref{sec:models}, we describe our example data and define our models of interest that we used to develop our VI methods. Section~\ref{sec:expersetup} details how we designed a VI experiment for significance testing and goodness of fit procedures for CTMC models, and Section~\ref{sec:res} details the results of our experiment. We close with a discussion in Section~\ref{sec:concl}.  

%\hh{where do the results come in?}\st{whoops, forgot to updat this with the reorg. }

\subsection{CTMC Models for Dynamic Social Network Data}\label{subsec:saom}

%To model family that we apply to the Senate data is the Stochastic Actor-Oriented Model (SAOM) family. 
We define CTMC models for dynamic social network data here in their barest form. Full details on these models can be found in \citet{saompaper, snijders01, snijders:2010, snijders2010, snijdersetall:2007, snijders:2017}. CTMCs are a family of models for dynamic network data that incorporate both network structure and node-level covariates to describe how a network changes over time \citep{saompaper}. Traditional network models, such as exponential random graph models, usually only consider network structure.  Social networks are ever-changing as relationships decay or grow over time, and each actor in a network has characteristics that affect ties to other actors in the network. 
% Talk about SAOMs
CTMC models use the network structure and the node covariate information, which can lead to very complicated models. The model complexity and the inherent complexity of network data combine to make interpreting model parameters and their estimates veryh difficult. \st{citation?} There are also many possible parameters to include in a CTMC, which makes parameter selection and goodness-of-fit testing challenging.

CTMC models use network structure and node covariates to model the network change one tie at a time. The model is hierarchical: first, the \textit{rate function} dictates \textit{how often} changes in the network occur, then the  \textit{objective function} determines \textit{what} those changes are.% The variables of interest are the edges of the network, $x_{ij}$ between nodes $i$ and $j$, where $i \neq j \in \{1, 2, \dots, n\}$ and $n$ the number of nodes in the network. The $x_{ij}$ are directed and binary, i.e.
%   \begin{equation}\label{eq:edgevars}
%   x_{ij} =
%   \begin{cases}
%                                    1 & \text{if an edge from $i$ to $j$ exists} \\
%                                    0 & \text{otherwise}
%   \end{cases}
%   \end{equation}
% and self-referencing edges or loops are not allowed, i.e.  $x_{ii} = 0$ for all $i=1, 2, \dots, n$. 
% The network is observed $M \geq 2$ times at time points  $t_1 < ... < t_M$, and the entire network at time point $t_m$ is denoted as $x(t_m)$ for $m \in \{1, \dots, M\}$.

\subsection{Visual Inference}\label{subsec:vi}

Data visualization is an important component of data analysis, providing a mechanism for discovering patterns in data. Pioneering research by \citet{gelman:2004}, \citet{Bujaetal} and \citet{majumder:2011} provide methods to quantify the significance of discoveries made from visualizations. \citet{Bujaetal} introduced two protocols, the Rorschach and the lineup protocol, which bridge the gulf between traditional statistical inference and exploratory data analysis. %The Rorschach protocol consists of a set of $m$ (usually, $m=20$) plots (called the {\it null plots}) rendered from data that is consistent with a given null model. The Rorschach protocol helps to understand the extent of randomness in the null model. 
Here, we use the lineup protocol to design significance, goodness-of-fit (GoF) and power tests for the CTMC model. Under the lineup protocol, we begin with a data set of interest to us, such as a network, a visualization of this data, such as a node-link diagram, and a model of interest. We consider two hypotheses: the null hypothesis that the model of interest generated the data, and the alternative hypothesis that the data were not generated under the model of interest. To construct a lineup of size $P$, $P-1$ sets of data are simulated from the null model. Each of the $P-1$ simulated datasets are visualized in the same way as the data, and the plot of the data is placed randomly among the set of $P-1$ \textit{null plots}. Human observers then examine the lineup and identify the plot(s) that look(s) most different from the others. If an observer identifies the data plot, this is evidence against the null hypothesis, \st{though an observer has a chance of 1 in $P$ to pick the data plot at random.}  The evidence grows in strength with the number of independent observers identifying the data plot.

The lineup protocol places a \textit{plot} in the framework of hypothesis tests: the plot of the data is the test statistic, which is compared against the null plots, representing the sampling distribution under the null hypothesis. %Obviously, the null generating mechanism, i.e.\ the method of obtaining the data for null plots, is crucial for the lineup protocol, as the null hypothesis directly affects the choice of null generating method. Null generating methods are typically based on (a) simulation, if the null hypothesis allows us to directly specify a parametric model, (b) sampling, as in the case of large data sets, or (c) permutation of the original data \citep[see e.g.\ ][]{Good05}s. The network model of interest here allows us to simulate directly from a parametric model for dynamic social network data. 
%In the experimental data that we analyzed the null generating methods used were permutation methods and direct simulation from a null model.
The lineup protocol was formally tested for linear models in a head-to-head comparison with the equivalent conventional test in \citet{majumder:2011}. The experiment utilized human subjects from Amazon's Mechanical Turk \citep{turk} and used simulation to control conditions. The results suggest that VI done in a controlled setting gives similar results to conventional tests. This is evidence that VI can be used when no conventional tests exist or when testing is difficult. %Interestingly, the power of a visual test increases with the number of observers engaged to evaluate lineups, and the pattern in results suggests that the power will provide results consistent with practical significance \citep{kirk:1996}.


\section{Example Data and Models}\label{sec:models}

The data we use are collaboration networks in the United States Senate during the $111^{th}$ through $114^{th}$ Congresses. These senates began on January 6, 2009, the start date of the $111^{th}$ Congress, and ended on January 3, 2017, the last date of the $114^{th}$ Congress.
For each of the four senate sessions, we have three node covariates: the party affiliation of each senator, the number of bills they authored in each session, and their gender. We use each of these covariates in CTMC models to try to explain how ties are formed between senators over time. The node-link diagram for the senate network is shown in Figure~\ref{fig:senateAll}. We labelled some of the nodes in the network whose names we think will be familiar the reader, either because they are leaders in their party or they have run for president. The size of the nodes represents how many bills the senator authored in a session, the color represents party affiliation, and the shape represents gender. In each of the four sessions, there is one very large connected component tying many of the prominent senators together, with many smaller connected groups surrounding the larger component. In each senate, the structure changes slightly as new senators arrive or come to prominence.

<<senateAll, fig.height = 5, fig.cap="The US senate collaboration network observed at four time points. Color represents party, shape represents gender, and size represents number of bills authored in a session. The Fruchterman-Reingold layout is shown \\citep{fr}. Drawn with the \\texttt{geomnet} R package. \\citep{geomnet}">>=
seobama <- read_csv("data/senateobamapres_gsw_25.csv")
to_label <- c("Joseph R. Biden Jr.", "John S. McCain", "Ted Cruz","Marco Rubio",
  "Lindsey O. Graham","Rand Paul", "Bernard Sanders", "Jim Webb", "Mitch McConnell",
  "Harry M. Reid", "Amy Jean Klobuchar", "Elizabeth Warren")
seobama$label <- as.factor(ifelse(seobama$source %in% to_label, seobama$source, ""))
levels(seobama$label) <- c("", "Amy Klobuchar", "Bernie Sanders", "Elizabeth Warren",
                           "Harry Reid", "Jim Webb", "John McCain", 
                           "Joe Biden","Lindsey Graham", "Marco Rubio", "Mitch McConnell", 
                           "Rand Paul", "Ted Cruz")
se111clint <- filter(seobama, senate == 111)
se111noclint <- filter(seobama, senate == 111)
se111noclint$target[which(se111noclint$target == "Hillary Rodham Clinton")] <- NA
seobama2 <- seobama %>% filter(senate != 111) %>% bind_rows(se111noclint)
mynames <- data.frame(do.call(rbind, stringr::str_split(seobama2$label, " ", n = 2) ))
names(mynames) <- c("first", "last")
seobama2$label <- mynames$last

set.seed(93654)
ggplot(data = seobama2) + 
  geom_net(directed = T, labelon=T, arrowsize = .25, singletons= T, fiteach = T, linewidth = .25, layout.alg = 'fruchtermanreingold', fontsize = 2.5, repel=F, labelcolour = "black", vjust = -.8,
           aes(from_id = source, to_id = target, color = party, 
               label = label,shape = sex,size = n_au)) + 
  ThemeNet + 
  scale_color_manual(values = c("royalblue", "forestgreen","firebrick")) + 
  scale_shape_manual(values = c(17,16)) + 
  scale_size_continuous(name = "Bills\nauthored", range = c(.75, 3)) + 
  theme(strip.text = element_text(size = 8)) + 
  facet_wrap(~senate, nrow = 2, labeller = "label_both") + 
  xlim(c(0,1.05)) + 
  ylim(c(0,1.05))
@

%For Senate 111, for instance, we see Hillary Clinton, serving out her second term in the senate until she became Secretary of State. She is isolated in Figure~\ref{fig:senateAll}, but in actuality, she had many co-sponsors on only two pieces of legislation she authored in that short time.%, as is shown in Figure~\ref{fig:senateClinton}. 
%We chose to remove Clinton and her edges from the network because they make the overall structure look vastly more connected than the other three senates. We suspect that because she had just recently run for President and had been selected by President-Elect Obama to be Secretary of State, the co-sponsorships she garnered at the start of the new congress were largely symbolic, so the $111^{th}$ Senate without Hillary Clinton is more typical than the $111^{th}$ Senate with her. In fact, she had several co-sponsors on both of her pieces of legislation, which meant she had the maximum WPC value of one with about a dozen other senators. Thus, though she is in the Senate for a time, subsequent analyses will not include her, and will instead include her replacement, Kirsten Gillibrand.  

<<senateClinton, fig.cap="We removed Hillary Clinton's ties from the network because she had abnormally high collaboration with senators during the time she was in the 111th senate and before she left office to become Secretary of State.", fig.height=3, eval=FALSE>>=
se111clint$Clinton <- 'Yes'
se111noclint$Clinton <- 'No'
clintonSenate <- rbind(se111clint, se111noclint)
clintonSenate$Clinton <- as.factor(clintonSenate$Clinton)
clintonSenate$Clinton <- ordered(clintonSenate$Clinton, levels = c("Yes", "No"))
clintonSenate %>% filter(!(source %in% c("Roland Burris", "Bernard Sanders", "Frank R. Lautenberg", "Mary L. Landrieu") & is.na(target))) %>%
ggplot() + 
  geom_net(directed = T, labelon=F, arrowsize = .3, singletons= F, fiteach = T, arrowgap = .01, layout.alg = 'fruchtermanreingold',
           aes(from_id = source, to_id = target, color = party, linewidth = gsw, shape = sex,size = n_au)) + 
  scale_size_continuous(name = "Bills\nauthored", range = c(.75, 3)) + 
  ThemeNet + 
  scale_shape_manual(values = c(17,16)) + 
  scale_color_manual(values = c("royalblue", "forestgreen","firebrick")) + 
  theme(legend.position = 'bottom') + 
  facet_wrap(~Clinton, nrow = 1, labeller = 'label_both')
@


\begin{table}
\centering
\scalebox{0.7}{
\begin{tabular}{c|p{2cm}|p{2.1cm}|c|l|p{1.75cm}|p{2cm}}
\toprule
$\beta_k$ & Effect name & Interaction Variable & Formula & Picture &  Initial estimate &  Wald $p$-value \\
\midrule
$\beta_1$ & density & -- & $s_{i1}(x) = \sum_{j} x_{ij}$ & \includegraphics[width=.6in]{img/density.png} & 2.204 & NA \\ 
$\beta_2$ & reciprocity & -- & $s_{i2}(x) = \sum_{j} x_{ij}x_{ji}$ & \includegraphics[width=.6in]{img/recip.png} & -4.903 & NA \\
$\beta_3$ & jumping transitive triplet & party & $s_{i3}(x, \mathbf{p}) = \sum_{j\neq h} x_{ij}x_{ih}x_{hj}\cdot \mathbb{I}(p_i = p_h \neq p_j)$ & \includegraphics[width=.6in]{img/jttp.png} & -5.884 & $<0.0001$\\
$\beta_4$ & jumping transitive triplet & sex & $s_{i4}(x, \mathbf{s}) = \sum_{j\neq h} x_{ij}x_{ih}x_{hj}\cdot \mathbb{I}(s_i = s_h \neq s_j)$ & \includegraphics[width=.6in]{img/jtts.png}  & 3.335 & 0.0002 \\
$\beta_5$ & similarity transitive triplet & bills & $s_{i5}(x, \mathbf{b}) = \sum_{j} x_{ij}x_{ih}x_{hj}\cdot (sim^b_{ij} - \overline{sim}^b)^*$ & \includegraphics[width=1in]{img/simttb.png} & 9.821 & 0.0128 \\
$\beta_6$ & same transitive triplet & party & $s_{i6}(x, \mathbf{p}) =\sum_{j} x_{ij}x_{ih}x_{hj}\cdot \mathbb{I}(p_i = p_j)$ & \includegraphics[width=1in]{img/samettp.png}  & 1.306 & 0.0642 \\
\bottomrule
%\\
%$\beta_7$ & same & party & $s_{i7}(x, \mathbf{p}) = \sum_j x_{ij}\mathbb{I}(p_i = p_j)$ & \includegraphics[width=.6in]{img/samep.png}  & 0.363 & 0.0074
\end{tabular}
}
\caption{\label{tab:effects} The effects we used in the CTMC models for the senate data. In the picture which represents each effect, the dotted tie is encouraged to form if the estimate is positive, and discouraged to form if the estimate is negative.  $^*$ - $sim^b_{ij}$ is defined in Equation~\ref{eq:similar} and $\overline{sim}^b = \frac{1}{n(n-1)}\sum_{i\neq j} sim^b_{ij}$ is the average bill similarity score between two senators.}
\end{table}


%\begin{wraptable}{r}{6.5cm}
\begin{table}
\begin{tabular}{lcccccc}
\toprule 
Model & $\beta_{1}$ & $\beta_{2}$ & $\beta_{3}$ & $\beta_{4}$ & $\beta_{5}$ & $\beta_{6}$ \\
\midrule
M1 & 	\checkmark & 	\checkmark & -- & -- & -- & -- \\
M3 & \checkmark & 	\checkmark & \checkmark & -- & -- & -- \\
M4 & \checkmark & 	\checkmark & -- & \checkmark & -- & -- \\
M5 & \checkmark & 	\checkmark & -- & -- & \checkmark & -- \\
M6 & \checkmark & 	\checkmark & -- & -- & -- & \checkmark \\
M7 & \checkmark & 	\checkmark & -- & \checkmark & \checkmark & \checkmark \\
\bottomrule
\end{tabular}
\caption{\label{tab:models}The models we use defined by the parameters in their objective functions. The corresponding parameter for $s_{ik}$ is $\beta_k$. Note there is no model M2.}
\end{table}
%\end{wraptable}
% 
% \begin{enumerate}
% \item Model M1: $f_{i}(x, \boldsymbol{\beta}) = \beta_1 s_{i1}(x) + \beta_2 s_{i2}(x)$
% \item Model M3: $f_{i}(x, \boldsymbol{\beta}, \mathbf{p}) = \beta_1 s_{i1}(x) + \beta_2 s_{i2}(x) + \beta_3 s_{i3}(x, \mathbf{p})$
% \item Model M4: $f_{i}(x, \boldsymbol{\beta}, \mathbf{s}) = \beta_1 s_{i1}(x) + \beta_2 s_{i2}(x) + \beta_4 s_{i4}(x, \mathbf{s})$
% \item Model M5: $f_{i}(x, \boldsymbol{\beta}, \mathbf{b}) = \beta_1 s_{i1}(x) + \beta_2 s_{i2}(x) + \beta_5 s_{i5}(x, \mathbf{b})$
% \item Model M6: $f_{i}(x, \boldsymbol{\beta}, \mathbf{p}) = \beta_1 s_{i1}(x) + \beta_2 s_{i2}(x) + \beta_6 s_{i6}(x, \mathbf{p})$
% \item Model M7: $f_{i}(x, \boldsymbol{\beta}, \mathbf{p}, \mathbf{b}, \mathbf{s}) = \beta_1 s_{i1}(x) + \beta_2 s_{i2}(x) + \beta_4 s_{i4}(x, \mathbf{s}) + \beta_5 s_{i5}(x, \mathbf{b}) + \beta_6 s_{i6}(x, \mathbf{p})$
% \end{enumerate}


%To determine the effects that we would move forward with, we followed this procedure: 

% \begin{enumerate}
% \item Define the simple effects structure of the data: the rate parameters and the outdegree and reciprocity parameters. 
% \item Add each additional possible effect, \st{as determined by the effects documentation function}, in \texttt{RSiena} one-at-a-time to the model\st{'s objective function}  \citep{RSiena}.
% \item Fit each model to the data and check for convergence.
%     \begin{enumerate}
%     \item If the model converged, move to 4.
%     \item If the model did not converge, use the previous fitted values as starting values and repeat 5 times or until convergence, whichever comes first.
%     \end{enumerate}
% \item Test the added parameter for significance using a Wald-type test.
% \item Report out the estimate of the additional parameter, its standard error, Wald $p$ value, and convergence criterion.
% \end{enumerate}
%  
% After completing the procedure for all model effects, we selected effects whose estimates converged, had a Wald $p$-value of less than 0.10, and seemed to have a reasonable interpretation for our data according to well-known properties of legislative networks \citep{legnet}.

%The parameters we use for the remainder of the paper are detailed in Table~\ref{tab:effects}.

\noindent \textbf{Models:} %In order to determine which models to examine, we first consider already well-known effects in legislative networks. We also selected some effects which were significant according to the built-in Wald-type tests in \texttt{RSiena} for application of our significance and goodness-of-fit methods. 
We fit six models to the senate data, and each model is identified by the parameters its objective function as shown in Table~\ref{tab:models}. The parameters in the objective function, $\beta_1, \dots, \beta_6$ are defined in Table~\ref{tab:effects}. All models have the outdegree and reciprocity parameters, $\beta_1$ and $\beta_2$, included. Other effects were added one at a time so that the simplest model $M1$ is nested in each of the other models. The largest model is M7: each of the other five models is nested in it. We used Wald-type as described in Section~\ref{sec:res} to perform significance tests on $\beta_3, \dots, \beta_6$ in models M3,$\dots$, M6. The most significant effect is $\beta_3$, the jumping transitive triplet (JTT) parameter for the party covariate, which was estimated to be about -5.9 with a standard error of 0.11, resulting in a Wald $p$-value of less than 0.0001. This parameter considers the number of transitive closures formed between two senators from different parties. The large negative estimate is an indication that forming transitive ties between two people from different parties is strongly discouraged, which comports with the divisive nature of American politics. Another significant effect is $\beta_4$, the same JTT parameter for the sex covariate, with an estimate of about 3.3 with a standard error of 0.89. This parameter also considers transitive closures, but for senators of different genders. The positive value indicates that transitive ties between senators of different genders are more likely to form. Next, we consider $\beta_5$, the covariate-related similarity score-weighted transitive triplets parameter for the number of bills authored by a senator. We chose to look at similarity instead of raw covariate value because the number of bills authored is more continuous than gender or party. The similarity measure is computed as: 
\begin{equation}\label{eq:similar}
sim^b_{ij} = \frac{\max_{hk}|b_h - b_k| - |b_i - b_j|}{\max_{hk}|b_h - b_k|} 
\end{equation}
where $\max_{hk}|b_h - b_k|$ is the range of number of bills authored by senators, and $b_i$, $b_j$ are the number of bills authored by senators $i,j$ respectively in the senate period. This effect was estimated at about 9.8 with standard error of 3.9. The high positive estimate suggests senators are encouraged to collaborate with other senators who author about the same number of bills they do. That senators tend co-sponsor bills written by senators who are similarly prolific corresponds to the tendency of senators to be either ``workhorses" or ``show horses" \citep{legnet}. Senators known as workhorses author many pieces of legislation in a session, and largely stay out of the public arena. The show horse senators, on the other hand, author relatively few pieces of legislation, and tend to appear in the media very frequently. Finally, we found $\beta_6$, the same party transitive triplet effect to be significant, with a fitted value of 1.3 and standard error of 0.7, meaning that transitive relationships between senators tend to form when they are from the same party, exactly as we would expect in a legislative body in a country with deeply entrenched partisan divides. 

<<geteffects, eval = FALSE>>=
# used for table below
initeff <- read_csv("data/sigEffsSenate912.csv")
initeff %>% filter(shortName == "sameX")
#density & -- & $\sum_j x_{ij}$ & & & \\
#reciprocity & -- & $\sum_j x_{ij}x_{ji}$ & & & \\
@


<<modelestimates>>=
load("data/allModelMeans.RDS")
modelMeanEsts %>% unnest(ests) %>% mutate(param = "rate") -> modelmeans
modelmeans$param[c(1,6,12, 18,24, 30)] <- "alpha1"
modelmeans$param[c(1,6,12, 18,24, 30)+1] <- "alpha2"
modelmeans$param[c(1,6,12, 18,24, 30)+2] <- "alpha3"
modelmeans$param[c(1,6,12, 18,24, 30)+3] <- "beta1"
modelmeans$param[c(1,6,12, 18,24, 30)+4] <- "beta2"
modelmeans$param[modelmeans$param == "rate"] <- c("beta3", 'beta4', 'samep', 'beta6', 'beta5')
modelmeans %>% filter(model != "samep") %>% spread(param, ests) -> modelmeans
M6 <- c(2.4405048,2.4594403,2.2098176,-4.9232775,4.8916183,2.3743720,0.2047038,6.9661589)
@


\begin{table}
\centering
\begin{tabular}{|lccccccccc|}
\toprule
Model & $\hat{\alpha}_1$ & $\hat{\alpha}_2$ & $\hat{\alpha}_3$ & $\hat{\beta}_1$ & $\hat{\beta}_2$ & $\hat{\beta}_3$ & $\hat{\beta}_4$ & $\hat{\beta}_5$ & $\hat{\beta}_6$ \\
\midrule
M1 & \Sexpr{round(modelmeans$alpha1[1], 3)} & \Sexpr{round(modelmeans$alpha2[1], 3)} & \Sexpr{round(modelmeans$alpha3[1], 3)} & \Sexpr{round(modelmeans$beta1[1], 3)} & \Sexpr{round(modelmeans$beta2[1], 3)} & -- & -- & -- & --\\
M3 & \Sexpr{round(modelmeans$alpha1[2], 3)} & \Sexpr{round(modelmeans$alpha2[2], 3)} & \Sexpr{round(modelmeans$alpha3[2], 3)} & \Sexpr{round(modelmeans$beta1[2], 3)} & \Sexpr{round(modelmeans$beta2[2], 3)} & \Sexpr{round(modelmeans$beta3[2], 3)} & -- & -- & --\\ 
M4 & \Sexpr{round(modelmeans$alpha1[3], 3)} & \Sexpr{round(modelmeans$alpha2[3], 3)} & \Sexpr{round(modelmeans$alpha3[3], 3)} & \Sexpr{round(modelmeans$beta1[3], 3)} & \Sexpr{round(modelmeans$beta2[3], 3)} & -- & \Sexpr{round(modelmeans$beta4[3], 3)} & -- & --\\
M5 & \Sexpr{round(modelmeans$alpha1[5], 3)} & \Sexpr{round(modelmeans$alpha2[5], 3)} & \Sexpr{round(modelmeans$alpha3[5], 3)} & \Sexpr{round(modelmeans$beta1[5], 3)}& \Sexpr{round(modelmeans$beta2[5], 3)} & -- & -- & \Sexpr{round(modelmeans$beta5[5], 3)} & -- \\
M6 & \Sexpr{round(modelmeans$alpha1[4], 3)} & \Sexpr{round(modelmeans$alpha2[4], 3)} & \Sexpr{round(modelmeans$alpha3[4], 3)} & \Sexpr{round(modelmeans$beta1[4], 3)} & \Sexpr{round(modelmeans$beta2[4], 3)} & -- & -- & -- & \Sexpr{round(modelmeans$beta6[4], 3)}\\
M7 & \Sexpr{round(M6[1], 3)} & \Sexpr{round(M6[2], 3)} & \Sexpr{round(M6[3], 3)} & \Sexpr{round(M6[4], 3)} & \Sexpr{round(M6[5], 3)} & -- & \Sexpr{round(M6[6], 3)} & \Sexpr{round(M6[8], 3)} & \Sexpr{round(M6[7], 3)} \\
\bottomrule
\end{tabular}
\caption{\label{tab:fittedvalues} The mean estimates from repeated fits of our models of interest. When simulating from these models, these are the estimates that we will use unless otherwise stated.}
\end{table}

We fit all six of our models in \texttt{RSiena} using Markov Chain Monte Carlo (MCMC) methods to approximate the method of moments estimates of the parameters. Because the estimation is done through MCMC simulation, we fit each model to the data 1,000 times. From the simulations that converged, which made up over 90\% of the fits for each model, we computed the mean of the estimates of each parameter to get final estimates of $\hat{\boldsymbol{\beta}}$ for each model, which are shown in Table~\ref{tab:fittedvalues}. 


We want to determine the role that each of these parameters plays in the objective functions for the different models. We use the estimates given in Table~\ref{tab:fittedvalues} to simulate from models M1 through M6. Then, we examine the effects on the structure of the simulated networks with a large visual inference experiment to determine significant effects, goodness-of-fit, and visual power. 

\section{Methodology}\label{sec:expersetup}

We want to assess CTMC models for the senate network data using the lineup protocol in three ways: with significance tests of parameters,  goodness-of-fit tests of a model, and determination of the visual power of the effects.
Each one of these situations requires a different setup, which we describe in detail, making use of the lineup protocol defined in \citet{Bujaetal}. In each lineup, we include plots from two models: a null model and an alternative model. The definition of the null and alternative model varies with the model and the assessment we explore. 
%For each lineup, we consider two models: model M1 is the null model, and another model, from M2-M5, which is chosen to be the alternative model. 

Typically, a lineup shows sets of 20 plots at a time, for example in \citet{loy:2015, vanderplas:2016}, but we determined that looking at 20 node-link diagrams at once is too difficult. We chose to present our participants with only six plots at a time in order to show the node-link diagrams in more detail and to reduce cognitive load.
%To construct a lineup, we simulate five networks from the null model and one network from the alternative model.  
Several lineups shown to our participants are presented and discussed in Section~\ref{sec:res}.  %In this lineup, model M4 is the alternative model (one plot), and model M1 is the null model (five plots). 

To simulate lineups from the models we used the \texttt{siena07} function in \texttt{RSiena} \citep{RSiena}. For the purposes of our experiment, we focus on simulating the second ``wave" of data, the $112^{th}$ Senate network, and we condition on the first wave of data, $111^{th}$ Senate network.  Sections~\ref{sec:setupst} through~\ref{sec:setupvp} describe in detail how we constructed the lineups, which parameter values used, and why. Lineups we created were shown to independent observers recruited through Amazon Mechanical Turk for feedback, and we provide more detail on the Turk setup in Section~\ref{sec:res}. %, we set the parameters to the values given in Table~\ref{tab:fittedvalues} for all parameters within the respective models, with the exception of $\beta_5$. For $\beta_5$, twice the estimated value was used. More detail on why we use twice the fitted value is provided in Section~\ref{sec:setupvp}.\footnote{If you would like to explore the kinds of lineups we use in further detail, please visit \url{https://sctyner.shinyapps.io/saom_lineup_creation/}}. To get the simulations,


We recruited 250 participants for our experiment through Amazon Mechanical Turk. Each participant was first presented with some brief training material. Before presenting the lineups for the hypothesis tests, each participant was shown two trial plots: one where the alternative plot was the most different from the others due to its relatively \textit{complex} structure, while the other trial included an alternative plot that was most different from the others due to its comparatively \textit{simple} structure. Only when participants were able to correctly identify the alternative plot from the trial lineups were they allowed to begin the experiment. 

Each participant was randomly assigned 13 lineups to look at. They were asked to select one or more plots that they perceived as ``most different" from the others, and provide a reasoning for their choice: ``most simple overall structure",``most complex overall structure", or ``other". If they selected ``other", they were required to describe their reasoning. The language in the reasoning is purposefully vague to avoid contextual bias. 

Twelve of the 13 lineups that the participants saw were used for the significance testing and the visual power methods discussed in Sections~\ref{sec:setupst} and \ref{sec:setupvp}. Among the 12 lineups were four from each difficulty level (easy, medium, and hard), and two of the four were from each condition (negative-positive, or positive-reverse). Each participant saw each of the six models twice at some combination of difficulty and condition.   %The six parameters, $\beta_1, \dots, \beta_6$ were set to three different values according to how difficult we thought picking the data plot from the lineup would be for our participants, and were also set to be less than or greater than the initial estimate, creating the lineup type variable.%\hh{what about the first 12 plots? 6 parameters, 3 levels, 2 directions}
The last lineups shown to participants contained the true data from the 112th senate shown in Section~\ref{sec:models} placed among five other plots from  one of the models M3, M4, M5, and M7 as discussed in Section~\ref{sec:gofsetup}. Upon completion of the 13 lineups, each participant was paid \$1.75    

% future:  which took an average of 308 seconds per person, 
\subsection{Significance Testing}\label{sec:setupst}

In the significance testing protocol, a parameter of interest $\beta_k$ is selected to test. The hypotheses we use to generate lineups are: 

\begin{equation}\label{eq:sig}
H_0: \beta_k = 0 \ \ \ \text{ versus } \ \ \ H_A: \beta_k \neq 0
\end{equation}
%\begin{description}
%\item $H_0$: $\beta_k = 0 $
%\item $H_A$: $\beta_k \neq 0$
%\end{description}
Under the null hypothesis, we assume that the model that generated the network data is M1, the simplest model presented in Section~\ref{sec:models}, and the alternative model is the one with $\beta_k$ included. In the lineup, there are five null plots constructed from  five simulations from M1 of the second wave, with $\beta_1, \beta_2$ set to the corresponding estimates given in Table~\ref{tab:fittedvalues}. The alternative data plot is simulated from the appropriate model with $\beta_1, \beta_2$, and $\beta_k$ in the objective function at corresponding values given in Table~\ref{tab:fittedvalues}. We constructed the lineups in this way because we hypothesize that if an effect is significant, it will alter the appearance of the simulated networks to the extent that they can be visually distinguished from the networks simulated from the simple model M1.   Thus, an observer picking the alternative data plot as most different is evidence against the null hypothesis, while picking one of the null plots is evidence in favor of the null hypothesis. To avoid over-working our participants, we chose to test only two parameters, $\beta_3$ and $\beta_4$. These two had the smallest $p$-values from the significance tests, so we chose them for the visual significance test as well because we hypothesized they would be easier to pick out of the lineup.  We compare the null model M1 to the alternative models M3 and M4, using three repetitions of each hypothesis test in the experiment to determine if $\beta_3, \beta_4$ are significant.

\subsection{Goodness-of-Fit Testing}\label{sec:gofsetup}

For the goodness-of-fit tests, we compare one model of interest to the second wave of sentate network data. This procedure is identical to that in \citet{Bujaetal} and \citet{majumder:2011}.  The hypotheses for the goodness-of-fit tests are: 
\begin{description}
\centering
\item $H_0$: The data come from the model of interest
\item $H_A$: The data come from some other, unknown model
\end{description}

\noindent To generate the null plots, we simulate five second wave networks from the model of interest using the corresponding parameters in Table~\ref{tab:fittedvalues}. Among these five plots, we place the true second wave of data. We cannot show the data more than once to each participant because doing so would bias the results, so each participant only sees one goodness-of-fit lineup. The models we chose for goodness-of-fit testing are M3, M4, M5, and M7. If a participant selects the data as most different, that is evidence that the model of interest is not a good fit. 

\subsection{Visual Power Testing}\label{sec:setupvp}



%Visual detection does not fit into the typical statistical testing framework as easily as the significance and goodness-of-fit tests we perform do. 
Using VI, we want to determine at what value an additional effect included in the model becomes noticeable. By \textit{noticeable}, we mean that the inclusion of the effect alters the appearance of networks simulated from the model so much that many independent viewers are able to pick out plot containing the data simulated from the model \textit{with} the effect in a lineup among five plots simulated from the model \textit{without} the effect. In this way, we measure the visual power of a parameter. We perform visual power tests for all parameters in the objective function, $\beta_1, \dots, \beta_6$. 

In model M1, with only two parameters in the objective function, we changed the density and reciprocity parameter values one at a time, keeping all other parameters constant. We look at some values greater and some values lower than the estimates of $\beta_1, \beta_2$ given in Table~\ref{tab:fittedvalues}. In models M3 through M6, we vary only the additional parameter, $\beta_3$ through $\beta_6$, respectively, while holding all other parameters constant. We want to determine how the size of these parameters affects the overall structure of the network data simulated from the models M1 through M6. %By varying  so we also vary the magnitude of the parameters in order to determine at what value the effects become noticeable. %in both negative and positive directions. 

%Because the traditional way to visualize a network is a node-link diagram, we will examine the effect of different parameter values using node-link visualizations. 

<<hypothesis, fig.height=2, fig.cap="As the parameter increases in absolute value, more viewers of the lineup should pick the alternative data out of a lineup. Note that the significance test we construct in Section~\\ref{sec:setupst} is just one point on the curve. The easy, medium, and hard lines represent how we determined which values of the parameters to show to our participants, and the horizontal dotted line is the chance of picking the alternative plot at random.">>=
# , represented by the vertical dotted line labeled $\\hat{\\beta}$
x <- seq(-10, 10, .05)
N <- length(x)
qplot(x = 1:N, y = pt(x, 1), geom = 'line') + geom_line(aes(x = -(1:N), pt(x,1))) + 
  scale_x_continuous(name = "Parameter Value", labels = c("very negative", "negative", "0", "positive", "very positive")) + 
  geom_hline(yintercept = 1/6, linetype = 'dotted') +
  geom_vline(xintercept = 0) + 
  geom_label(data = NULL, aes(x = 0, y = .9, label = "Null Model = Alt. Model"), size = 2) +
 # geom_vline(xintercept = 150, color = 'red', linetype = 'dashed') + 
#  annotate('text', x = 160, y = .9, label = "hat(beta)",parse = TRUE, size = 2) + 
  geom_vline(xintercept = c(-300,-230, -175), color = c('green', 'yellow', 'red')) + 
  geom_vline(xintercept = c(300,230, 175), color = c('green', 'yellow', 'red')) + 
  geom_text(data = NULL, aes(x = -300, y = .75, label = "easy"), angle = 90, vjust = -.5, size = 3) + 
  geom_text(data = NULL, aes(x = -230, y = .5, label = "medium"), angle = 90, vjust = -.5, size = 3) +
  geom_text(data = NULL, aes(x = -175, y = .06, label = "hard"), angle = 90, vjust = -.5, size = 3) + 
  geom_text(data = NULL, aes(x = 300, y = .75, label = "easy"), angle = 90, vjust = -.5, size = 3) + 
  geom_text(data = NULL, aes(x = 230, y = .5, label = "medium"), angle = 90, vjust = -.5, size = 3) +
  geom_text(data = NULL, aes(x = 175, y = .06, label = "hard"), angle = 90, vjust = -.5, size = 3) + 
  scale_y_continuous(name = "% detecting plot from\nthe alternative model", labels = paste0(c(0,25,50,75,100), "%")) + 
  theme_bw() + 
  theme(text = element_text(size = rel(3)))
@

\begin{figure}
\includegraphics[width=.9\textwidth]{img/shinyappscreen.png}
\caption{\label{fig:shiny} A screen shot of the web application we created to help design our lineup experiment.% More details about this application are given in Section~\ref{sec:setupvp}. In the lineup, M5 is the alternative model with $\beta_5$ set to twice its estimated value given in Table~\ref{tab:fittedvalues}. One plot simulated from this model is placed at random among five observations simulated from the null model, M1. %Participants of the study are asked to identify the most different plot.  %The other two tabs in the right side of the screen tell you which plot comes from the alternative model, and allow you to view and download the data that is plotted in the lineup.
}
\end{figure}

To determine when an effect becomes noticeable, we examine six different levels of the effect: three negative and three positive. Figure~\ref{fig:hypothesis} shows a sketch of the hypothetical selection probability with varying effect size.  We hypothesize that as the parameter increases in absolute value, an observer is more likely to select the alternative data plot out of the lineup. The six levels of the effect are vertical lines labeled ``easy," ``medium," and ``hard" in Figure~\ref{fig:hypothesis}.  We expect most observers to pick out the alternative data plot at the ``easy" values, and we expect very few, if any, observers to pick out the alternative data plot at the ``hard" values. To help us decide on the parameter values to use for each effect at each difficulty level, we constructed an online application that simulated the lineup protocol for us to be the guinea pigs in our own experiment \citep{theqs}. A screen shot of the app we created with the \texttt{shiny} package by \citet{shiny} is shown in Figure~\ref{fig:shiny}. On the left side of the screen, the user\footnote{Please visit \url{https://sctyner.shinyapps.io/saom_lineup_creation/} to create lineups constructed from the models we present for this data for yourself.} can input the information necessary for creating a lineup using network data simulated from models M1 through M7.
%\begin{enumerate}
%\item Choose to simulate one plot to change the alternative model in the lineup, or simulate $P-1$ plots to change %the null model in the lineup
%\item Choose the model from which to simulate and choose the wave of data to simulate. 
%\item If M1 is selected in (2), select whether to alter the density ($\beta_1$) or the reciprocity ($\beta_2$) %parameter. 
%\item Choose the size of the lineup, $P$, the magnitude of the effect size, a random seed for reproducibility, and a %layout algorithm to use for the node-link diagrams. There is also a checkbox if the user wishes the nodes to be %colored by the size of the connected component to which they belong. 
%\end{enumerate}
The plots in the lineup that are not specified by the user contain data simulated from model M1. %estimates of the rate parameters, and $\beta_1$ and $\beta_2$ as given in Table~\ref{tab:fittedvalues}. 

% latex table generated in R 3.5.2 by xtable 1.8-3 package
% Mon Mar  4 14:49:43 2019
\begin{table}[ht]
\centering
\begin{tabular}{llrrr}
  \hline
Parameter & Condition & Easy Value & Medium Value & Hard Value \\ 
  \hline
\hline
$\beta_1$ & neg & -7.354 & -6.6187 & -5.883 \\ 
   & pos & -3.922 & -4.1674 & -4.412 \\ 
  $\beta_2$ & neg &  0.000 & 0.0005 & 0.049 \\ % -17.249 & -10.3497 & -3.450 \\ 
   & pos & 7.340 & 6.8504 & 6.361 \\  %10.350 & 6.8998 & 5.175 \\ 
  $\beta_3$ & neg & -17.249 & -10.3497 & -3.450 \\ %8.351 & 6.6806 & 5.010 \\ 
   & pos & 10.350 & 6.8998 & 5.175 \\   %6.681 & 5.0105 & 3.340 \\ 
  $\beta_5$ & neg & -30.272 & -20.1817 & -10.091 \\ %0.000 & 0.0005 & 0.049 \\ 
   & pos &  20.182 & 17.6590 & 16.145 \\ %7.340 & 6.8504 & 6.361 \\ 
   \hline
\hline
$\beta_4$ & pos &  6.681 & 5.0105 & 3.340  \\ % 5.316 & 3.9872 & 3.323 \\ 
   & reverse & 8.351 & 6.6806 & 5.010 \\  %5.316 & 3.9872 & 3.323 \\ 
  $\beta_6$ & pos & 5.316 & 3.9872 & 3.323 \\  %-30.272 & -20.1817 & -10.091 \\ 
   & reverse & 5.316 & 3.9872 & 3.323 \\ %20.182 & 17.6590 & 16.145 \\ 
   \hline
\end{tabular}
\caption{All conditions used for our experiment. For parameters $\beta_1, \beta_2, \beta_3,$ and $\beta_5$, M1 served as the null model. For $\beta_4$ and $\beta_6$, null model M1 and the alternative model (M4 or M6) switch roles in the reversed lineups, i.e. five plots show data simulated from the alternative model and only one plot shows data from M1.} 
\label{tab:experdetail}
\end{table}

Using the Shiny application, we settled on six parameter values to test for each of the six effects, $\beta_1, \dots \beta_6$. All values of the parameters used in the experiment are given in Table~\ref{tab:experdetail}. In the case of both $\beta_4$ and $\beta_6$, we could not determine any negative parameter values that made the networks simulated from M4 and M6 look different than the networks simulated from model M1. We hypothesize that this is due to negative effects \textit{removing} visually interesting structural elements as opposed to \textit{adding} noticeable structural elements.  Since we could not detect the effects, we decided that the participants in our experiment would also not be able to. So, instead of testing the negative values of these effects, we use a different scenario: we place five simulations from model M4 or M6 (with positive values of the parameter) with one simulation from model M1 in a lineup. We will refer to this as the ``reverse" lineup scenario. We used the reverse scenario to determine if the perception of the effect size is symmetric: if an effect is noticed $p$\% of the time at value $\beta_k = \beta_{k_0}$ when \textit{one} simulation from the corresponding model is placed among \textit{five} null plots from model M1, then when \textit{five} simulations from the model with $\beta_k = \beta_{k_0}$ are put in a lineup with \textit{one} simulation from model M1, the plot from the simpler model should be noticed about $p$\% of the time as well. 

<<getexperdetail, results='asis', eval = FALSE>>=

modelData_sig <- turk22 %>% 
  mutate(type2 = ifelse(sign == 0, sign(initialEst), sign),
         type2 = ifelse(type == "one", type2, -1)) %>% 
  filter(pic_id != 3132) 
modelData_sig %>% group_by(test_param, type2, difficulty, size) %>%
  count() %>% ungroup() %>%
  spread(difficulty, size) -> step1
step1 %>% select(1:4) %>% filter(!is.na(`1`)) -> step2a
step1 %>% select(c(1:3,5)) %>% filter(!is.na(`2`)) -> step2b
step1 %>% select(c(1:3,6)) %>% filter(!is.na(`3`)) -> step2c
experdet <- step2a %>% mutate(easy = `1`, medium = step2b$`2`, hard = step2c$`3`) %>% select(-c(`1`, n))
experdet$test_param <- as.factor(experdet$test_param)
levels(experdet$test_param) <- paste0("$\\beta_", c(1,2,3,5,4,6), "$")
experdet$test_param <- factor(experdet$test_param, levels=paste0("$\\beta_", c(1,2,3,5,4,6), "$"))
experdet <- experdet %>% arrange(test_param)
experdet$test_param <- as.character(experdet$test_param)
experdet$test_param[c(2,4,6,8,10,12)] <- NA
experdet$type2 <- c(rep(c("neg", "pos"), 4), rep(c("regular", "reverse"), 2))
names(experdet) <- c("Parameter", "Condition", "Easy Value", "Medium Value", "Hard Value")

# xtable::print.xtable(xtable::xtable(experdet, caption = "All conditions used for the MTurk experiment. For parameters $\\beta_1, \\beta_2, \\beta_3,$ and $\\beta_5$ M1 served as null model. For $\\beta_4$ and $\\beta_6$, null model M1 and the alternative model switch roles in the reversed lineups, i.e. five plots show data simluated from the laternative model and only one plot shows data from M1.", label = "tab:experdetail", digits = c(0,0,0,3,4,3)), include.rownames=FALSE, 
#       hline.after = c(-1,0,0,8,8,12), sanitize.text.function = function(x){x})
@

%\subsection{Execution}


\section{Results}\label{sec:res}

In this section, we present the results from our experiment and compare them to traditional statistical tests where applicable.  Because each lineup shown to participants has only six plots, the probability of picking the data by chance is high at 1 in 6, but if many independent viewers pick out the data from the nulls, the evidence against the null hypothesis becomes stronger. The $p$-values from the lineups were calculated using the \texttt{vinference} package by \citet{vinference}. This package contains methods to calculate \textit{visual distributions} for lineup experiment data.  The distribution depends on the number of evaluations of a plot, $L$, the size of the lineup, $P$, and the lineup scenario, which here is that each lineup containing the same data and the same set of null plots is shown to $L$ independent observers. The visual inference family of distributions is similar to the binomial distribution, but takes the dependency among the $P$ plots in a single lineup shown to multiple viewers into account. Suppose that out of $L$ observers of a lineup of size $P$, there are $\ell$ observers who pick out the alternative data plot. Then the corresponding $p$-value from the visual distribution gives the probability that $\ell$ or more out of $L$ independent observers would pick out the alternative data plot by chance. 

\subsection{Significance Testing}\label{sec:sigtest}

<<sigtestWald, eval = FALSE, echo = FALSE>>=
SenBasic <- getEffects(senateSiena)
Senjtt_p <- includeEffects(SenBasic, "jumpXTransTrip", include = TRUE, type = "eval", interaction1 = "party", character = TRUE)
myalg <- sienaAlgorithmCreate( projname = Sys.time() , n3 = 1000)
fits <- siena07(myalg, data = senateSiena, effects = Senjtt_p, returnDeps = TRUE,
                                      batch=TRUE, verbose = FALSE, silent = TRUE)
thet <- fits$theta
sig <- fits$covtheta
Wald.RSiena(A = c(0,0,1), fits)
th <- c(0,0,1) %*% thet
covmat <- c(0,0,1) %*% sig %*% c(0,0,1)
csq <- drop(th %*% solve(covmat) %*% th)
1 - pchisq(csq, 1)
@

For CTMC models, significance tests of the parameters are available in \texttt{RSiena}. There are $t$-type and Wald-type tests for a single parameter and for multiple parameters. The $t$-type test statistic is simply the parameter estimate divided by its standard error, and compared to a standard normal distribution \citep{RSienamanual}. The Wald-type test statistic for a single parameter, $\beta_k$ is 
\begin{equation}\label{eq:wald1}
\frac{(\hat{\beta_k})^2}{var(\hat{\beta_k})} \sim \chi^2_1,
\end{equation}
\citep{RSienamanual}. %Testing the significance of multiple parameters depends on the hypothesis we wish to test, and an $H \times K$ matrix, $\mathbf{A}$, must be appropriately designed to test the $H$ hypotheses of interest. The null hypothesis is that $\mathbf{A}\boldsymbol{\beta} = \mathbf{0}$, and the test statistic is    
% \begin{equation}\label{eq:wald2}
% (\mathbf{A}\boldsymbol{\hat{\beta}})' \hat{\Sigma}^{-1} \mathbf{A}\boldsymbol{\hat{\beta}} \sim \chi^2_H,
% \end{equation}
% where $\hat{\Sigma}$ is the estimated covariance matrix of $\boldsymbol{\beta}$. 
Both parameters we test for significance using the lineup protocol, $\beta_3$ and $\beta_4$, were determined to be statistically significant using Equation~\ref{eq:wald1} with $p$-values given in Table~\ref{tab:sigtesting}. For the visual significance test, if the number of participants who pick out the alternative plot results in a $p$-value less than 0.05, we reject the null hypothesis that the true value of the additional parameter, either $\beta_3$ or $\beta_4$, is equal to zero. If the null hypothesis is not rejected, then there is evidence that the additional parameter does not affect the overall structure of the network even though it was found to be sigificant with the Wald-type test. The $p$-values from the visual distribution for the significance tests of $\beta_3, \beta_4$ are given in Table~\ref{tab:sigtesting}. The results for each lineup, whether to reject or fail to reject the null hypothesis, vary, with one of three lineups from each parameter  group resulting in rejection of the null hypothesis.   %\hh{XXX how do we interpret these  visual results? Include the six lineups in an appendix and refer to them from here.} \st{XXX should be taken care of in next paragraph now.}

<<sigtesting, results='asis'>>=
turk22_sig <- turk22 %>% 
  mutate(type2 = ifelse(sign == 0, sign(initialEst), sign),
         type2 = ifelse(type == "one", type2, -1)) %>% 
  filter(param_value == 1)
summ_sig <- turk22_sig %>% group_by(pic_id, test_param) %>% 
  summarize(npick = sum(datapick), n = n(), 
            pv = map2_dbl(npick, n, pV, m=6, scenario=3))
names(summ_sig) <- c("Lineup ID", "parameter", "\\# ID", "Total Views", "p-value")
summ_sig$parameter <- as.factor(summ_sig$parameter)
levels(summ_sig$parameter) <- c("$ \\beta_3 $", "$ \\beta_4$ ")
xtable::print.xtable(xtable::xtable(summ_sig, caption = "Experiment results for the two parameters for which we performed significance tests. \\# ID indicates the number of participants who identified the alternative data plot. There were three lineups for each parameter, so there are three results for each plot.", label = "tab:sigtesting", digits = c(0,0,0,0,0,5)), include.rownames = FALSE, sanitize.text.function=function(x){x})
@

 %Using these $p$-values, all but one lineup results in a rejection of the null hypothesis at Type-I error rate of $\alpha = 0.05$. 


<<lineup3132, fig.height=4, fig.cap="Lineup 3132, which led to rejection of the null hypothesis that $\\beta_3$ = 0. The network simulated from model M3 is found in panel $\\sqrt{16} - 1$, and the remaining panels show networks simulated from model M1.">>=
dat <- read_csv("data/jttp_pos_hard_2.csv")
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size =1) + 
        ThemeNet + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

<<lineup3133, fig.height=4, fig.cap="Lineup 3133, which resulted in failure to reject the null hypothesis that $\\beta_3$ = 0. The network simulated from model M3 is found in panel $\\sqrt{25} - 4$, and the remaining panels show networks simulated from model M1. 16 of 27 viewers picked plot two as most different.">>=
dat <- read_csv("data/jttp_pos_hard_3.csv")
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size =1) + 
        ThemeNet + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

The lineup for significance testing of $\beta_3$ which resulted in a very small $p$-value and rejection of the null hypothesis is shown in Figure~\ref{fig:lineup3132}. Another significance lineup for model M3, which resulted in failure to reject the null hypothesis, is shown in Figure~\ref{fig:lineup3133}. When viewing Figure~\ref{fig:lineup3132}, 26 of 31 viewers chose the alternative plot from M3, while only 2 of 27 chose the alternative plot from M3 when viewing Figure~\ref{fig:lineup3133}. The most common choice in the latter was panel two, which 16 of 27 viewers chose as the most different due to its large connected component that makes it seem more complex than the others. In viewing these two lineups, it is evident that there is a large amount of variability in networks simulated from CTMC models. The variability in significance test results is introduced through the null plots generated from M1. It is difficult to see that five of the six networks come from the same model when they all look very different.  In addition, the small number of null plots do not give the viewer as complete of a view of the null model as the usual 19 null plots would. In both lineups in Figures~\ref{fig:lineup3132} and ~\ref{fig:lineup3133}, most participants chose the plot with the largest connected component as the most different. 

The results of the significance tests given in Table~\ref{tab:sigtesting} for $\beta_3$ and $\beta_4$ are not definitive. For the test of $\beta_3$, two of the three tests are not significant, while the third is highly significant. For the test of $\beta_4$, one test is significant, one is decidedly not significant, and the third is significant at the level of 0.10. Thus, unlike with the Wald-type tests described at the beginning of this section, we cannot decisively reject or to fail to reject the null hypothesis that the parameter value is 0. %We include all of the lineups shown to our participants in the appendix. 

\subsection{Goodness-of-Fit Testing}

Goodness-of-fit testing for network models is notoriously difficult. Most network models, other than the most simple, lack the necessary asymptotics for developing goodness-of-fit methods \citep{goldenberg09}. Some simulation-based methods have been developed using what auxiliary statistics such as the indegree or outdegree distribution on the nodes \citep{RSienamanual}. In \texttt{RSiena}, 
@
%A plot comparing the data to the simulations is also considered, and a similar plot is shown in Figure~\ref{fig:gofsiena} for the outdegree distribution of small data set, shown in the points and connected lines, with the simulated values of $\mathbf{u}_d$ shown in box plots and overlaid violin plots.
%The \texttt{RSiena} software also provides a Rao score-type test for goodness-of-fit for assessing one or more parameters, the test statistic of which is compared to a Chi-square distribution with $H$ degrees of freedom, where $H$ has the same definition as in Section~\ref{sec:sigtest}. For full detail on the score-type test, see \citet{scoretest}. 
\noindent This goodness-of-fit test is very limited because it only considers one measure on the data and simulations at a time. To assess goodness-of-fit on the whole network, many auxiliary statistics need to be considered. Thus, by using visual inference instead of more traditional statistical methods, we construct a more holistic goodness-of-fit test. 

Using the lineup protocol, we show each Amazon Mechanical Turk worker the data once, in a lineup with five other plots of simulated data from one of the models we chose. We examined four different models, M3, M4, M5, and M7, and examined three repetitions of each, for a total of 12 goodness-of-fit lineups shown to participants. In each lineup, the null model is one of the four models and the alternative model is the true, unknown model that generated the senate network data. %The hypotheses for our goodness-of-fit tests are: 
% \begin{description}
% \item $H_0$:  The senate network data come from the null model, M$i$.
% \item $H_A$: The senate network data do not come from the null model. 
% \end{description}
If a lineup viewer picks out the data among the five simulations from the null model, it is evidence against the null hypothesis. On the contrary, if the lineup viewer picks one of the null plots, that is evidence in favor of the null hypothesis. Results from our MTurk goodness-of-fit plots are provided in Table~\ref{tab:gofstats}. 

<<goftab, cache = TRUE, results='asis'>>=
turk22_gof <- read_csv("data/turk22-gof.csv")
turk22_gof_stats <- turk22_gof %>% group_by(pic_id) %>% 
  summarize(npickdata = sum(datapick), total = n(), 
          pvinf = map2_dbl(npickdata, total, pV, m = 6, scenario = 3))
turk22_gof_stats %>% separate(pic_id, into = c("discard", "model", "rep"), sep = c(2,3)) -> print_gof_stats
print_gof_stats %>% select(-discard) %>% mutate(model = paste0("M", (as.integer(model))))-> print_gof_stats
names(print_gof_stats) <- c("Model", "Replicate", "Data Picks", "Total Viewers", "p-value")
print_gof_stats$Model[c(2:3,5:6,8:9,11:12)] <- ""
#print_gof_stats$Model[2] <- "jtt party"
#print_gof_stats$Model[5] <- "jtt sex"
#print_gof_stats$Model[8] <- "stt party"
print_gof_stats$`p-value` <- ifelse(print_gof_stats$`p-value` < .0001, "< 0.0001", sprintf("%.4f", print_gof_stats$`p-value`))
print(xtable::xtable(print_gof_stats, label = "tab:gofstats", caption = "An overview of the results from the 12 goodness-of-fit lineup tests.", digits = 4,
                      align="rlrrrr"), caption.placement = "bottom", include.rownames = FALSE, hline.after = c(-1,0,0,3,6,9,nrow(print_gof_stats)))
@

The $p$-values were again computed using the \texttt{vinference} package by \citet{vinference}. In all lineups except replicate 2 of M4, the null hypothesis that the data were generated by the model is rejected. Thus, none of the models we have chosen are a good fit to the data. The lineup that resulted in a failure to reject the null hypothesis is shown in Figure~\ref{fig:failgof}. The null model in this lineup is M4, and the senate data is shown in panel number $3^2 - 7$. However, the panel most participants chose was number four, and the most common reasoning for that choice was that it had the most simple structure. Some of the other panels, such as three and six, in Figure~\ref{fig:failgof} have large connected components that are similar in size to the connected component of the data plot shown in panel two. Thus, model M4 is sometimes capable of capturing the network structure of the senate collaboration data.

<<failgof, fig.height=4, fig.cap="The goodness-of-fit lineup which resulted in failure to reject the null hypothesis. The null model for this lineup is M4. Only 7 of 20 viewers of this lineup selected the data plot as the most different from the others. The most commonly chosen panel was number four, which has a relatively simple structure compared to the other panels.">>=
load("data/se112adjmat.RDS")
wave2 <- data.frame(se112adj)
senators <- data.frame(id = colnames(wave2), number = 1:155)
wave2$from <- colnames(wave2)
wave2 <- wave2 %>% gather(to, val, Alan.Stuart.Franken:William.Cowan) %>% filter(val > 0)
wave2 <- merge(wave2, senators, by.x = "from", by.y = "id", all = T)
wave2$from <- senators$number[match(wave2$from, senators$id)]
wave2$to <- senators$number[match(wave2$to, senators$id)]
wave2$from <- paste0("V", wave2$from)
wave2$to <- ifelse(is.na(wave2$to), NA, paste0("V", wave2$to))
wave2 <- wave2 %>% mutate(sim = 1001, model = "data", wave = 1) %>% select(-c(val, number))
dat <- read_csv(paste0("data/jtts_gof_9_2.csv"))
datplot <- unique(dat$ord[dat$sim == 1001])
dat <- dat %>% filter(sim != 1001) %>% 
      bind_rows(wave2) %>% 
      mutate(ord = ifelse(is.na(ord), datplot, ord))
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size = .5) + 
        ThemeNet + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

The smallest $p$-value for one of the goodness-of-fit lineups was for the third replicate of the null model M4. This result contrasts with our previous finding that the only lineup to fail to reject the null was also when the null model was M4. This lineup is shown in Figure~\ref{fig:mostsiggof}. In the remaining replicate of M4 as the null model, 13 of 16 viewers identified the data plot, corresponding to a $p$-values of less than $0.0001$, just like the third replicate. This variability in results is similar to the variability we found in Section~\ref{sec:sigtest}, and does not provide us with a clear cut result from the goodness-of-fit tests. For model M4, we can neither reject nor fail to reject the null hypothesis that the data come from model M4. This is evidence that the goodness-of-fit of network models cannot always be determined by one dimensional derived features.%, such as $p$-value shown on the $x$-axis in Figure~\ref{fig:gofsiena}.   %This leads us to believe that the high $p$-value observed in the second replicate is more of a fluke. %This demonstrates some of the problems inherent with network models, such as model degeneracy, as discussed in \citet{degeneracy}. Network models 
%Furthermore, for the first lineup replicate with model M5 as the null model, only 9 of 21 participants were able to pick out the data, but the Visual distribution $p$-value is less than 0.05. %This is much higher than the corresponding Binomial probability of $Pr(X = 9|n = 21, p = \frac{1}{6}) = \binom{21}{9}(\frac{1}{6})^9(\frac{5}{6})^{21-9} \approx 0.003$, but we are not sure that having fewer than half of our participants identify the data plot is a truly significant result. We could move the threshold from $\alpha = 0.05$ to $\alpha = 0.01$ or $\alpha = 0.001$, but more work needs to be done to determine which, if any of these, is appropriate.  

<<mostsiggof, fig.height = 4, fig.cap="The lineup resulting in the smallest $p$-value rejecting the null hypothesis. Surprisingly, this another repetition for M5 as the null model. The data are in panel 6.">>=
dat <- read_csv(paste0("data/jtts_gof_9_3.csv"))
datplot <- unique(dat$ord[dat$sim == 1001])
dat <- dat %>% filter(sim != 1001) %>% 
      bind_rows(wave2) %>% 
      mutate(ord = ifelse(is.na(ord), datplot, ord))
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size = .5) + 
        ThemeNet + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@


For the other models for which we tested goodness-of-fit, however, we \textit{do} have significant evidence from all three replicates to reject the null hypothesis that the null model generated the data. For models M3, M5, and M7, these goodness-of-fit tests have rejected the null hypotheses that the senate data come from these models. We hypothesized that the model with the most effects, M7, would be the best fit. However, as shown in Figure~\ref{fig:gofm71}, the model does not capture the overall structure very well at all. The rest of the goodness-of-fit lineups as shown to participants are provided in the appendix. 

%\hh{XXX we need to go throught the logic of the argument below.}\st{ XXX I reformulated the discussion per our convo today.  }

<<gofm71, fig.height=4, fig.cap="One repitition of a goodness-of-fit lineup testing model M7. The senate data are shown in panel two, and it is evident that none of the other five panels, which show data simulated from model M7, come close to creating the large connected component that is central to the structure of the senate data.">>=
dat <- read_csv(paste0("data/bigmod_gof_9_1.csv"))
datplot <- unique(dat$ord[dat$sim == 1001])
dat <- dat %>% filter(sim != 1001) %>% 
      bind_rows(wave2) %>% 
      mutate(ord = ifelse(is.na(ord), datplot, ord))
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size =1) + 
        ThemeNet + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

We believe this goodness-of-fit testing method holds promise for the future of network analysis. The participants in our experiments are very good overall at picking out the data when it is noticeably different from the null plots in the lineups. In addition, as in replicate three for null model M4, when the null plots contain similarly sized structures as the data plot, our participants have a hard time distinguishing the data. We believe that running these tests multiple times using several different sets of null models to adequately explore the possible structures generated by the models \st{results in} a more comprehensive goodness-of-fit test for network models. 

\subsection{Visual Power}

The results from the visual power part of our experiment are shown in Figure~\ref{fig:predictglmm}. On the $x$ axis, we plot the value of the parameter of interest, and on the $y$ axis, the proportion of times the alternative data plot was picked out from each lineup. The results are split into groups based on the value of the parameter and the lineup type. We can see clear patterns in the added parameters $\beta_3, \dots, \beta_6$: as the parameter value approaches 0, fewer participants identified the alternative plot. Similarly, as $\beta_1, \beta_2$ approach their estimated values $\hat{\beta_1}, \hat{\beta_2}$, fewer people are able to identify the alternative plot. 


<<readturk22, fig.height=6, fig.cap="A summary of some results from the Amazon mechanical turk study. On the $x$-axis, the value of the parameter changed in the model of interest, and on the $y$-axis, the proprotion of times turkers identified the different data plot. A simple linear regression is fit to each group. The pattern as the parameter value approaches zero is clear in all conditions.", eval = FALSE>>=
turk22 %>% mutate(type2 = ifelse(sign == 0, sign(initialEst), sign),
                       type2 = ifelse(type == "one", type2, -1)) %>% 
  group_by(pic_id, size, test_param, sign, type2,alt_model) %>% 
  summarize(datapick = mean(datapick)) %>%
  filter(pic_id != 3132) %>% 
  ggplot(aes(x = size, y = datapick, colour=factor(type2))) + 
  geom_point() +
  scale_color_brewer(palette = "Dark2", name = "Lineup Type") + 
  facet_wrap(~test_param, scales="free", labeller = 'label_both') +
  geom_smooth(se=FALSE, method="lm") + 
  ThemeNoNet + 
  theme(legend.position = 'bottom')
@

We model identification of the alternative data in the lineup by the parameter of interest, the effect size, and the lineup type with a generalized linear mixed model (GLMM) that provides us with an estimate of the power of the visual significance test. The visual power is the probability of detecting the parameter in a lineup of size six. The response variable in our model, $X_{k\ell qr}$, is binary, indicating whether participant $r$ picked the alternative data plot in lineup type $\ell$, rep $q$, for effect $k$, and it follows a Bernoulli distribution with probability $\pi_{klqr}$. We model $\text{logit}(\pi_{klqr})$ as a sum of the effects for parameter and lineup type, plus random effects for each lineup, $\delta_{klq}$, and for each participant, $\epsilon_r$. There is one continuous covariate $v$, which is the centered and scaled size of the effect of interest from which the alternative data were simulated, the values of which are labeled ``easy", ``medium", and ``hard" in Table~\ref{tab:experdetail} according to how difficult we thought the Turk participants would find each lineup. The complete hierarchical model is given in Equation~\ref{eq:glmm}, where $k \in \{1, 2, 3, 4, 5, 6\}$ corresponds to the effects $\beta_1,\dots, \beta_6$, respectively, $\ell \in \{-1,1\}$, and $q \in \{1,2,3\}$. 

\begin{align}
\begin{split}
X_{klqr} &\sim \text{Bernoulli}(\pi_{klqr}) \\
\text{logit}(\pi_{klqr}) & = \eta_{kl} + \gamma_{kl} v + \delta_{klq} + \epsilon_{r} \label{eq:glmm} \\
\delta_{klq} & \stackrel{iid}{\sim} N(0, \sigma^2_{\delta}) \\
\epsilon_{r} & \stackrel{iid}{\sim} N(0, \sigma^2_{\epsilon})
\end{split}
\end{align} 

% \hh{
% $\ell$ encompasses all 12 combinations of parameter and positive/negative direction of the effect size.
% }

% \begin{align}
% \begin{split}
% Y_{i\ell m} &\sim \text{Bernoulli}(\pi_{i\ell m}) \\
% \text{logit}(\pi_{i\ell m}) & = \mu + \alpha_{i} + \theta\mathbb{I}(x_{\ell 1} = 1) + \gamma x_{\ell 2} + \\ & (\alpha\theta)_i \mathbb{I}(x_{\ell 1} = 1) + (\alpha\gamma)_i x_{\ell 2} + (\theta\gamma)\mathbb{I}(x_{\ell 1} = 1)x_{\ell 2} + \\ & (\alpha\theta\gamma)_i\mathbb{I}(x_{\ell 1} = 1) x_{\ell 2} + \delta_{\ell} + \epsilon_{m} \label{eq:glmm} \\
% \delta_{\ell} & \sim N(0, \sigma^2_{\delta}) \\
% \epsilon_{m} & \sim N(0, \sigma^2_{\epsilon})
% \end{split}
% \end{align}

%\hh{XXX define all of the parameters above XXX}
%\st{
%This model estimates, as a function of the parameter size, the expected value of $\text{logit}(\pi_{ijkm})$ for each of the 12 lineup conditions, repitions, and participants. The inverse logit of the estimates are the prediction lines drawn in Figure~\ref{fig:predictglmm}.

We fit the hierarchical model with \texttt{glmer} from the \texttt{lme4} package \citep{lme4}. The parameter estimates, standard errors, $p$-values, and the odds ratio multipliers from the model  are summarized in Table~\ref{tab:glmmests}. For each combination of parameter and lineup type, the expected value of the link function for a new lineup and a new observer with parameter value $v$ is 
\begin{equation}
E[\text{logit}(\pi_{kl})] = \eta_{kl} + \gamma_{kl}v
\end{equation}
and the corresponding probability of picking out the alternative data plot is 
\begin{equation}
\pi_{kl} = \frac{\exp\{\alpha_{kl} + \gamma_{kl}v\}}{1 + \exp\{\alpha_{kl} + \gamma_{kl}x\}}
\end{equation}

%The baseline condition for this model against which all other experimental conditions are compared is the first condition in Table~\ref{tab:experdetail}, for parameter $\beta_1$ and lineup type -1. The expected value of the link function for this scenario with $\beta_1 = x_{12}$ is $\mu + \gamma x_{12}$, and the expected probability that a new observer will pick out the data plot in a new lineup generated from this scenario is $\frac{\exp(\mu + \gamma x_{12})}{1+\exp{\mu + \gamma x_{12}}}$. Similarly, for a new lineup generated from the second condition in in Table~\ref{tab:experdetail}, with $\beta_1 = x_{12}$, and lineup type 1, the expected value of the link function is $\mu + \theta + (\gamma + (\theta\gamma))x_{12}$ and corresponding probability a new observer will pick out the data plot in a new lineup is $\exp\{\mu + \theta + (\gamma + (\theta\gamma))x_{12}\} / 1+\exp\{\mu + \theta + (\gamma + (\theta\gamma))x_{12}\}$. 

% For any of the remaining parameters, $\beta_2, \dots, \beta_6$, with lineup type -1, the expectation of the link function as a function of the parameter value, $x_{\ell 2}$ is: 
% \begin{equation}\label{eq:predneg1}
% \mu + \alpha_{i} +  (\gamma + (\alpha\gamma)_i)x_{\ell 2}  
% \end{equation}
% where $i \in \{1,2,3,4,5\}$ corresponds to $\beta_3, \beta_4, \beta_2, \beta_6, \beta_5$, respectively. Similarly, for lineup type 1, the expectation of the link function as a function of the parameter value is: \begin{equation}\label{eq:predpos1}
% \mu + \alpha_{i} + \theta + (\alpha\theta)_i  +  (\gamma + (\alpha\gamma)_i + (\gamma\theta) + (\alpha\theta\gamma)_i)x_{\ell 2}  
% \end{equation}
% 
% The corresponding expected probability that a new observer viewing a new lineup from this scenario is: 
% \begin{equation}
% \frac{\exp\{\mu + \alpha_{4} + \theta + \gamma + (\alpha\theta)_4 + (\alpha\gamma)_4  + (\theta\gamma) + (\alpha\theta\gamma)_4\}}{1 + \exp\{\mu + \alpha_{4} + \theta + \gamma + (\alpha\theta)_4 + (\alpha\gamma)_4  + (\theta\gamma) + (\alpha\theta\gamma)_4\}}
% \end{equation}

<<glmmres>>=
load("data/finalglmm31-2.RDA")
mod <- model2randomscalesize
res <- summary(mod)[[10]]
ests <- as.numeric(round(res[,1], 4))
oddsidx <- which(exp(ests) < .0001)
oddsidx2 <- which(exp(ests) > 1000000)
odds <- round(exp(ests), 4)
odds[oddsidx] <- "$<$0.0001"
odds[oddsidx2] <- "$>$1e+06"
se <- as.numeric(round(res[,2], 4))
pvalidx <- which(res[,4] < .0001)
pval <- sprintf("%.4f",round(res[,4],4))
pval[pvalidx] <- "$<$0.0001"
sigma_epsilon <- summary(model2randomscalesize)[[13]]$nick_name[[1]]
sigma_delta <- summary(model2randomscalesize)[[13]]$pic_id[[1]]
@


<<predictglmm, fig.height=6, fig.cap='Predictions from our GLMM given in Equation~\\ref{eq:glmm}. For new observers of new lineups, the lines show the expected probability of detecting the alternative data in a lineup of size 6 as a function of the parameter value. The proportions detected by our Turk participants for each lineup group are shown by the points, with the probability of picking out the data plot at random shown by a horizontal line at 1/6. The lineup marked as ``outlier" was removed from modeling. Estimated parameter values shown by vertical dotted lines.'>>=
newdata <- read_csv("data/newdata_pred_glmm.csv")
newdata$param <- newdata$test_param
newdata$param <- as.factor(newdata$test_param)
levels(newdata$param) <-  c(1,3,4,2,6,5) 
newdata$param <- as.integer(as.character(newdata$param))
plotdata <- turk22 %>% mutate(type2 = ifelse(sign == 0, sign(initialEst), sign),
                       type2 = ifelse(type == "one", type2, -1)) %>% 
  group_by(pic_id, size, test_param, sign, type2,alt_model) %>% summarize(
    datapick = mean(datapick)
)
plotdata$param <- plotdata$test_param
plotdata$param <- as.factor(plotdata$param)
levels(plotdata$param) <- c(1,3,4,2,6,5) 
plotdata$param <- as.integer(as.character(plotdata$param))
labdat <- data_frame(test_param = "jttp", param = 3, type2 = -1, x = -5, y = .9, label = "outlier")
betahat <- tibble(param = 1:6, size = c(-4.903, 4.893, -3.45, 3.34, 10.091, 1.329))

ggplot() + 
  geom_vline(data = betahat, aes(xintercept = size), linetype = "dotted", color = "grey60") + 
  geom_text(data = betahat, aes(x = size, y = Inf, label = paste0("hat(beta)[", param, "]")), parse = T, color = "grey60", hjust = -.5, vjust = 1.1) + 
  geom_line(data = newdata, aes(x = size, y = predictfinal, color = as.factor(type2))) + 
  geom_point(data = plotdata, aes(x = size, y = datapick, color = as.factor(type2))) + 
  #geom_text(data = plotdata, aes(x = size, y = datapick, color = as.factor(type2), label = pic_id)) +
  geom_text(data = labdat, aes(x = x, y=y, label = label, color = as.factor(type2)),
            show.legend = F, vjust = 2) + 
  geom_hline(yintercept = 1/6, linetype='dashed') +
  scale_color_brewer(palette = "Dark2", name = "Lineup Type") + 
  facet_wrap(~param, scales = 'free_x', 
             labeller = label_bquote(beta [.(param)])) + 
  labs(x = "Parameter value", y = "Expected probability of detection in a lineup of size 6") + 
  ThemeNoNet + 
  theme(legend.position = 'bottom')


#vals <- predict(model1, type = "response")
#modelData_sig$prediction <- vals
#ggplot(data = modelData_sig) + 
#  geom_point(aes(x = size, y = prediction, color = as.factor(type2))) + 
#  facet_wrap(~test_param, scales = "free")
@

In Figure~\ref{fig:predictglmm}, we see a clear trend in $\beta_2$ through $\beta_6$ that as the parameter value approaches zero from either side, the probability of picking the data plot in a lineup of size six decreases. This supports our hypothesis shown in Figure~\ref{fig:hypothesis}. For $\beta_3$ and $\beta_5$, the slope of the fitted line is \textit{much} steeper for positive values of the parameter than for negative values, meaning that our participants perceived differences more often for positive parameter values than for negative parameter values. This finding is similar to that of \citet{corrviz}, who found that people detect positive correlations better and at lower values than negative correlations. 

<<morepredicts>>=
alphas <- (coef(mod)[[1]][,-1] %>% unique())[1:12] 
betas <- (coef(mod)[[1]][,-1] %>% unique())[13:24] 
names(betas) <- str_replace(str_replace(names(betas), "lps_param", ""), ":centersize", "")
names(betas) <- str_replace(names(betas), ".-1", "neg")
names(betas) <- str_replace(names(betas), ".1", "pos")
names(alphas) <- str_replace(names(alphas), "lps_param", "")
names(alphas) <- str_replace(names(alphas), ".-1", "neg")
names(alphas) <- str_replace(names(alphas), ".1", "pos")
source("code/newpredictions.R")
newdata <- newdata3
newdata$param <- newdata$test_param
newdata$param <- as.factor(newdata$test_param)
#levels(newdata$param) <-  c(1,3,4,2,6,5) 
#newdata$param <- as.integer(as.character(newdata$param))
plotdata <- turk22 %>% mutate(type2 = ifelse(sign == 0, sign(initialEst), sign),
                       type2 = ifelse(type == "one", type2, -1)) %>% 
  group_by(pic_id, size, test_param, sign, type2,alt_model) %>% summarize(
    datapick = mean(datapick)
)
plotdata$param <- plotdata$test_param
plotdata$param <- as.factor(plotdata$param)
#levels(plotdata$param) <- c(1,3,4,2,6,5) 
#plotdata$param <- as.integer(as.character(plotdata$param))
labdat <- data_frame(test_param = "jttp", param = 3, type2 = -1, x = -5, y = .9, label = "jttp_neg_hard_2")
@

<<recipzoom, fig.height=3, fig.cap='The top middle panel of Figure~\\ref{fig:predictglmm} expanded to show greater detail. The square root of the parameter value is shown on the $x$-axis. For this parameter, as its value approaches zero, the probability of identifying the alternate data model decreases, then increases, which is noticeably different from the pattern exhibited by the others. Again, a horizontal line is drawn at 1/6, the chance of selecting the data plot at random.'>>=
newdatabeta2 <- newdata %>% filter(test_param == "recip", category == "inside") %>% mutate(param2 = "beta[2]", size2 = sqrt(size)) %>% as.tibble()
plotdatabeta2 <- plotdata %>% filter(test_param=="recip")%>% mutate(param2 = "beta[2]", size2 = sqrt(size)) %>% as.tibble()
newdatabeta22 <-  newdata %>% filter(test_param == "recip", category == "outside")%>% mutate(param2 = "beta[2]", size2 = sqrt(size)) %>% as.tibble()
newdatabeta23 <- newdata %>% filter(test_param == "recip", category == "outside2")%>% mutate(param2 = "beta[2]", size2 = sqrt(size)) %>% filter(!is.nan(size2)) %>% as.tibble()
interceptlabel <- tibble(x = sqrt(4.893), y = .05, label = "hat(beta)[2]")
ggplot() + 
  geom_line(data = newdatabeta2, aes(x = size2, y = predictfinal, color = as.factor(type2)), size = 1.25) + 
  geom_point(data = plotdatabeta2, aes(x = size2, y = datapick, color = as.factor(type2)), size = 2) + 
  geom_line(data =newdatabeta22, aes(x = size2, y = predictfinal, group = as.factor(type2)), color = "gray60", alpha = .7) +
  geom_line(data = newdatabeta23, aes(x = size2, y = predictfinal, group = as.factor(type2)), color = "gray60", alpha = .7) +
  #geom_text(data = plotdata, aes(x = size, y = datapick, color = as.factor(type2), label = pic_id)) +
  #geom_text(data = labdat, aes(x = x, y=y, label = label, color = as.factor(type2)),
  #          show.legend = F) + 
  geom_vline(xintercept = sqrt(4.893),  linetype = "dotted", color = "grey60") + 
  geom_text(data = interceptlabel, aes(x =x, y = y, label = label), parse = T, inherit.aes = F, hjust = -.2) + 
  geom_hline(yintercept = 1/6, linetype='dotted') +
  scale_color_brewer(palette = "Dark2", name = "Lineup Type", labels = c("-", "+")) +  
  facet_wrap(~param2, labeller = label_parsed) + 
  labs(x = "Square root of parameter value", y = "Expected probability of detection\nin a lineup of size 6", title = ) + 
  ThemeNoNet + 
  theme(legend.position = 'bottom')
@

<<beta5zoom, fig.height=3, fig.cap="The bottom middle panel of Figure~\\ref{fig:predictglmm} expanded to show greater detail. The parameter value is shown on the $x$-axis. This parameter most closely follows our hypothesis shown in Figure~\\ref{fig:hypothesis}. However, the result is not symmetric. According to the model, people will detect the effect at lower values and with greater frequency as the value increases when it is positive instead of negative.">>=
newdatabeta5 <- newdata %>% filter(test_param == "simttb", category == "inside") %>% mutate(param = "beta[5]")
plotdatabeta5 <- plotdata %>% filter(test_param=="simttb")%>% mutate(param = "beta[5]")
newdatabeta52 <- newdata %>% filter(test_param == "simttb", category == "outside") %>% mutate(param = "beta[5]")
newdatabeta53 <- newdata %>% filter(test_param == "simttb", category == "outside2") %>% mutate(param = "beta[5]")
interceptdata <- tibble(x = 10.090829, y= .05, label = "hat(beta)[5]")
ggplot() + 
  geom_line(data = newdatabeta5, aes(x = size, y = predictfinal, color = as.factor(type2)), size = 1.25) + 
  geom_point(data = plotdatabeta5, aes(x = size, y = datapick, color = as.factor(type2)), size = 2) + 
  geom_line(data = newdatabeta52, aes(x = size, y = predictfinal, group = as.factor(type2)), color = "gray60", alpha = .7) +
  geom_line(data = newdatabeta53, aes(x = size, y = predictfinal, group = as.factor(type2)), color = "gray60",  alpha = .7) +
  #geom_text(data = plotdata, aes(x = size, y = datapick, color = as.factor(type2), label = pic_id)) +
  #geom_text(data = labdat, aes(x = x, y=y, label = label, color = as.factor(type2)),
  #          show.legend = F) + 
  geom_vline(xintercept = 10.090829, linetype = "dotted", color = "grey60") + 
  geom_text(data = interceptdata, aes(x=x,y=y,label=label), parse = T, hjust = -.2) + 
  geom_hline(yintercept = 1/6, linetype='dotted') +
  scale_color_brewer(palette = "Dark2", name = "Lineup Type", labels = c("-", "+")) +
  facet_grid(~param, labeller = label_parsed) + 
  labs(x = "Parameter value", y = "Expected probability of detection\nin a lineup of size 6") + 
  ThemeNoNet + 
  theme(legend.position = 'bottom', strip.text = element_text(size = 10), text = element_text(size = rel(3)), legend.text = element_text(size = rel(3)), legend.key.size = unit(.25, "inches"))
@

<<beta4zoom, fig.height=3, fig.cap='The bottom right panel of Figure~\\ref{fig:predictglmm} expanded to show greater detail. The parameter value is shown on the $x$-axis. The ``reverse" lineup has a much flatter slope than the ``regular" lineup, which means the participants had a harder time detecting a more simple M1 structure among many more complex M4 structures. Reversing the lineup scenario was not symmetric as we hypothesized.'>>=
newdatabeta6 <- newdata %>% filter(test_param == "jtts", category == "inside") %>% mutate(param = "beta[6]")
plotdatabeta6 <- plotdata %>% filter(test_param=="jtts") %>% mutate(param = "beta[6]")
newdatabeta62 <- newdata %>% filter(test_param == "jtts", category == "outside")%>% mutate(param = "beta[6]")
newdatabeta63 <- newdata %>% filter(test_param == "jtts", category == "outside2") %>% mutate(param = "beta[6]")
labellerdata <- tibble(x = 3.340302, y = .8, label = "hat(beta)[6]")
ggplot() + 
  geom_vline(xintercept = 3.340302, linetype = "dotted", color = "grey60") + 
  geom_text(data = labellerdata, aes(x = x, y = y, label = label), parse =T, hjust = 1) + 
  geom_line(data = newdatabeta6, aes(x = size, y = predictfinal, color = as.factor(type2)), size = 1.25) + 
  geom_point(data = plotdatabeta6, aes(x = size, y = datapick, color = as.factor(type2)), size = 2) + 
  geom_line(data = newdatabeta62, aes(x = size, y = predictfinal, group = as.factor(type2)), color = "gray60", alpha = .7) +
  geom_line(data = newdatabeta63, aes(x = size, y = predictfinal, group = as.factor(type2)), color = "gray60", alpha = .7) +
  #geom_text(data = plotdata, aes(x = size, y = datapick, color = as.factor(type2), label = pic_id)) +
  #geom_text(data = labdat, aes(x = x, y=y, label = label, color = as.factor(type2)),
  #          show.legend = F) + 
  geom_hline(yintercept = 1/6, linetype='dotted') +
  scale_color_brewer(palette = "Dark2", name = "Lineup Condition", labels = c("reverse", "regular")) +
  facet_grid(~param, labeller = label_parsed) + 
  labs(x = "Parameter value", y = "Expected probability of detection\nin a lineup of size 6") + 
  ThemeNoNet + 
  theme(legend.position = 'bottom', strip.text = element_text(size = 10), text = element_text(size = rel(3)), legend.text = element_text(size = rel(3)), legend.key.size = unit(.25, "inches"))
@

We expand portions of Figure~\ref{fig:predictglmm} in Figures~\ref{fig:recipzoom}-\ref{fig:beta4zoom}. These figures show the same prediction regions as in Figure~\ref{fig:predictglmm}, plus some additional predictions outside of the data range shown in light gray. Again, the points represent the results from the experiment. In all three of these figures, the lack of symmetry is apparent. In the reverse lineup scenario shown in Figure~\ref{fig:beta4zoom}, the probability of prediction is consistently far less than the probability of prediction in the regular lineup scenario. This demonstrates that the visual signal of one plot from M4 among five plots from M1 is much stronger than that of one plot from M1 among five plots from M4. We posit that the latter is a more difficult task because it involves noticing a \textit{lack of structure} as opposed to the presence of \textit{more} structure. We can see a similar effect in Figure~\ref{fig:beta5zoom}. At a value of $\beta_5 = 20$, the model predicts a probability of about 0.60 that a new viewer of a new lineup will identify the alternative data plot. At a value of $\beta_5 = -20$, however, the model predicts this same probability to be about 0.40. This again demonstrates that the presence of structure is detected  more frequently and at smaller values than the absence of structure. 


<<beta41, fig.height = 4, fig.cap='In our experiment, 52.8\\% of viewers of this plot selected the plot from the alternative model, M4. The ``reverse" of this lineup is given in Figure~\\ref{fig:beta4neg1}, where 41.4\\% of viewers selected the plot from the alternative model, M1. Here, the alternative plot is $\\sqrt{25} - 3$.'>>=
dat <- read_csv("data/jtts_pos_easy_3.csv")
# "answer" is 2
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size = .5) + 
        ThemeNet + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

<<beta4neg1,fig.height=4, fig.cap='In our experiment, 41.4\\% people viewing this lineup selected the plot from the alternative model, M1. Here, the alternative plot is $\\sqrt{25} - 1$. The other five plots were simulated from model M4.' >>=
dat <- read_csv("data/jtts_neg_med_2.csv")
# 'answer' is 4
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size = .5) + 
        ThemeNet + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

For $\beta_4$ and $\beta_6$, where one plot simulated from M1 was placed among five plots from the corresponding model, we see that the predictions for the reverse lineup type (-1), are less than the standard lineup type (1) for all values of the parameter that we have. This contradicts our hypothesis for this scenario, which was that these two lineup types would perform similarly. A lineup type 1 scenario is given in Figure~\ref{fig:beta41}, and a corresponding lineup for the lineup type -1 scenario is given in Figure~\ref{fig:beta4neg1}, where the parameter of interest is $\beta_4 = 6.681$. For identical values of the parameter, viewers had a harder time identifying the different plot when they were selecting the most ``simple" structure, detecting M1 in five plots from the more complicated model, than they did identifying the most ``complex" structure, the plot from the more complicated model, from the five plots from M1. This result is also similar to that of \citet{corrviz} because it emphasizes the difficulty of picking out the absence of an effect relative to picking out the presence of an effect. Overall, the visual power of an effect increases as the effect size increases in absolute value, but the power is not symmetric around zero because stong positive effects are noticeable sooner than strong negative effects. 

\section{Discussion}\label{sec:concl}

% what did we do? what did we learn from it? 
By using visual inference methods, we have developed new ways to perform significance and goodness-of-fit tests for a complicated family of statistical models for social network data. We have also developed a way to determine the power of these new visual tests. Our methods can be used to supplement traditional methods and check our assumptions about network models. The traditional methods only look at one piece or derived measure of a network model, whereas our methods look at the models holistically for a broader sense of what it means for a parameter to be significant or a model to be a good fit. By looking at an entire network simulated from a network model side-by-side with other instances of networks simulated from a null model, instead of singular %one-dimensional derived 
features, we develop an idea of the model in terms of the \textit{data} itself, not in terms of statistical summaries of the data. %These methods place the model in the data space: they don't summarize or compress the data to put it in the model space \citep{Wickham2015}.

Furthermore, we have found the visual power of some effects in the objective function of a CTMC model for this particular senate data example,  and we have shown that, for the same effects, there is a lot of variability in results from significance and goodness of fit tests. Because the visual tests we performed show a great deal of variability, we can see that the decisions with respect to the significance of a parameter or the goodness of fit of a model to data are not as cut-and-dried as the more traditional methods would have us believe. 

% what are the limitations of these findings 

These results do not come without limitations. In VI, the null plots are supposed to play the role of good representatives of the null model. Here, the number of null plots is reduced to five, which has lead to very different conclusions for the same lineup scenario. Furthermore, these results do not generalize to all CTMC models or to one particular subset of CTMC models. The lineups shown are made for only one set of data, and it is not clear that the visual power results will transfer to other situations with different number of actors, different edge densities, or different layout algorithm of the node-link diagram. We can make some generalizations about what participants are picking up on in the lineups based on their feedback and previous research, but we cannot apply our hierarchical model directly to lineups constructed for new data or new models or parameters. 

% what is the future of this research? 

We hope these methods will be applied to different types of network data and different types of network models. But given the limitations of the node-link visualization, the cognitive load of looking at a lineup is very high for the average observer. More research is needed to apply these methods to larger datasets, different layout algorithms, and different ways of visualizing network data, such as adjacency matrix visualizations, using visual inference to see if similar findings emerge. 

%\hh{Best results: visual detection/ power analysis, a lot of variability in goodness of fit and significance tests. BUT: visualizations show that a decision of significance/ goodness of fit are not as clear cut as the traditional tests want us to believe.

%Limitations: 
%\begin{itemize}
%\item number of null plots: null plots are representatives of the null %distribution of all possible node-link diagrams of data sampled from the %null model. Here, the number of null plots is  reduced to five, which %increases the variability seen from a single lineup dramatically and %unfortunately leads to different conclusions.
%\item ability to generalize: the lineups are made for only one test case %- it is not clear, whether and how far, power results transfer to other %situations with different number of actors and different edge density.
%\end{itemize}}

% Talk about visual inference
%We propose to attack some of the aforementioned difficulties with SAOMs by using a technique known as \textit{visual inference}. This technique was created by \citet{Bujaetal} to provide well-defined, statistical rigor to the usual exploratory data analyses and model diagnostics that are typically performed by visualizing the data as opposed to looking at it raw or gathering numerical summaries of it. In visual inference there are \textit{null plots} and \textit{data plots}: the null plots are visualizations of data simulated from the model according to the null hypothesis, while the data plots are visualizations of data simulated from the model according to an alternative hypothesis. These two data sources are visualized side-by-side using small multiples using what Buja et al called the \textit{lineup protocol}. The idea behind the lineup protocol is the police lineup, where witnesses to crimes are brought to the police station to observe a group of people, one of whom is suspected to have committed the crime, and are asked to identify the perpetrator of the crime. If the witness identifies the suspect in the lineup, that is taken as evidence the suspect is guilty, whereas if the witness does not identify the suspect, that is taken as evidence the suspect is not guilty. In the visual inference lineup protocol, the \textit{suspect} is the data plot among $M-1$ null plots. If the "witness" can pick the data plot out of the lineup, that is taken as evidence that the null model should be rejected, whereas failing to pick out the data plot is taken as evidence that the null model should not be rejected. In our application of the lineup protocol to SAOMs, we include various effects of varying sizes in the SAOMs we fit to some data and try to \textit{see} those effects in node-link diagram corresponding to simulations from the various models.

%-Talk about political networks-

%-Talk about goodness-of-fit-

%-Talk about significance testings- 

