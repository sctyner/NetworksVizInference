% \newcommand{\st}[1]{{\color{orange} #1}}
% \newcommand{\hh}[1]{{\color{magenta} #1}}

<<setup-ch-2, echo = FALSE, message = FALSE, warning = FALSE>>=
options(replace.assign=TRUE,width=70, digits=3)
require(knitr)
opts_chunk$set(fig.path='figure/', cache.path='cache/', fig.align='center', fig.pos='h', out.width='.99\\textwidth', par=TRUE, cache=FALSE, concordance=TRUE, autodep=TRUE, message=F, warning=F, echo = FALSE, dev="cairo_pdf", fig.width = 6, fig.height = 6)#, root.dir = "~/Desktop/Dissertation/SAOM-removing-blindfold/")
@

<<pkgs>>=
library(tidyverse)
library(xtable)
library(RSienaTest)
library(RSiena)
library(geomnet)
library(GGally)
library(vinference)
library(lme4)
library(extrafont)
loadfonts(quiet = T)
load("../Data/senate/senateSienaNoHRC.rda")
ThemeNoNet <- theme_bw() %+replace% 
            theme(plot.title = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0, 
                                            family="Times New Roman"), 
                  axis.title.x = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0,
                                            family="Times New Roman"),
                  axis.title.y = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 90,
                                            family="Times New Roman"),
                  axis.text.x = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0,
                                            family="Times New Roman"), 
                  # axis.text.x.top = element_text(size = 12,
                  #                           face = 'plain', 
                  #                           angle = 0,
                  #                           family="Times New Roman"), 
                  # axis.text.x.bottom = element_text(size = 12,
                  #                           face = 'plain', 
                  #                           angle = 0,
                  #                           family="Times New Roman"), 
                  axis.text.y = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0,
                                            family="Times New Roman"),
                  # axis.text.y.left = element_text(size = 12,
                  #                           face = 'plain', 
                  #                           angle = 0,
                  #                           family="Times New Roman"),
                  # axis.text.y.right = element_text(size = 12,
                  #                           face = 'plain', 
                  #                           angle = 0,
                  #                           family="Times New Roman"),
                  strip.text.x = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0, 
                                            family="Times New Roman",
                                            margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.text.y =element_text(size = 12,
                                            face = 'plain', 
                                            angle = 90,
                                            family="Times New Roman",
                                            margin = margin(t = 3, r = 3, b = 3, l = 3, unit = "pt")),
                  strip.background = element_rect(colour = "black", fill = "white")
                    )
ThemeNet <- theme_net() %+replace% 
            theme(plot.title = element_text(size = 12,
                                            face = 'plain', angle = 0, family = "Times New Roman"), 
                  strip.text.x = element_text(size = 12,
                                            face = 'plain', angle = 0, family = "Times New Roman", 
                                            margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.text.y =element_text(size = 12,
                                            face = 'plain', angle = 0, family = "Times New Roman",margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  panel.border = element_rect(fill = NA, color = 'black'),
                  strip.background = element_rect(colour = "black", fill = "white"))
@

\section{Background} \label{sec:bg}
% \st{
% \begin{enumerate}
% \item significance testing: what is it and why is it important
% \item goodness-of-fit testing: what is it and why is it important
% \item difficulty of these two for sna, why do vis inf
% \end{enumerate}
% }
When selecting and fitting statistical models, there are typically three key assessments of interest: significance tests of parameters, goodness-of-fit tests, and power calculations of the tests. With significance testing, the null hypothesis assumes that the data come from a simple model nested within the model of interest. We then conduct significance tests of one or more additional parameters to determine if they significantly explain the variability in the data. For goodness-of-fit tests, we examine one or more models of interest to assess how well they fit the data. Finally, power tests quantify the ability of the hypothesis test to detect the difference between the null and alternative hypothesis.  All three of these aspects of statistical modeling are vital for inference. 
%The more complicated the model, the harder it is to determine fit or to include or exclude a parameter. 

The more complex the model, however, the harder it is to assess significance, power, and goodness-of-fit. One particularly complicated family of models are those designed to model network change. A \textit{network} is any set of things, such as people, or computers, that are connected in some way, through social relations or the Internet. In a social network, the people are the \textit{nodes}, or \textit{actors}, and the relationships between the people are \textit{edges}, or \textit{ties}. Models for networks are particularly complex, as 
dependencies inherent to network data make them  difficult to model. %Even more challenging is the situation 
This difficulty only increases when studying \textit{dynamic networks}, the same set of nodes and their changing relationships observed at many points in time, because of the added temporal dependence. Yet, researchers are often interested in modeling dynamic social networks, such as friendship networks among students or collaboration networks between researchers. % X ST a bit less jarring hopefully. X 
Even some of the simplest network models, however, lack the asymptotics required to perform traditional goodness-of-fit tests \citep{holland81}. In additional, direct maximum likelihood estimation of model parameters is frequently impossible due to the intractability of the models \citep{hummeletal}. 
%\hh{is the previous sentence a quote? I'm not sure that I follow.} %\st{it's a paraphrase... shall I make a quote? Also I definitely botched a word. Hopefully makes more sense now? }
% stop 4pm on 1/28/19
In order to circumvent some of these difficulties, we propose new methods for significance testing of parameters, goodness-of-fit testing, and power calculations %of these 
for one family social network models: continuous time Markov-chain (CTMC) models for dynamic network data, as defined in \citet{saompaper}. Specifically, we are using \textit{visual inference} in place of traditional statistical methods for social network models, such as Wald or $t$-tests for significance of parameters and in- and outdegree distribution metrics for determining goodness-of-fit. Visual inference (VI), introduced by \citet{Bujaetal}, will allow us to look at the \textit{entire} dataset simulated from a network model as opposed to a (set of) usually one-dimensional metric(s) derived from the network, or a $p$-value for a parameter in the model. By using visual inference to supplement traditional statistical tests, we gain a new understanding of the parameters in these CTMC models and ability to assess the fit of the CTMCs to dynamic network data.%In addition, we use visual inference to better understand some parameters in a social network model, by looking at how different values for parameters affect the networks simulated from these models.   

The paper is outlined as follows: Section~\ref{vi} gives a basic overview of visual inference and the lineup protocol. Section~\ref{sec:saom} provides an introduction to the our models of interest, stochastic actor-oriented models. Section~\ref{sec:expersetup} details how we define significance testing and goodness of fit procedures for SAOMs through visual inference, and Section~\ref{sec:res} details the results of a visual inference survey of Amazon Mechanical Turk workers. We close with a discussion in Section~\ref{sec:concl}.  

%\hh{where do the results come in?}\st{whoops, forgot to updat this with the reorg. }

\subsection{CTMC Models for Dynamic Social Network Data}\label{subsec:saom}

%To model family that we apply to the Senate data is the Stochastic Actor-Oriented Model (SAOM) family. 
CTMCs are a family of models for dynamic network data that incorporate both network structure and node-level information to describe how a network observed on two or more occasions changes over time \citep{saompaper}. The CTMCs model two key properties: social networks are ever-changing as relationships decay or grow in seemingly random ways, and the actors in them have characteristics that could affect how they change their ties to other nodes in the network. 
% Talk about SAOMs
These unique properties allow for the fitting of some very complicated models to inherently complex data, so it can be exceedingly difficult to interpret parameters and their corresponding estimates. The large amount of available parameters to include in the model combined with the difficulty of interpretation make parameter selection and goodness-of-fit testing burdensome as well.

Broadly, a CTMC model takes network structure and node covariate information into account in two ways and models the network changes one-at-a-time. First, the rate of change between states is dictated by a rate function that describes \textit{how often} changes in the network occur, and second, the objective function describes \textit{what} those state changes are. As in most other network models, the variables of interest are the edges of the network, $x_{ij}$, where $x_{ij}$ denotes the edge between nodes $i$ and $j$, where $i \neq j \in \{1, 2, \dots, n=\text{the number of nodes}\}$. 
$x_{ij}$ is binary, i.e.
  \begin{equation}\label{eq:edgevars}
  x_{ij} =
  \begin{cases}
                                   1 & \text{if an edge from $i$ to $j$ exists} \\
                                   0 & \text{otherwise}
  \end{cases}
  \end{equation}
Edges are treated as \textit{directed}, so that in general $x_{ij} \neq x_{ji}$, and self-referencing edges or loops are not allowed, i.e.  $x_{ii} = 0$ for all $i=1, 2, \dots, n$. 
The network is observed $M \geq 2$ times at time points  $t_1 < ... < t_M$, and the entire network at time point $t_m$ is denoted as $x(t_m)$ for $m \in \{1, \dots, M\}$. We discuss the rate and objective functions of a CTMC model in more depth below.  Additional details on these models can be found in \citet{saompaper, snijders01, snijders:2010, snijders2010, snijdersetall:2007, snijders:2017},

\textbf{Rate Function:} All changes in SAOMs are treated as changes made by the nodes, or \textit{actors}, in the network. Each actor, $i$, gets a chance to make a change according to the rate function, which dictates when relationships between nodes in the network can change. In general, the rate function can take the network structure such as the outdegree of a node, and the node covariates into account, but we use a simple rate function, which is constant over all nodes in a given time period, $\alpha_m$ $\forall i \in \{1, 2, \dots, n\}$. We denote the rate from $t_{m}$ to $t_{m+1}$ as $\alpha_m$ for $m = 1, \dots, M-1$. Using this notation, the \textit{waiting time} to the next chance for actor $i$ to make a change is exponentially distributed with expected value $\alpha_m^{-1}$. Since the rate is the same for all actors, the waiting time for \textit{any} actor to get the oppurtunity to change its set of ties is exponentially distributed with expected value $(n\alpha_m)^{-1}$.

\textbf{Objective Function:} After actor $i$ has been given the opportunity to change, it randomly selects one of its current ties, $x_{ij}$, to change. The probability that actor $i$ changes its current tie to actor $j$ is determined by the \textit{objective function} of the model and a random error term $U$ with log-Weibull distribution \citep{modelsSnijders}. .  %\hh{XXX can you paraphrase the purpose of U in half a sentence?}
%The random component is assumed to have log-Weibull distribution with location parameter $\mu = 0$ and scale parameter $\sigma = 0$ (see \citet{modelsSnijders}), so the probability density function of $U$ is
%\begin{equation}\label{eq:logweibull}
%g(u) = \exp\{-(u + \exp\{-u\})\}.
%\end{equation}
%The objective function, $f_i$, is the driving force of a SAOM. 
Actor $i$ aims to maximize its corresponding objective function $f_i$ given the current state of the network, $x$ and the node-level covariates, $\mathbf{Z}$. This function is defined as:% which has the form
\begin{equation}\label{eq:objective}
f_i(x, \boldsymbol{\beta}, \mathbf{Z}) = \sum_{k = 1}^K \beta_k s_{ik}(x, \mathbf{Z}),
\end{equation}
where $\boldsymbol{\beta} = (\beta_1, \dots, \beta_K)$ are additional model parameters, each associated with some network statistics, $s_{ik}(x, \mathbf{Z})$, $s_{ik}(x, \mathbf{Z})$, calculated with respect to actor $i$ at the current network state $x$. Network statistics range from the simple outdegree, $s_i(x) = \sum_{i\neq j} x_{ij}$, to the more complicated {\it transitive triplets jumping to different covariate}, $s_i(x, \mathbf{Z}) = \sum_{i \neq j \neq h} x_{ij}x_{ih}x_{hj} \cdot \mathbb{I}(z_i = z_h \neq z_j)$. %, plus many more. At last count, in the software we use to fit these models to network data, 
Version 1.2-3 of \texttt{RSiena} \citep{RSiena}, the software used to fit the models here, provides over 80 possible effects that can be included in the objective function. We discuss these statistics in more detail in Section~\ref{sec:models}. 

Objective function $f_{i}(x, \boldsymbol{\beta}, \mathbf{Z})$ and  random component $U$ combine to form the \textit{transition probability}, $p_{ij}$, of the network changing from its current state $x$ to the state with changed tie $x_{ij}$, denoted as $x(i \leadsto j)$:
\begin{equation}\label{eq:transprob}
p_{ij} = \dfrac{\exp\{f_i(x(i \leadsto j), \boldsymbol{\beta}, \mathbf{Z})\}}{\sum_{h} \exp\{f_i(x(i \leadsto h), \boldsymbol{\beta}, \mathbf{Z})\}}
\end{equation}
This probability dictates which edge change is made by the acting node. The acting node can also choose to \textit{not} change at all. This is most likely to occur when the numerator of Equation~\ref{eq:transprob}, as calculated for the current state of the network, is larger than for any changes $x(i \leadsto j)$ that could be made.  

%\hh{XXX how is the probability computed in the case that no change is made? Why is the notation $j \equiv i$ used for no change?}\st{that's more for the computation. I'll adjust accordingly}

According to \citet{RSienamanual}, at least two parameters must be included in the objective function: the density and the reciprocity. We denote the density, or out-degree, parameter by $\beta_1$ and the associated statistic as $s_{i1}(x) = \sum_{j} x_{ij}$. Similarly, we denote the reciprocity parameter by $\beta_2$ and the associated statistic as $s_{i2}(x) = \sum_{j} x_{ij}x_{ji}$. We will refer to the very simple model with only these two parameters in the objective function as model M1. We define additional models of interest in Section~\ref{sec:models} 

\subsection{Visual Inference}\label{subsec:vi}

Data visualizations are an important component of data analysis, providing a mechanism for discovering patterns in data. Pioneering research by \citet{gelman:2004}, \citet{Bujaetal} and \citet{majumder:2011} provide methods to quantify the significance of discoveries made from visualizations. \citet{Bujaetal} introduced two protocols, the Rorschach and the lineup protocol, which bridge the gulf between traditional statistical inference and exploratory data analysis. %The Rorschach protocol consists of a set of $m$ (usually, $m=20$) plots (called the {\it null plots}) rendered from data that is consistent with a given null model. The Rorschach protocol helps to understand the extent of randomness in the null model. 
Here, we use the lineup protocol. Under this protocol, we begin with a data set of interest to us, such as a network, and a visualization of this data, such as a node-link diagram. We would like to know the model that generated this data. There are two hypothesis considered: a null hypothesis which states the model that is assumed to have generated the data, and an alternative hypothesis that the data were not generated under this model. Then, the plot of the data of interest is placed randomly among a set of $m-1$ null plots, which are visualizations of data simulated from the null model (where $m =20$, usually). Human observers are then asked to examine the lineup and to identify the plot(s) that look(s) most different from the others. If an observer identifies the data plot, this is quantifiable evidence against the null hypothesis. Since an observer has a chance of 1 in $m$ to pick the data plot from the lineup by simply guessing, i.e. in a situation where the data plot is virtually indistinguishable from the null plots, the evidence grows in strength with the number of independent observers identifying the data plot.

The lineup protocol places a plot firmly in the framework of hypothesis tests: a plot of the data is considered to be the test statistic, which is compared against the sampling distribution under the null hypothesis represented by the null plots. Obviously, the null generating mechanism, i.e.\ the method of obtaining the data for null plots, is crucial for the lineup protocol, as the null hypothesis directly affects the choice of null generating method. Null generating methods are typically based on (a) simulation, if the null hypothesis allows us to directly specify a parametric model, (b) sampling, as for example in the case of large data sets, or (c) permutation of the original data \citep[see e.g.\ ][]{Good05}, which allows for non-parametric testing that preserves marginal distributions  while ensuring independence in higher dimensions. The model of interest here allows us to simulate directly from a parametric model for dynamic social network data. 
%In the experimental data that we analyzed the null generating methods used were permutation methods and direct simulation from a null model.

The lineup protocol was formally tested in a head-to-head comparison with the equivalent conventional test in \citet{majumder:2011}. The experiment utilized human subjects from Amazon's Mechanical Turk \citep{turk} and used simulation to control conditions. The results suggest that  visual inference is comparable to conventional tests in a controlled conventional setting. This provides support for its appropriateness for testing in situations where no conventional test exists or is difficult. %Interestingly, the power of a visual test increases with the number of observers engaged to evaluate lineups, and the pattern in results suggests that the power will provide results consistent with practical significance \citep{kirk:1996}.


\section{Example Data and Models}\label{sec:models}

\noindent \textbf{Data:} The data we use are collaboration networks in the United States Senate during the $111^{th}$ through $114^{th}$ Congresses, overlapping with Barack Obama's presidency. These senates began on January 6, 2009, the start date of the $111^{th}$ Congress, and ended on January 3, 2017, the last date of the $114^{th}$ Congress\footnote{Details of how this data can be downloaded are provided by FranÃ§ois Briatte at \url{https://github.com/briatte/congress}.}. In the US Senate, there are three ways that senators can show support for a piece of legislation: they can author the bill, cosponsor the bill, and vote for the bill. We use cosponsorship as a metric because it results in a network that is unimodal (all nodes are senators) and directed. In this network, ties are directed from senator $i$ to senator $j$ when senator $i$ signs on as a cosponsor to the bill that senator $j$ authored. There are many hundreds of ties between senators when they are connected in this way, so we simplify the network by computing a single value for each senator-senator collaboration called the \textit{weighted propensity to cosponsor} (WPC). This value is defined in \citet{senate} as 

\begin{equation}\label{eq:sen1}
    WPC_{ij} = \dfrac{\sum\limits_{k=1}^{n_j} \frac{Y_{ij(k)}}{c_{j(k)}}}{\sum\limits_{k=1}^{n_j} \frac{1}{c_{j(k)}}}
\end{equation}

where $n_j$ is the number of bills in a congressional session authored by senator $j$, $c_{j(k)}$ is the number of cosponsors on senator $j$'s $k^{th}$ bill, where $k \in \{1,\dots, n_j\}$, and $Y_{ij(k)}$ is a binary variable that is 1 if senator $i$ cosponsored senator $j$'s $k^{th}$ bill, and is 0 otherwise. This measure ranges in value from 0 to 1, where $WPC_{ij} = 1$ if senator $i$ is a cosponsor on every one of senator $j$'s bills and $WPC_{ij} = 0$ if senator $i$ is never a cosponsor any of senator $j$'s bills. Because SAOMs require binary edges, we construct the edges as follows: 
 \begin{equation}\label{eq:edgewpc}
  x_{ij} =
\begin{cases}
                                   1 & WPC_{ij} > 0.25 \\
                                   0 & WPC_{ij} \leq 0.25
\end{cases}
\end{equation}
For each of the four senate sessions, in addition to the WPC value between any two senators in the session, we have three node covariates: the party affiliation of each senator, the number of bills they authored in each session, and their gender. We explored each of these covariates in the model to determine if they affect the overall network structure and how ties are formed between senators. The node-link diagram representations of the data we use for modelling are shown in Figure~\ref{fig:senateAll}. We have labelled some of the nodes in these networks whose names will be familiar to US readers, because they are leaders in their party or they have run for president. The size of the nodes represent how many bills the senator authored in a session, the color represents party affiliation, and the shape represent gender. In each of the four sessions, there is one very large connected component tying many of the prominent senators together, with many smaller groups of two to ten senators surrounding the larger component. In each senate, the structure changes slightly as new senators arrive or come to prominence.

<<senateAll, fig.cap="The four senate collaboration networks that we use as our example data to visually assess the SAOM effects. Color represents party, shape represents gender, and size represents number of bills authored in a session. The Frucherman-Reingold layout is shown. ">>=
seobama <- read_csv("data/senateobamapres_gsw_25.csv")
to_label <- c("Joseph R. Biden Jr.", "John S. McCain", "Ted Cruz","Marco Rubio",
  "Lindsey O. Graham","Rand Paul", "Bernard Sanders", "Jim Webb", "Mitch McConnell",
  "Harry M. Reid", "Hillary Rodham Clinton", "Amy Jean Klobuchar", "Elizabeth Warren")
seobama$label <- as.factor(ifelse(seobama$source %in% to_label, seobama$source, ""))
levels(seobama$label) <- c("", "Amy Klobuchar", "Bernie Sanders", "Elizabeth Warren",
                           "Harry Reid", "Hillary Clinton", "Jim Webb", "John McCain", 
                           "Joe Biden","Lindsey Graham", "Marco Rubio", "Mitch McConnell", 
                           "Rand Paul", "Ted Cruz")
se111clint <- filter(seobama, senate == 111)
se111noclint <- filter(seobama, senate == 111)
se111noclint$target[which(se111noclint$target == "Hillary Rodham Clinton")] <- NA
seobama2 <- seobama %>% filter(senate != 111) %>% bind_rows(se111noclint)

set.seed(56049382)
ggplot(data = seobama2) + 
  geom_net(directed = T, labelon=T, arrowsize = .25, singletons= T, fiteach = T, linewidth = .25, layout.alg = 'fruchtermanreingold', fontsize = 2, repel=T, 
           aes(from_id = source, to_id = target, color = party, 
               label = label,shape = sex,size = n_au)) + 
  ThemeNet + 
  scale_color_manual(values = c("royalblue", "forestgreen","firebrick")) + 
  scale_shape_manual(values = c(17,16)) + 
  scale_size_continuous(name = "Bills\nauthored", range = c(.75, 3)) + 
  theme(legend.position = 'bottom', strip.text = element_text(size = 8)) + 
  facet_wrap(~senate, nrow = 2, labeller = "label_both") + 
  xlim(c(0,1.05)) + 
  ylim(c(0,1.05))
@

For Senate 111, for instance, we see Hillary Clinton, serving out her second term in the senate until she became Secretary of State. She is isolated in Figure~\ref{fig:senateAll}, but in actuality, she had many cosponsors on only two pieces of legislation she authored in that short time.%, as is shown in Figure~\ref{fig:senateClinton}. 
We chose to remove Clinton and her edges from the network because they make the overall structure look vastly more connected than the other three senates. We suspect that because she had just recently run for President and had just been selected as Obama's Secretary of State, the cosponsorships she garnered between the start of the new congress and becoming Secretary of State were largely symbolic, so the $111^{th}$ Senate without Hillary Clinton is more typical than the $111^{th}$ Senate with her. In fact, she had several co-sponsors on both of her pieces of legislation, which meant she had the maximum WPC value of one with about a dozen other senators. Thus, though she is in the Senate for a time, subsequent analyses will not include her, and will instead include her replacement, Kirsten Gillibrand.  

<<senateClinton, fig.cap="We removed Hillary Clinton's ties from the network because she had abnormally high collaboration with senators during the time she was in the 111th senate and before she left office to become Secretary of State.", fig.height=3, eval=FALSE>>=
se111clint$Clinton <- 'Yes'
se111noclint$Clinton <- 'No'
clintonSenate <- rbind(se111clint, se111noclint)
clintonSenate$Clinton <- as.factor(clintonSenate$Clinton)
clintonSenate$Clinton <- ordered(clintonSenate$Clinton, levels = c("Yes", "No"))
clintonSenate %>% filter(!(source %in% c("Roland Burris", "Bernard Sanders", "Frank R. Lautenberg", "Mary L. Landrieu") & is.na(target))) %>%
ggplot() + 
  geom_net(directed = T, labelon=F, arrowsize = .3, singletons= F, fiteach = T, arrowgap = .01, layout.alg = 'fruchtermanreingold',
           aes(from_id = source, to_id = target, color = party, linewidth = gsw, shape = sex,size = n_au)) + 
  scale_size_continuous(name = "Bills\nauthored", range = c(.75, 3)) + 
  ThemeNet + 
  scale_shape_manual(values = c(17,16)) + 
  scale_color_manual(values = c("royalblue", "forestgreen","firebrick")) + 
  theme(legend.position = 'bottom') + 
  facet_wrap(~Clinton, nrow = 1, labeller = 'label_both')
@

In legislative cosponsorship networks, it is well known that party affiliation, reciprocity of relationships, and whether senators are ``workhorses" who author many bills or ``showhorses" who author few bills, are major influences on structure \citep{legnet}. We focus on these covariates, plus the additional gender covariate when choosing which SAO models to fit to the data. 


\noindent \textbf{Models:} In order to determine which models to examine, we first consider already well-known effects in legislative networks. We also selected some effects which were significant according to the built-in Wald-type tests in \texttt{RSiena} for application of our significance and goodness-of-fit methods. We examine a total of six models, each identified by its objective function: 
\begin{enumerate}
\item Model M1: $f_{i}(x, \boldsymbol{\beta}) = \beta_1 s_{i1}(x) + \beta_2 s_{i2}(x)$
\item Model M3: $f_{i}(x, \boldsymbol{\beta}, \mathbf{p}) = \beta_1 s_{i1}(x) + \beta_2 s_{i2}(x) + \beta_3 s_{i3}(x, \mathbf{p})$
\item Model M4: $f_{i}(x, \boldsymbol{\beta}, \mathbf{s}) = \beta_1 s_{i1}(x) + \beta_2 s_{i2}(x) + \beta_4 s_{i4}(x, \mathbf{s})$
\item Model M5: $f_{i}(x, \boldsymbol{\beta}, \mathbf{b}) = \beta_1 s_{i1}(x) + \beta_2 s_{i2}(x) + \beta_5 s_{i5}(x, \mathbf{b})$
\item Model M6: $f_{i}(x, \boldsymbol{\beta}, \mathbf{p}) = \beta_1 s_{i1}(x) + \beta_2 s_{i2}(x) + \beta_6 s_{i6}(x, \mathbf{p})$
\item Model M7: $f_{i}(x, \boldsymbol{\beta}, \mathbf{p}, \mathbf{b}, \mathbf{s}) = \beta_1 s_{i1}(x) + \beta_2 s_{i2}(x) + \beta_4 s_{i4}(x, \mathbf{s}) + \beta_5 s_{i5}(x, \mathbf{b}) + \beta_6 s_{i6}(x, \mathbf{p})$
\end{enumerate}

The parameters in the objective function, $\beta_1, \dots, \beta_6$ are defined in Table~\ref{tab:effects}. 

%To determine the effects that we would move forward with, we followed this procedure: 

% \begin{enumerate}
% \item Define the simple effects structure of the data: the rate parameters and the outdegree and reciprocity parameters. 
% \item Add each additional possible effect, \st{as determined by the effects documentation function}, in \texttt{RSiena} one-at-a-time to the model\st{'s objective function}  \citep{RSiena}.
% \item Fit each model to the data and check for convergence.
%     \begin{enumerate}
%     \item If the model converged, move to 4.
%     \item If the model did not converge, use the previous fitted values as starting values and repeat 5 times or until convergence, whichever comes first.
%     \end{enumerate}
% \item Test the added parameter for significance using a Wald-type test.
% \item Report out the estimate of the additional parameter, its standard error, Wald $p$ value, and convergence criterion.
% \end{enumerate}
%  
% After completing the procedure for all model effects, we selected effects whose estimates converged, had a Wald $p$-value of less than 0.10, and seemed to have a reasonable interpretation for our data according to well-known properties of legislative networks \citep{legnet}.

%The parameters we use for the remainder of the paper are detailed in Table~\ref{tab:effects}.
All models had the first two parameters, $\beta_1, \beta_2$ at minimum included. Other effects were added one at a time so that the simplest model $M1$ is nested in each of the other models. The most significant additional effect was $\beta_3$, the jumping transitive triplet (JTT) parameter for the party covariate, which was estimated to be about -6 with a standard error of 0.11, resulting in a $p$-value of less than 0.0001. This estimate of the parameter associated with this statistic considers the number of transitive closures formed between two senators from different parties. The negative estimate is an indication that forming transitive ties between two people from different parties is discouraged, which corresponds to the divisive nature of American politics, where party affilitation is the dominant effect. Another significant effect was $\beta_4$, the same JTT parameter for the sex covariate, with an estimate of about 3 with a standard error of 0.89. This parameter also consideres transitive closures, but for senators of different genders. The positive value indicates that transitive ties between senators of different genders are more likley to form. Another significant effect was $\beta_6$, the covariate-related similarity score-weighted transitive triplets parameter estimate for the number of bills authored by a senator. We chose to look at similarity instead of covariate "jumps" because the number of bills authored is more continuous than gender or party. The similarity measure is computed as: 
\begin{equation}\label{eq:similar}
sim^b_{ij} = \frac{\max_{hk}|b_h - b_k| - |b_i - b_j|}{\max_{hk}|b_h - b_k|} 
\end{equation}
where $\max_{hk}|b_h - b_k|$ is the range of number of bills authored by senators, and $b_i$, $b_j$ are the number of bills authored by senators $i,j$ repectively in the senate period. This effect was estimated at about 10 with standard error of 3.9. The high positive estimate suggests senators are encouraged to collaborate with other senators who author about the same number of bills they do. This tendency of senators to cosponsor bills written by senators who are similarly ``prolific" corresponds to another well-known property of the U.S. Senate structure: the tendency of senators to be either ``workhorses" or ``showhorses". Senators known as workhorses author many pieces of legislation in a session, and largely stay out of the public arena. The showhorse senators, on the other hand, author relatively few pieces of legislation, and tend to appear on television, radio, and other media a great deal. So, showhorses and workhorses tend to stick together. Finally, we found $\beta_5$, the same party transitive triplet effect was also significant, with a fitted value of 1.3 and standard error of 0.7, meaning that transitive relationships between senators tend to form when they are from the same party, exactly as we would expect in a legislative body in a country with extremely entrenched partisan divides as in the US. 

<<geteffects, eval = FALSE>>=
# used for table below
initeff <- read_csv("data/sigEffsSenate912.csv")
initeff %>% filter(shortName == "sameX")
#density & -- & $\sum_j x_{ij}$ & & & \\
#reciprocity & -- & $\sum_j x_{ij}x_{ji}$ & & & \\
@

\begin{table}
\scalebox{0.8}{
\begin{tabular}{c|p{2cm}|p{2.1cm}|c|l|p{1.75cm}|p{2cm}}
$\beta_k$ & {\bf Effect name} & {\bf Interaction Variable} & {\bf Formula} & {\bf Picture} & {\bf Initial estimate} & {\bf Wald $p$-value} \\
\hline 
$\beta_1$ & density & NA & $s_{i1}(x) = \sum_{j} x_{ij}$ & \includegraphics[width=.6in]{img/density.png} & 2.204 & NA \\ 
$\beta_2$ & reciprocity & NA & $s_{i2}(x) = \sum_{j} x_{ij}x_{ji}$ & \includegraphics[width=.6in]{img/recip.png} & -4.903 & NA \\
$\beta_3$ & jumping transitive triplet & party & $s_{i3}(x, \mathbf{p}) = \sum_{j\neq h} x_{ij}x_{ih}x_{hj}\cdot \mathbb{I}(p_i = p_h \neq p_j)$ & \includegraphics[width=.6in]{img/jttp.png} & -5.884 & $<0.0001$\\
$\beta_4$ & jumping transitive triplet & sex & $s_{i4}(x, \mathbf{s}) = \sum_{j\neq h} x_{ij}x_{ih}x_{hj}\cdot \mathbb{I}(s_i = s_h \neq s_j)$ & \includegraphics[width=.6in]{img/jtts.png}  & 3.335 & 0.0002 \\
$\beta_5$ & similarity transitive triplet & bills & $s_{i5}(x, \mathbf{b}) = \sum_{j} x_{ij}x_{ih}x_{hj}\cdot (sim^b_{ij} - \overline{sim}^b)^*$ & \includegraphics[width=1in]{img/simttb.png} & 9.821 & 0.0128 \\
$\beta_6$ & same transtive triplet & party & $s_{i6}(x, \mathbf{p}) =\sum_{j} x_{ij}x_{ih}x_{hj}\cdot \mathbb{I}(p_i = p_j)$ & \includegraphics[width=1in]{img/samettp.png}  & 1.306 & 0.0642 %\\
%$\beta_7$ & same & party & $s_{i7}(x, \mathbf{p}) = \sum_j x_{ij}\mathbb{I}(p_i = p_j)$ & \includegraphics[width=.6in]{img/samep.png}  & 0.363 & 0.0074
\end{tabular}
}
\caption{\label{tab:effects} The effects we used in the SAOMs fit to the senate data. In the picture which represents each effect, the dotted tie is encouraged to form if the estimate is positive, and discouraged to form if the estimate is negative.  $^*$ - $sim^b_{ij} = \frac{\max_{hk}|b_h - b_k| - |b_i - b_j|}{\max_{hk}|b_h - b_k|}$ is the similarity score between two senators based on the number of bills authored, and $\overline{sim}^b = \frac{1}{n(n-1)}\sum_{i\neq j} sim^b_{ij}$ is the average bill similarity score between any two senators.}
\end{table}

We fit all six of our models in \texttt{RSiena} using Markov Chain Monte Carlo (MCMC) methods to approximate the method of moments estimates of the parameters. Because the estimation is done through MCMC simulation, we fit each model to the data 1,000 times to get a better estimate of the true value of $\boldsymbol{\beta}$. From the simulations in which the MC converged, which made up over 90\% of the fits for each model, we computed the mean of the 1,000 estimates of each parameter to get final estimates of  $\hat{\boldsymbol{\beta}}$ for each model, which are given in Table~\ref{tab:fittedvalues}. 

<<modelestimates>>=
load("data/allModelMeans.RDS")
modelMeanEsts %>% unnest(ests) %>% mutate(param = "rate") -> modelmeans
modelmeans$param[c(1,6,12, 18,24, 30)] <- "alpha1"
modelmeans$param[c(1,6,12, 18,24, 30)+1] <- "alpha2"
modelmeans$param[c(1,6,12, 18,24, 30)+2] <- "alpha3"
modelmeans$param[c(1,6,12, 18,24, 30)+3] <- "beta1"
modelmeans$param[c(1,6,12, 18,24, 30)+4] <- "beta2"
modelmeans$param[modelmeans$param == "rate"] <- c("beta3", 'beta4', 'samep', 'beta6', 'beta5')
modelmeans %>% filter(model != "samep") %>% spread(param, ests) -> modelmeans
M6 <- c(2.4405048,2.4594403,2.2098176,-4.9232775,4.8916183,2.3743720,0.2047038,6.9661589)
@

\begin{table}
\centering
\begin{tabular}{|l|ccccccccc|}
\hline
Model & $\hat{\alpha}_1$ & $\hat{\alpha}_2$ & $\hat{\alpha}_3$ & $\hat{\beta}_1$ & $\hat{\beta}_2$ & $\hat{\beta}_3$ & $\hat{\beta}_4$ & $\hat{\beta}_5$ & $\hat{\beta}_6$ \\
\hline
\hline
M1 & \Sexpr{round(modelmeans$alpha1[1], 3)} & \Sexpr{round(modelmeans$alpha2[1], 3)} & \Sexpr{round(modelmeans$alpha3[1], 3)} & \Sexpr{round(modelmeans$beta1[1], 3)} & \Sexpr{round(modelmeans$beta2[1], 3)} & -- & -- & -- & --\\
M3 & \Sexpr{round(modelmeans$alpha1[2], 3)} & \Sexpr{round(modelmeans$alpha2[2], 3)} & \Sexpr{round(modelmeans$alpha3[2], 3)} & \Sexpr{round(modelmeans$beta1[2], 3)} & \Sexpr{round(modelmeans$beta2[2], 3)} & \Sexpr{round(modelmeans$beta3[2], 3)} & -- & -- & --\\ 
M4 & \Sexpr{round(modelmeans$alpha1[3], 3)} & \Sexpr{round(modelmeans$alpha2[3], 3)} & \Sexpr{round(modelmeans$alpha3[3], 3)} & \Sexpr{round(modelmeans$beta1[3], 3)} & \Sexpr{round(modelmeans$beta2[3], 3)} & -- & \Sexpr{round(modelmeans$beta4[3], 3)} & -- & --\\
M5 & \Sexpr{round(modelmeans$alpha1[5], 3)} & \Sexpr{round(modelmeans$alpha2[5], 3)} & \Sexpr{round(modelmeans$alpha3[5], 3)} & \Sexpr{round(modelmeans$beta1[5], 3)}& \Sexpr{round(modelmeans$beta2[5], 3)} & -- & -- & \Sexpr{round(modelmeans$beta5[5], 3)} & -- \\
M6 & \Sexpr{round(modelmeans$alpha1[4], 3)} & \Sexpr{round(modelmeans$alpha2[4], 3)} & \Sexpr{round(modelmeans$alpha3[4], 3)} & \Sexpr{round(modelmeans$beta1[4], 3)} & \Sexpr{round(modelmeans$beta2[4], 3)} & -- & -- & -- & \Sexpr{round(modelmeans$beta6[4], 3)}\\
M7 & \Sexpr{round(M6[1], 3)} & \Sexpr{round(M6[2], 3)} & \Sexpr{round(M6[3], 3)} & \Sexpr{round(M6[4], 3)} & \Sexpr{round(M6[5], 3)} & -- & \Sexpr{round(M6[6], 3)} & \Sexpr{round(M6[8], 3)} & \Sexpr{round(M6[7], 3)} \\
\hline
\end{tabular}
\caption{\label{tab:fittedvalues} The final estimates from repeated estimation of our models of interest. When simulating from these models, these are the estimates that we will use unless otherwise stated.}
\end{table}

We want to determine the role that each of these parameters plays in the objective functions for the different models. So, we use the estimates given in Table~\ref{tab:fittedvalues} to simulate from models M1 through M6. We discuss the simulation procedure and how we use the simulations in Section~\ref{sec:expersetup}.  

\section{Experimental Design}\label{sec:expersetup}

We want to explore three different aspects of the SAOM models using the lineup protocol: (1) significance of parameters, (2) goodness-of-fit of a model, and (3) \st{visual power of the effects.} %visual detection of parameters. 
Each one of these situations requires a different setup, which we describe in detail, making use of the lineup protocol to study all of these aspects. 

In each lineup, we include plots from two models: the null model and an alternative model. The definition of the null and alternative model varies with the aspect of the SAOMs we are exploring. 
%For each lineup, we consider two models: model M1 is the null model, and another model, from M2-M5, which is chosen to be the alternative model. 

 Typically, a lineup shows sets of 20 plots at a time c.f.~\citet{loy:2015, vanderplas:2016}, but we determined that not enough structure could be shown in each plot for 20 large node-link diagrams. We chose to present our participants with only six plots at a time in order to show the node-link diagrams in more detail and to lower cognitive load for participants.
To construct a lineup, we simulate five networks from the null model and one network from the alternative model.  An example of a lineup like those shown to our participants is given on the right side of the image  in Figure~\ref{fig:shiny}.  In this lineup, model M4 is the alternative model, and model M1 is the null model. 

To simulate lineups from the models we used the \texttt{siena07} function in \texttt{RSiena} \citep{RSiena}. Sections~\ref{sec:setupst} through~\ref{sec:setupvp} describe in detail how we set up the lineups, which parameter values we simulate from, and why. Sets of the lineups we create are shown to independent observers recruited through Amazon Mechanical Turk for feedback (more details on the Turk setup in Section~\ref{sec:res}). %, we set the parameters to the values given in Table~\ref{tab:fittedvalues} for all parameters within the respective models, with the exception of $\beta_5$. For $\beta_5$, twice the estimated value was used. More detail on why we use twice the fitted value is provided in Section~\ref{sec:setupvp}.\footnote{If you would like to explore the kinds of lineups we use in further detail, please visit \url{https://sctyner.shinyapps.io/saom_lineup_creation/}}. To get the simulations,

\begin{figure}
\includegraphics[width=.9\textwidth]{img/shinyappscreen.png}
\caption{\label{fig:shiny} A screen shot of the web application we created to design our lineup experiment. More details about this application are given in Section~\ref{sec:setupvp}. In the lineup, M5 is the alternative model with $\beta_5$ set to twice its estimated value given in Table~\ref{tab:fittedvalues}. One plot simulated from this model is placed at random among five observations simulated from the null model, M1. Participants of the study are asked to identify the most different plot.  %The other two tabs in the right side of the screen tell you which plot comes from the alternative model, and allow you to view and download the data that is plotted in the lineup.
}
\end{figure}

\subsection{Significance Testing}\label{sec:setupst}

In the significance testing protocol, a parameter of interest is selected to test, say $\beta_k$. The hypotheses we use to generate lineups are: 

\begin{equation}\label{eq:sig}
H_0: \beta_k = 0 \ \ \ \text{ versus } \ \ \ H_A: \beta_k \neq 0
\end{equation}
%\begin{description}
%\item $H_0$: $\beta_k = 0 $
%\item $H_A$: $\beta_k \neq 0$
%\end{description}

Under the null hypothesis, we assume that the model that generated the network data is $M_1$, the simplest model presented in Section~\ref{sec:models}. Thus, the five null plots in the lineup are simulations from M1 with $\beta_1, \beta_2$ set to the estimates given in Table~\ref{tab:fittedvalues} for M1. The alternative model is the model with $\beta_1, \beta_2$, and $\beta_k$ in the objective function. The alternative data plot is simulated from the appropriate model also with values set to the estimates from Table~\ref{tab:fittedvalues}. 

The lineup generated under this scenario is shown to a number of independent viewers. If an observer picks out the alternative data plot, that is evidence against the null hypothesis, while picking one of the null plots is evidence in favor of the null hypothesis. The significance tests that we perform in our experiment are for $\beta_3$ and $\beta_4$, making the alternative models M3 and M4, respectively, for three repitions each.  

\subsection{Goodness-of-Fit}\label{sec:gofsetup}

For the goodness-of-fit tests, we compare one model of interest, say M\textit{i} to the data. The hypothesis we use to generate lineups are 
\begin{description}
\item $H_0$: The data come from model M\textit{i}
\item $H_A$: The data come from some other, unknown model
\end{description}

To generate the null plots, we simulate five networks from model M\textit{i} using the corresponding parameters in Table~\ref{tab:fittedvalues}. We pick a wave to focus on, wave two, which is the first simulated network, and among these five plots, we place a node-link diagram of the true second wave data. We cannot show the data more than once to each participant, so we examine several different models with three repitions each in our Amazon Mechanical Turk experiment, with each participant never seeing the actual data twice. The models we chose for goodness-of-fit testing are M3, M4, M5, and M7. 

\subsection{Visual Power}\label{sec:setupvp}

%Visual detection does not fit into the typical statistical testing framework as easily as the significance and goodness-of-fit tests we perform do. 
Through visual inference, we want to determine at which point an effect becomes noticeable in a SAOM. By \textit{noticeable}, we mean that the inclusion of the effect alters the appearance of networks simulated from a model to a degree that viewers are able to reliably pick out a node-link diagram rendered from data simulated from a model \textit{with} the effect in a lineup among plots \textit{without} the effect. This is a way to determine the power of the visual significance test.  We explore all parameters in the objective function, $\beta_1, \dots, \beta_6$ in this way. 

In model M1, with only two parameters in the objective function, we varied both the density and reciprocity parameter values one at a time, \st{keeping all other parameters at their fitted values given in Table~\ref{tab:fittedvalues}}. In models M3 through M6, we vary the additional parameter, $\beta_3$ through $\beta_6$, respectively, while again holding the rate and other parameters at their value in Table~\ref{tab:fittedvalues}. Thus, we have six different parameters of interest to us in total: $\beta_1, \dots, \beta_6$. We want to determine how the size of these parameters affects the overall structure of the network data simulated from the models M1 through M6, so we also vary the value of the parameters in order to determine at what level the effects become noticeable. %in both negative and positive directions. 

%Because the traditional way to visualize a network is a node-link diagram, we will examine the effect of different parameter values using node-link visualizations. 

<<hypothesis, fig.height=2, fig.cap="We hypothesize that as the parameter value of interest increases in absolute value, more viewers of the lineup will pick the alternative data out of a lineup. Note that the significance test we construct in Section~\\ref{sec:setupst} is just one point on the line below, represented by the vertical dotted line labeled $\\hat{\\beta}$. The easy, medium, and hard lines represent how we determined which values of the parameters to show to our participants, and the horizontal dotted line shows the type-I error for one viewer of a lineup of size 6.">>=
x <- seq(-10, 10, .05)
N <- length(x)
qplot(x = 1:N, y = pt(x, 1), geom = 'line') + geom_line(aes(x = -(1:N), pt(x,1))) + 
  scale_x_continuous(name = "Parameter Value", labels = c("very negative", "negative", "0", "positive", "very positive")) + 
  geom_hline(yintercept = 1/6, linetype = 'dotted') +
  geom_vline(xintercept = 0) + 
  geom_label(data = NULL, aes(x = 0, y = .9, label = "Null Model = Alt. Model"), size = 2) +
  geom_vline(xintercept = 150, color = 'red', linetype = 'dotdash') + 
  annotate('text', x = 160, y = .9, label = "hat(beta)",parse = TRUE, size = 2) + 
  geom_vline(xintercept = c(-300,-230, -175), color = c('green', 'yellow', 'red')) + 
  geom_text(data = NULL, aes(x = -300, y = .75, label = "easy"), angle = 90, vjust = -.5, size = 3) + 
  geom_text(data = NULL, aes(x = -230, y = .5, label = "medium"), angle = 90, vjust = -.5, size = 3) +
  geom_text(data = NULL, aes(x = -175, y = .05, label = "hard"), angle = 90, vjust = -.5, size = 3) + 
  scale_y_continuous(name = "% detecting plot from\nthe alternative model", labels = paste0(c(0,25,50,75,100), "%")) + 
  theme_bw() + 
  theme(text = element_text(size = rel(3)))
@

To determine the threshold at which an effect becomes noticeable, we examine six different levels of the effect, three negative and three positive ones. Figure~\ref{fig:hypothesis} shows a sketch of what the detection probability by participants' looks like hypothetically with varying effect size: the higher in absolute value the parameter is, the more likely participants are to choose the alternative model out of the lineup. To determine the exact values of the six levels we want to test for each effect, we started with the estimates of the parameter at hand (see  Section~\ref{sec:models}), and used small negative and positive factors to determine at what point \textit{we} noticed the effect of the parameter in simulations from the changed models. \st{In Figure~\ref{fig:hypothesis}, we demonstrate these values with the vertical lines labeled ``easy," ``medium," and ``hard." We expected most viewers to see the effects at the ``easy" values, and we expected very few, if any, viewers to see the effects at the ``hard" values. }

\st{To decide on the values to use for each difficulty level,} we constructed an online application that created the lineup protocol for us to be the guinea pigs of our own experiment \citep{theqs}. A screen shot of the app we created with the \texttt{shiny} package by \citet{shiny} is shown in Figure~\ref{fig:shiny}. On the left side of the screen, the user\footnote{Please visit \url{https://sctyner.shinyapps.io/saom_lineup_creation/} to create lineups constructed from the models we present for this data for yourself.} can input the information necessary for creating a lineup of the models M1 through M7 for the data in Section~\ref{sec:senatedata}: first, choose to simulate only one plot from the specified model (analogous to changing the alternative model in the lineup protocol) or to simulate $M-1$ plots from the specified model (analogous to changing the null model in the lineup protocol); the model of interest; the wave of the data to examine; if model M1 is selected, whether to alter the density or the reciprocity parameter; the size of the lineup; the amount by which to multiply the effect selected; a random seed for replicability; and a layout algorithm to use for the node-link diagrams. There is also a checkbox if the user wishes the nodes to be colored by the size of the connected component to which they belong. The plot(s) that appear(s) that are \textit{not} from the model specified using the other options are simulated from model M1 with the estimates of the rate parameters, and $\beta_1$ and $\beta_2$ as given in Table~\ref{tab:fittedvalues}. 

Using a ``Picking Lineups" web application, we settled on six parameter values to test for each of our six effects, $\beta_1, \dots \beta_6$. The complete details of the parameters tested using the lineup protocol is given in Table~\ref{tab:experdetail}. In the case of both $\beta_4$ and $\beta_6$, we could not determine any values for negative effects that made the data simulated from M4 and M6 look different than null model simulations from model M1. \st{We hypothesized that this was due to the negative effects \textit{removing} visually interesting structural elements as opposed to \textit{adding} noticeable structural elements.}  Therefore we decided that the lesser experienced participants in our experiment would also not be able to. Instead of testing the negative values of these effects, we are examining a different scenario: we placed 5 simulations from model M4 or M6 with positive values of the parameter with one simulation from model M1 in a lineup. We will refer to this as the ``reverse" lineup scenario. We used the reverse scenario to determine if the perception of the effect size is symmetric: if an effect is noticed $x$\% of the time at value $\beta_k = \beta_{k_0}$ when \textit{one} simulation from the corresponding model is placed among \textit{five} null plots from model M1, then when \textit{five} simulations from the model with $\beta_k = \beta_{k_0}$ are put in a lineup with \textit{one} simulation from model M1, the plot from the simpler model should be noticed about $x$\% of the time as well. 

<<getexperdetail, results='asis'>>=
turk22 <- read_csv("data/turk22-sig.csv")
modelData_sig <- turk22 %>% 
  mutate(type2 = ifelse(sign == 0, sign(initialEst), sign),
         type2 = ifelse(type == "one", type2, -1)) %>% 
  filter(pic_id != 3132) 
modelData_sig %>% group_by(test_param, type2, difficulty, size) %>%
  count() %>% ungroup() %>%
  spread(difficulty, size) -> step1
step1 %>% select(1:4) %>% filter(!is.na(`1`)) -> step2a
step1 %>% select(c(1:3,5)) %>% filter(!is.na(`2`)) -> step2b
step1 %>% select(c(1:3,6)) %>% filter(!is.na(`3`)) -> step2c
experdet <- step2a %>% mutate(easy = `1`, medium = step2b$`2`, hard = step2c$`3`) %>% select(-c(`1`, n))
experdet$test_param <- as.factor(experdet$test_param)
levels(experdet$test_param) <- paste0("\\$\\beta_", c(1,2,3,5,4,6), "\\$")
experdet$test_param <- factor(experdet$test_param, levels=paste0("\\$\\beta_", c(1,2,3,5,4,6), "\\$"))
experdet <- experdet %>% arrange(test_param)
experdet$test_param <- as.character(experdet$test_param)
experdet$test_param[c(2,4,6,8,10,12)] <- NA
experdet$type2 <- c(rep(c("neg", "pos"), 4), rep(c("regular", "reverse"), 2))
names(experdet) <- c("Parameter", "Condition", "Easy Value", "Medium Value", "Hard Value")

print(xtable::xtable(experdet, caption = "All conditions used for the MTurk experiment. For parameters $\\beta_1, \\beta_2, \\beta_3,$ and $\\beta_5$ M1 served as null model. For $\\beta_4$ and $\\beta_6$, null model M1 and the alternative model switch roles in the reversed lineups, i.e. five plots show data simluated from the laternative model and only one plot shows data from M1.", label = "tab:experdetail", digits = c(0,0,0,3,4,3)), include.rownames=FALSE, 
      hline.after = c(-1,0,0,8,8,12))
@

\section{Results}\label{sec:res}

We recruited 250 participants for our experiment through Amazon Mechanical Turk. Each participant was presented with some brief training material before beginning the experiment. After agreeing to participate, the participants were shown two trial plots, one where the data plot was the most different from the others due to its relatively complex structure, while the other trial included a data plot that was most different from the others due to its comparatively simple structure. Only when participants were able to correctly identify the data plot from the trial lineups were they allowed to begin the experiment. Each participant was randomly assigned 13 lineups to look at. The were asked to select one or more plots that they perceived as ``most different" from the others, and provide a reasoning for their choice. They could select from ``Most simple overall structure" or ``Most complex overall structure", \st{corresponding to what they saw in the examples and trials, or they could choose }``Other" and provide their own text description of their reasoning. \st{These language in these reasons is purposefully vague: we want participants to tell us what they see. We do not want to tell them what they ``should" be seeing. In this way, we can truly determine what  effects are \textit{noticeable}.}

Twelve of the 13 lineups that the participants saw were used for the significance testing and the visual power methods discussed in Sections~\ref{sec:setupst} and \ref{sec:setupvp}. %The six parameters, $\beta_1, \dots, \beta_6$ were set to three different values according to how difficult we thought picking the data plot from the lineup would be for our participants, and were also set to be less than or greater than the initial estimate, creating the lineup type variable.%\hh{what about the first 12 plots? 6 parameters, 3 levels, 2 directions}
The final of the 13 lineups shown to participants contained the true data from the 112th senate shown in Section~\ref{sec:senatedata} placed among five other plots from models M3, M4, M5, and M7 as discussed in Section~\ref{sec:gofsetup}. Each participant only saw the data one time in order to avoid bias. Upon completion of the 13 lineups, each participant was paid \$1.75.   

\subsection{Significance Testing}\label{sec:sigtest}

<<sigtestWald, eval = FALSE, echo = FALSE>>=
SenBasic <- getEffects(senateSiena)
Senjtt_p <- includeEffects(SenBasic, "jumpXTransTrip", include = TRUE, type = "eval", interaction1 = "party", character = TRUE)
myalg <- sienaAlgorithmCreate( projname = Sys.time() , n3 = 1000)
fits <- siena07(myalg, data = senateSiena, effects = Senjtt_p, returnDeps = TRUE,
                                      batch=TRUE, verbose = FALSE, silent = TRUE)
thet <- fits$theta
sig <- fits$covtheta
Wald.RSiena(A = c(0,0,1), fits)
th <- c(0,0,1) %*% thet
covmat <- c(0,0,1) %*% sig %*% c(0,0,1)
csq <- drop(th %*% solve(covmat) %*% th)
1 - pchisq(csq, 1)
@

For a SAOM, there are two ways a conventional significance test of the parameters can be performed. In \texttt{RSiena}, there are $t$-type and Wald-type tests for a single parameter and for multiple parameters. The $t$-type test statistic is simply the parameter estimate divided by its standard error, and compared to a standard normal distribution \citep{RSienamanual}. The Wald-type test statistic for a single parameter, $\beta_k$ is 
\begin{equation}\label{eq:wald1}
\frac{(\hat{\beta_k})^2}{var(\hat{\beta_k})} \sim \chi^2_1,
\end{equation}
which is compared to a Chi-square distribution with one degree of freedom\citep{RSienamanual}. Testing the significance of multiple parameters depends on the hypothesis we wish to test, and a $P \times K$ matrix, $A$, must be appropriately designed to test the $P$ hypotheses of interest. The null hypothesis is that $A\boldsymbol{\beta} = \mathbf{0}$, and the test statistic is    
\begin{equation}\label{eq:wald2}
(A\boldsymbol{\hat{\beta}})' \hat{\Sigma}^{-1} A\boldsymbol{\hat{\beta}} \sim \chi^2_P,
\end{equation}
where $\hat{\Sigma}$ is the estimated covariance matrix of $\boldsymbol{\beta}$. This statistic is then compared to a Chi-square distribution with $P$ degrees of freedom. 

Both the parameters we test for significance using the lineup protocol, $\beta_3$ and $\beta_4$, were determined to be statistically significant using Equation~\ref{eq:wald1}. The correspoding results from the significance tests we performed using the lineup protocol are given in Table~\ref{tab:sigtesting}. \st{Corresponding to traditional methods, if enough participants pick out the alternative plot to result in a $p$-value less than 0.05, we reject the null hypothesis that the true value of the additional parameter, either $\beta_3$ or $\beta_4$, is equal to zero.} %\hh{XXX how do we interpret these  visual results? Include the six lineups in an appendix and refer to them from here.} \st{XXX should be taken care of in next paragraph now.}

<<sigtesting, results='asis'>>=
turk22_sig <- turk22 %>% 
  mutate(type2 = ifelse(sign == 0, sign(initialEst), sign),
         type2 = ifelse(type == "one", type2, -1)) %>% 
  filter(param_value == 1)
summ_sig <- turk22_sig %>% group_by(pic_id, test_param) %>% 
  summarize(npick = sum(datapick), n = n(), 
            pv = map2_dbl(npick, n, pV, m=6, scenario=3))
names(summ_sig) <- c("Lineup ID", "parameter", "# Alt. Model Picks", "Total Views", "p-value")
summ_sig$parameter <- as.factor(summ_sig$parameter)
levels(summ_sig$parameter) <- c("beta3", "beta4")
print(xtable::xtable(summ_sig, caption = "Experiment results for the two parameters for which we performed significance tests. There were three lineups for each parameter, so there are three results for each plot.", label = "tab:sigtesting", digits = c(0,0,0,0,0,5)), include.rownames = FALSE)
@

\st{The $p$-values were calculated using the \texttt{vinference} package by \citet{vinference}. This package contains methods to calculate \textit{Visual distributions} for lineup experiment data.  The distribution depends on the number of evaluations of a plot, $K$, the size of the lineup, $m$, and the lineup scenario, which here is that each lineup containing the same data and the same set of null plots is shown to $K$ independent observers. The visual inference family of distributions is similar to the binomial distribution, but takes the dependency among the $m$ plots in a single lineup shown to multiple viewers into account. Using these $p$-values, all but one lineup results in a rejection of the null hypothesis at Type-I error rate of $\alpha = 0.05$.}. We see that the $p$-values for visual inference here are highly variable.

<<lineup3132, fig.height=4, fig.cap="The lineup which caused a rejection of the null hypothesis that $\\beta_3$ = 0. The network simulated from model M3 is found in panel $\\sqrt{16} - 1$, and the remaining panels show networks simulated from model M1.">>=
dat <- read_csv("data/jttp_pos_hard_2.csv")
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size =1) + 
        ThemeNet + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

<<lineup3133, fig.height=4, fig.cap="One of the lineups which failed to reject the null hypothesis that $\\beta_3$ = 0. The network simulated from model M3 is found in panel $\\sqrt{25} - 4$, and the remaining panels show networks simulated from model M1.">>=
dat <- read_csv("data/jttp_pos_hard_3.csv")
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size =1) + 
        ThemeNet + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

\st{The lineup for significance testing of $\beta_3$ which resulted in a very small $p$-value and rejection of the null hypothesis is shown in Figure~\ref{fig:lineup3132}. Another significance lineup for model M3, which resulted in failure to reject the null hypothesis, is shown in Figure~\ref{fig:lineup3133}. When viewing Figure~\ref{fig:lineup3132}, 26 of 31 viewers chose the plot from M3, while only 2 of 27 chose the plot from M3 when viewing Figure~\ref{fig:lineup3133}. The most common choice in the latter was panel two, which 16 of 27 viewers chose as the most different due to its large connected component, making it seem more complex than the others. In viewing these two lineups, it is evident that there is a large amount of variability. It is difficult to see that five of the six come from the same model when they can all look different in their own way.} Thus, the variability in results is introduced through the null plots generated from M1, as not all simulations look alike. In addition, the necessarily small number of null plots do not give the viewer as complete of a view of the null model as the usual 19 null plots would. The results of the significance tests given in Table~\ref{tab:sigtesting} for $\beta_3$ and $\beta_4$ are not \st{definitive. For the test of $\beta_3$, two of the three tests are not significant, while the third is highly significant. For the test of $\beta_4$, one test is significant, one is decidedly not significant, and the third is significant at the level of 0.10.}  Thus, unlike the Wald-type tests described at the beginning of this section, there is no way to \st{decisively} reject or to fail to reject the null hypothesis that the parameter value is 0. We include all of the lineups shown to our participants in the appendix. 

\subsection{Goodness-of-Fit Testing}

Goodness-of-fit testing for network models is notoriously difficult. Most network models, other than the most simple, lack the necessary asymptotics for developing goodness-of-fit methods \citep{goldenberg09}. Some methods have been developed based on what \citeauthor{RSienamanual} call ``auxiliary statistics" such as the indegree or outdegree distribution on the nodes. In \texttt{RSiena}, the \texttt{sienaGOF} function performs goodness-of-fit testing as follows:
\begin{enumerate}
\item Auxiliary statistics, \st{such as the cumulative outdegree counts on the nodes,} are computed on the observed data ($\mathbf{u}_d$) and on $N$ simulated observations from the model ($\mathbf{u}_1 \dots \mathbf{u}_N$). (Usually, $N=1000$) 
\item The mean vector, $\overline{\mathbf{u}}$ and covariance matrix, $\mathbf{S}$ of the statistics on the simulations from the model are computed, and the Mahalanobis distance, $d_M(\mathbf{u})$ from the observed statistics to the distribution of the simulated statistics is computed:
\begin{equation}\label{eq:mahal}
d_M(\mathbf{u}) = \sqrt{(\mathbf{u} - \overline{\mathbf{u}})' S^{-1} (\mathbf{u} - \overline{\mathbf{u}})}
\end{equation}
\item The Mahalanobis distance for each of the $N$ simulations is calculated and $d_M(\mathbf{u}_d)$ is compared to this distribution of distances.
\item An empirical $p$-value is found by computing the proportion of simulated distances found in step 4 that are as large or larger than $d_M(\mathbf{u}_d)$. A SAOM is thus considered a good fit to the data if $p$ is large. A plot comparing the data to the simulations is also considered, and a similar plot is shown in Figure~\ref{fig:gofsiena} \st{for the outdegree distribution of small data set, shown in the points and connected lines, with the simulated values of $\mathbf{u}_d$ shown in boxplots and overlaid violin plots.}
\end{enumerate}

<<gofsiena, fig.height=3, fig.cap="An example of what a goodness-of-fit plot from \\texttt{RSiena} looks like. The overlaid boxplots and violin plots show the distribution of each of the outdegree count values on the simulated networks, and the red points and lines are the observed data values.", cache = TRUE>>=
load("data/ansnullpaper.rda")
library(lattice)
gof1 <- sienaGOF(ansnull, OutdegreeDistribution, varName ="friendship", join = FALSE)
sims1 <- data.frame(gof1$`Period 1`$Simulations)
dat1 <- gof1$`Period 1`$Observations
dat1 %>% data.frame() %>% gather(outdegree, val) %>%
  mutate(outdegree = parse_number(outdegree)-1) -> dat1
p1 <- sims1 %>%
  mutate(sim = row_number()) %>%
  gather(outdegree, val, X1:X9) %>%
  mutate(outdegree = parse_number(outdegree) - 1) %>%
  filter(outdegree <= 6) %>%
  ggplot(aes(x = outdegree, y = val)) + 
  geom_boxplot(aes(group = outdegree), size = .5, outlier.shape = "x", outlier.size = 5) + 
  geom_violin(aes(group = outdegree), bw = 2, fill = NA) + 
  geom_point(data = dat1, color = 'red') + 
  geom_line(data= dat1, color = 'red') + 
  geom_text(data = dat1, aes(label = val), hjust = -.5) + 
  labs(x = "Outdegree (p = 0.154)", y = "Statistic", 
       title = "Goodness-of-Fit: Outdegree distribution period 1") + 
  ThemeNoNet
p1
@

The \texttt{RSiena} software also provides a Rao score-type test for goodness-of-fit for assessing one or more parameters, the test statistic of which is compared to a Chi-square distribution with $P$ degrees of freedom, where $P$ has the same definition as in Section~\ref{sec:sigtest}. For full detail on the score-type test, see \citet{scoretest}. 

These methods are both restriced: the \texttt{sienaGOF} method only considers \textit{one} measure on the data and simulations from the model, while the score-type tests only consider \textit{subsets} of parameters, ``nuisance parameters" in \citet{scoretest}, not the entire set of parameters. By using visual inference instead of more traditional statistical methods, we hope to perform a more \textit{holistic} goodness-of-fit test. 

Using the lineup protocol, we show each Amazon Mechanical Turk worker the data once, in a lineup with five other plots of simulated data from one of the models we chose. We examined four different models, M3, M4, M5, and M7, and examined three repetitions of each, for a total of 12 goodness-of-fit lineups. In each lineup, the ``null model" is one of the four models and the ``alternative" model is the true, unknown model that generated the senate network data. The hypotheses for our goodness-of-fit tests are: 
\begin{description}
\item $H_0$:  The senate network data come from (or could have come from) the null model, M$i$.
\item $H_A$: The senate network data do not come from the null model. 
\end{description}
If a lineup viewer picks out the data among the five simulations from the null model, it is evidence against the null hypothesis. On the contrary, if the lineup viewer picks one of the null plots, that is evidence in favor of the null hypothesis. Because the size of the lineups is small, the probability of picking the data by chance is high, $\frac{1}{6}$, but if \textit{many} independent viewers pick out the data from the nulls, the evidence in favor of the alternative hypothesis becomes stronger. Results from our MTurk goodness-of-fit plots are provided in Table~\ref{tab:gofstats}. 

<<goftab, cache = TRUE, results='asis'>>=
turk22_gof <- read_csv("data/turk22-gof.csv")
turk22_gof_stats <- turk22_gof %>% group_by(pic_id) %>% 
  summarize(npickdata = sum(datapick), total = n(), 
          pvinf = map2_dbl(npickdata, total, pV, m = 6, scenario = 3))
turk22_gof_stats %>% separate(pic_id, into = c("discard", "model", "rep"), sep = c(2,3)) -> print_gof_stats
print_gof_stats %>% select(-discard) %>% mutate(model = paste0("M", (as.integer(model))))-> print_gof_stats
names(print_gof_stats) <- c("Model", "Replicate", "Data Picks", "Total Viewers", "p-value")
print_gof_stats$Model[c(2:3,5:6,8:9,11:12)] <- ""
print_gof_stats$Model[2] <- "jtt party"
print_gof_stats$Model[5] <- "jtt sex"
print_gof_stats$Model[8] <- "stt party"
print_gof_stats$`p-value` <- ifelse(print_gof_stats$`p-value` < .0001, "< 0.0001", sprintf("%.4f", print_gof_stats$`p-value`))
print(xtable::xtable(print_gof_stats, label = "tab:gofstats", caption = "An overview of the results from the 12 goodness-of-fit lineup tests.", digits = 4,
                      align="rlrrrr"), caption.placement = "bottom", include.rownames = FALSE, hline.after = c(-1,0,0,3,6,9,nrow(print_gof_stats)))
@

\st{The $p$-values were again computed using the \texttt{vinference} package by \citet{vinference}}. The lineup that resulted in a failure to reject the null hypothesis is shown in Figure~\ref{fig:failgof}. The null model in this lineup is M5, and the senate data is shown in panel number $3^2 - 7$. However, the panel most participants chose was number four, and the most common reasoning for that choice was that it had the most simple structure. \st{Some of the other panels, such as three and six, in Figure~\ref{fig:failgof} have large connected components that are similar in size to the connected component of the data plot shown in panel two. Thus, model M5 is sometimes capable of capturing the network structure of the senate collaboration data.}

<<failgof, fig.height=4, fig.cap="The goodness-of-fit lineup that failed to reject the null hypothesis. The null model for this lineup is M5. Only 7 of 20 viewers of this lineup selected the data plot as the most different from the others. The most commonly chosen panel was number four, which has a relatively simple structure compared to panels 2, 3, and 6 especially.">>=
load("data/se112adjmat.RDS")
wave2 <- data.frame(se112adj)
senators <- data.frame(id = colnames(wave2), number = 1:155)
wave2$from <- colnames(wave2)
wave2 <- wave2 %>% gather(to, val, Alan.Stuart.Franken:William.Cowan) %>% filter(val > 0)
wave2 <- merge(wave2, senators, by.x = "from", by.y = "id", all = T)
wave2$from <- senators$number[match(wave2$from, senators$id)]
wave2$to <- senators$number[match(wave2$to, senators$id)]
wave2$from <- paste0("V", wave2$from)
wave2$to <- ifelse(is.na(wave2$to), NA, paste0("V", wave2$to))
wave2 <- wave2 %>% mutate(sim = 1001, model = "data", wave = 1) %>% select(-c(val, number))
dat <- read_csv(paste0("data/jtts_gof_9_2.csv"))
datplot <- unique(dat$ord[dat$sim == 1001])
dat <- dat %>% filter(sim != 1001) %>% 
      bind_rows(wave2) %>% 
      mutate(ord = ifelse(is.na(ord), datplot, ord))
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size = .5) + 
        theme_net() + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

The smallest $p$-value for one of the goodness-of-fit lineups was for the third replicate of the null model M5. This \st{result contrasts with} our previous finding that the only lineup to fail to reject the null was also when the null model was M5. This lineup is shown in Figure~\ref{fig:mostsiggof}. In the remaining replicate of M5 as the null model, 13 of 16 viewers identified the data plot, corresponding to a $p$-values of less than $0.0001$, just like the third replicate. This variability in results is similar to the variability we found in Section~\ref{sec:sigtest}. This variability is again introduced through the plots simulated from null model, and does not provide us with a clear cut decision resulting the hypothesis test. For model M5, we can neither reject nor fail to reject the null hypothesis that the data come from model M5. This is evidence that the goodness-of-fit of network models cannot always be determined by one dimensional derived features, such as $p$-value shown on the $x$-axis in Figure~\ref{fig:gofsiena}.   %This leads us to believe that the high $p$-value observed in the second replicate is more of a fluke. %This demonstrates some of the problems inherent with network models, such as model degeneracy, as discussed in \citet{degeneracy}. Network models 
%Furthermore, for the first lineup replicate with model M5 as the null model, only 9 of 21 participants were able to pick out the data, but the Visual distribution $p$-value is less than 0.05. %This is much higher than the corresponding Binomial probability of $Pr(X = 9|n = 21, p = \frac{1}{6}) = \binom{21}{9}(\frac{1}{6})^9(\frac{5}{6})^{21-9} \approx 0.003$, but we are not sure that having fewer than half of our participants identify the data plot is a truly significant result. We could move the threshold from $\alpha = 0.05$ to $\alpha = 0.01$ or $\alpha = 0.001$, but more work needs to be done to determine which, if any of these, is appropriate.  

<<gofm71, fig.height=4, fig.cap="One repitition of a goodness-of-fit lineup testing modle M7. The senate data are shown in panel two, and it is evident that none of the other five panels, which show data simulated from model M7, come close to creating the large connected component that is central to the structure of the senate data.">>=
dat <- read_csv(paste0("data/bigmod_gof_9_1.csv"))
datplot <- unique(dat$ord[dat$sim == 1001])
dat <- dat %>% filter(sim != 1001) %>% 
      bind_rows(wave2) %>% 
      mutate(ord = ifelse(is.na(ord), datplot, ord))
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size =1) + 
        ThemeNet + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@
For the other models for which we tested goodness-of-fit, however, we \textit{do} have significant evidence from all three replicates to reject the null hypothesis that the null model generated the data. \st{For models M3, M5, and M7, these goodness-of-fit tests have rejected the null hypotheses that the senate data come from these models. We hypothesized that the model with the most effects, M7, would be the best fit. However, as shown in Figure~\ref{fig:gofm71}, the model does not capture the overall structure very well at all.} The rest of the goodness-of-fit lineups as shown to participants are provided in the appendix. 

%\hh{XXX we need to go throught the logic of the argument below.}\st{ XXX I reformulated the discussion per our convo today.  }

<<mostsiggof, fig.height = 4, fig.cap="The lineup resulting in the smallest $p$-value rejecting the null hypothesis. Surprisingly, this another repetition for M5 as the null model.">>=
dat <- read_csv(paste0("data/jtts_gof_9_3.csv"))
datplot <- unique(dat$ord[dat$sim == 1001])
dat <- dat %>% filter(sim != 1001) %>% 
      bind_rows(wave2) %>% 
      mutate(ord = ifelse(is.na(ord), datplot, ord))
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size = .5) + 
        theme_net() + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

We believe this goodness-of-fit testing method holds promise for the future of social network analysis. The participants in our experiments are very good overall at picking out the data when it is noticeably different from the null plots in the lineups. In addition, as in replicate three for null model M4, when the null plots contain similarly sized structures as the data plot, our participants have a hard time distinguishing the data. We believe that running these tests multiple times using several different sets of null models to adequately explore the possible structures generated by the models is a step in the right direction for a more comprehensive goodness-of-fit test for network models. 

\subsection{Visual Power}

A summary of the results from our experiment is shown \st{as points }in Figure~\ref{fig:predictglmm}. On the $x$ axis, we plot the value of the parameter of interest, and on the $y$ axis, the proportion of times the alternative data plot was picked out for each lineup. The results are split into groups based on the value of the parameter and the lineup type. We can see clear patterns \st{in the added parameters $\beta_3, \dots, \beta_6$ }: as the parameter value approaches 0, fewer participants identified the alternative plot. Similarly, as $\beta_1, \beta_2$ approach their estimated values $\hat{\beta_1}, \hat{\beta_2}$, fewer people are able to identify the alternative plot. 


<<readturk22, fig.height=6, fig.cap="A summary of some results from the Amazon mechanical turk study. On the $x$-axis, the value of the parameter changed in the model of interest, and on the $y$-axis, the proprotion of times turkers identified the different data plot. A simple linear regression is fit to each group. The pattern as the parameter value approaches zero is clear in all conditions.", eval = FALSE>>=
turk22 %>% mutate(type2 = ifelse(sign == 0, sign(initialEst), sign),
                       type2 = ifelse(type == "one", type2, -1)) %>% 
  group_by(pic_id, size, test_param, sign, type2,alt_model) %>% 
  summarize(datapick = mean(datapick)) %>%
  filter(pic_id != 3132) %>% 
  ggplot(aes(x = size, y = datapick, colour=factor(type2))) + 
  geom_point() +
  scale_color_brewer(palette = "Dark2", name = "Lineup Type") + 
  facet_wrap(~test_param, scales="free", labeller = 'label_both') +
  geom_smooth(se=FALSE, method="lm") + 
  ThemeNoNet + 
  theme(legend.position = 'bottom')
@

We further explore this relationship between identification of the alternative data in the lineup and the parameter \st{of interest}, effect size, and lineup type with a generalized linear mixed model that provides us with an estimate of the power of the visual \st{significance } test. The response variable, $Y_{ijkm}$, is binary, indicating whether participant $m$ picked the alternative data plot in lineup type $j$, rep $k$, for effect $i$. There is one continuous covariate $x$, which is the centered and scaled size of the effect of interest from which the alternative data were simulated, the values of which are labeled ``easy", ``medium", and ``hard" in Table~\ref{tab:experdetail} according to how difficult we thought the Turk participants would find each lineup. In Equation~\ref{eq:glmm}, $i \in \{1, 2, 3, 4, 6\}$ corresponds to the effects $\beta_1,\dots, \beta_6$, respectively, $j \in \{-1,1\}$, and $k \in \{1,2,3\}$. We also include random effects in the model: one for each lineup, $\delta_{ijk}$, and one for each participant, $\epsilon_m$, and fit a hierarchical model given in Equation~\ref{eq:glmm}. 

\begin{align}
\begin{split}
Y_{ijkm} &\sim \text{Bernoulli}(\pi_{ijkm}) \\
\text{logit}(\pi_{ijkm}) & = \alpha_{ij} + \gamma_{ij} x + \delta_{ijk} + \epsilon_{m} \label{eq:glmm} \\
\delta_{ijk} & \stackrel{iid}{\sim} N(0, \sigma^2_{\delta}) \\
\epsilon_{m} & \stackrel{iid}{\sim} N(0, \sigma^2_{\epsilon})
\end{split}
\end{align}

% \hh{
% $\ell$ encompasses all 12 combinations of parameter and positive/negative direction of the effect size.
% }

% \begin{align}
% \begin{split}
% Y_{i\ell m} &\sim \text{Bernoulli}(\pi_{i\ell m}) \\
% \text{logit}(\pi_{i\ell m}) & = \mu + \alpha_{i} + \theta\mathbb{I}(x_{\ell 1} = 1) + \gamma x_{\ell 2} + \\ & (\alpha\theta)_i \mathbb{I}(x_{\ell 1} = 1) + (\alpha\gamma)_i x_{\ell 2} + (\theta\gamma)\mathbb{I}(x_{\ell 1} = 1)x_{\ell 2} + \\ & (\alpha\theta\gamma)_i\mathbb{I}(x_{\ell 1} = 1) x_{\ell 2} + \delta_{\ell} + \epsilon_{m} \label{eq:glmm} \\
% \delta_{\ell} & \sim N(0, \sigma^2_{\delta}) \\
% \epsilon_{m} & \sim N(0, \sigma^2_{\epsilon})
% \end{split}
% \end{align}

%\hh{XXX define all of the parameters above XXX}
%\st{
%This model estimates, as a function of the parameter size, the expected value of $\text{logit}(\pi_{ijkm})$ for each of the 12 lineup conditions, repitions, and participants. The inverse logit of the estimates are the prediction lines drawn in Figure~\ref{fig:predictglmm}.

The results of fitting this model, including estimates of parameters, standard errors, $p$-values, and the odds ratio multipliers, using \texttt{glmer} from the \texttt{lme4} package are summarized in Table~\ref{tab:glmmests} \citep{lme4}. For each combination of parameter and lineup type, the expected value of the link function for a new lineup and a new observer with parameter value $x$ is 
\begin{equation}
E[\text{logit}(\pi_{ij})] = \alpha_{ij} + \gamma_{ij}x
\end{equation}
and the corresponding probability of picking out the alternative data plot is 
\begin{equation}
\pi_{ij} = \frac{\exp\{\alpha_{ij} + \gamma_{ij}x\}}{1 + \exp\{\alpha_{ij} + \gamma_{ij}x\}}
\end{equation}

%The baseline condition for this model against which all other experimental conditions are compared is the first condition in Table~\ref{tab:experdetail}, for parameter $\beta_1$ and lineup type -1. The expected value of the link function for this scenario with $\beta_1 = x_{12}$ is $\mu + \gamma x_{12}$, and the expected probability that a new observer will pick out the data plot in a new lineup generated from this scenario is $\frac{\exp(\mu + \gamma x_{12})}{1+\exp{\mu + \gamma x_{12}}}$. Similarly, for a new lineup generated from the second condition in in Table~\ref{tab:experdetail}, with $\beta_1 = x_{12}$, and lineup type 1, the expected value of the link function is $\mu + \theta + (\gamma + (\theta\gamma))x_{12}$ and corresponding probability a new observer will pick out the data plot in a new lineup is $\exp\{\mu + \theta + (\gamma + (\theta\gamma))x_{12}\} / 1+\exp\{\mu + \theta + (\gamma + (\theta\gamma))x_{12}\}$. 

% For any of the remaining parameters, $\beta_2, \dots, \beta_6$, with lineup type -1, the expectation of the link function as a function of the parameter value, $x_{\ell 2}$ is: 
% \begin{equation}\label{eq:predneg1}
% \mu + \alpha_{i} +  (\gamma + (\alpha\gamma)_i)x_{\ell 2}  
% \end{equation}
% where $i \in \{1,2,3,4,5\}$ corresponds to $\beta_3, \beta_4, \beta_2, \beta_6, \beta_5$, respectively. Similarly, for lineup type 1, the expectation of the link function as a function of the parameter value is: \begin{equation}\label{eq:predpos1}
% \mu + \alpha_{i} + \theta + (\alpha\theta)_i  +  (\gamma + (\alpha\gamma)_i + (\gamma\theta) + (\alpha\theta\gamma)_i)x_{\ell 2}  
% \end{equation}
% 
% The corresponding expected probability that a new observer viewing a new lineup from this scenario is: 
% \begin{equation}
% \frac{\exp\{\mu + \alpha_{4} + \theta + \gamma + (\alpha\theta)_4 + (\alpha\gamma)_4  + (\theta\gamma) + (\alpha\theta\gamma)_4\}}{1 + \exp\{\mu + \alpha_{4} + \theta + \gamma + (\alpha\theta)_4 + (\alpha\gamma)_4  + (\theta\gamma) + (\alpha\theta\gamma)_4\}}
% \end{equation}

<<glmmres>>=
load("data/finalglmm31-2.RDA")
mod <- model2randomscalesize
res <- summary(mod)[[10]]
ests <- as.numeric(round(res[,1], 4))
oddsidx <- which(exp(ests) < .0001)
oddsidx2 <- which(exp(ests) > 1000000)
odds <- round(exp(ests), 4)
odds[oddsidx] <- "$<$0.0001"
odds[oddsidx2] <- "$>$1e+06"
se <- as.numeric(round(res[,2], 4))
pvalidx <- which(res[,4] < .0001)
pval <- sprintf("%.4f",round(res[,4],4))
pval[pvalidx] <- "$<$0.0001"
sigma_epsilon <- summary(model2randomscalesize)[[13]]$nick_name[[1]]
sigma_delta <- summary(model2randomscalesize)[[13]]$pic_id[[1]]
@


\npdecimalsign{.}
%\nprounddigits{3}

\begin{table}
\centering
\begin{tabular}{cn{5}{2}n{5}{3}rr}\hline
Parameter & \multicolumn{1}{r}{Estimate} & \multicolumn{1}{r}{Std Error} & $p$-value & Odds Multiplier \\ \hline\hline
$\alpha_{1+}$ & \Sexpr{ests[7]} & \Sexpr{se[7]} & \Sexpr{pval[7]}$^{\ddagger}$ & \Sexpr{odds[7]} \\
$\alpha_{1-}$ & \Sexpr{ests[1]} & \Sexpr{se[1]} & \Sexpr{pval[1]}$^{\dagger}$ & \Sexpr{odds[1]}\\
$\gamma_{1+}$ & \Sexpr{ests[19]} & \Sexpr{se[19]} & \Sexpr{pval[19]}$^{\ddagger}$ & \Sexpr{odds[19]} \\
$\gamma_{1-}$  & \Sexpr{ests[13]} & \Sexpr{se[13]} & \Sexpr{pval[13]}$^{\dagger}$ & \Sexpr{odds[13]} \\ \hline
$\alpha_{2+}$  & \Sexpr{ests[10]} & \Sexpr{se[10]} & \Sexpr{pval[10]} & \Sexpr{odds[10]} \\
$\alpha_{2-}$  & \Sexpr{ests[4]} & \Sexpr{se[4]} & \Sexpr{pval[4]}$^{\ddagger}$ & \Sexpr{odds[4]} \\
$\gamma_{2+}$ & \Sexpr{ests[22]} & \Sexpr{se[22]} & \Sexpr{pval[22]}$^{*}$ & \Sexpr{odds[22]} \\
$\gamma_{2-}$  & \Sexpr{ests[16]} & \Sexpr{se[16]} & \Sexpr{pval[16]}$^{\ddagger}$ & \Sexpr{odds[16]} \\ \hline
$\alpha_{3+}$  & \Sexpr{ests[8]} & \Sexpr{se[8]} & \Sexpr{pval[8]}$^{\dagger}$ & \Sexpr{odds[8]} \\ 
$\alpha_{3-}$ & \Sexpr{ests[2]} & \Sexpr{se[2]} & \Sexpr{pval[2]}$^{\dagger}$ & \Sexpr{odds[2]} \\
$\gamma_{3+}$ & \Sexpr{ests[20]} & \Sexpr{se[20]} & \Sexpr{pval[20]}$^{\dagger}$ & \Sexpr{odds[20]}  \\
$\gamma_{3-}$   & \Sexpr{ests[14]} & \Sexpr{se[14]} & \Sexpr{pval[14]}$^{*}$ & \Sexpr{odds[14]} \\ \hline
$\alpha_{4+}$ & \Sexpr{ests[9]} & \Sexpr{se[9]} & \Sexpr{pval[9]}$^{**}$ & \Sexpr{odds[9]} \\
$\alpha_{4-}$ & \Sexpr{ests[3]} & \Sexpr{se[3]} & \Sexpr{pval[3]}$^{**}$ & \Sexpr{odds[3]}  \\
$\gamma_{4+}$ & \Sexpr{ests[21]} & \Sexpr{se[21]} & \Sexpr{pval[21]}$^{**}$ & \Sexpr{odds[21]} \\ 
$\gamma_{4-}$  & \Sexpr{ests[15]} & \Sexpr{se[15]} & \Sexpr{pval[15]} & \Sexpr{odds[15]} \\ \hline
$\alpha_{5+}$  & \Sexpr{ests[12]} & \Sexpr{se[12]} & \Sexpr{pval[12]}$^{*}$ & \Sexpr{odds[12]} \\
$\alpha_{5-}$  & \Sexpr{ests[6]} & \Sexpr{se[6]} & \Sexpr{pval[6]}$^{\ddagger}$ & \Sexpr{odds[6]} \\
$\gamma_{5+}$ & \Sexpr{ests[24]} & \Sexpr{se[24]} & \Sexpr{pval[24]}$^{*}$ & \Sexpr{odds[24]} \\ 
$\gamma_{5-}$ & \Sexpr{ests[18]} & \Sexpr{se[18]} & \Sexpr{pval[18]}$^{\ddagger}$ & \Sexpr{odds[18]} \\ \hline
$\alpha_{6+}$  & \Sexpr{ests[11]} & \Sexpr{se[11]} & \Sexpr{pval[11]} & \Sexpr{odds[11]} \\
$\alpha_{6-}$ & \Sexpr{ests[5]} & \Sexpr{se[5]} & \Sexpr{pval[5]}$^{\ddagger}$ & \Sexpr{odds[5]}  \\
$\gamma_{6+}$ & \Sexpr{ests[23]} & \Sexpr{se[23]} & \Sexpr{pval[23]} & \Sexpr{odds[23]} \\
$\gamma_{6-}$ & \Sexpr{ests[17]} & \Sexpr{se[17]} & \Sexpr{pval[17]}$^{\ddagger}$ & \Sexpr{odds[17]} \\ \hline
$\sigma^2_{\delta}$ & \Sexpr{round(sigma_delta, 4)} & \multicolumn{1}{r}{--} & -- & --\\
$\sigma^2_{\epsilon}$ &  \Sexpr{round(sigma_epsilon, 4)}  & \multicolumn{1}{r}{--} & -- & --\\
\hline 
\end{tabular}
\caption{\label{tab:glmmests}Summary of the results from fitting the model given in Equation~\ref{eq:glmm}. Significance levels: * - $< 0.10$; ** - $<0.05$; $\dagger$ - $<0.01$; $\ddagger$ - $<0.001$}
\end{table}

% \begin{table}
% \centering
% \begin{tabular}{lcccc}
% Parameter & Estimate & Std Error & $p$-value & Odds Multiplier \\
% $\mu$ & \Sexpr{ests[1]} & \Sexpr{se[1]} & \Sexpr{pval[1]} & \Sexpr{odds[1]}\\
% $\alpha_1$ & \Sexpr{ests[2]} & \Sexpr{se[2]} & \Sexpr{pval[2]} & \Sexpr{odds[2]} \\
% $\alpha_2$ & \Sexpr{ests[3]} & \Sexpr{se[3]} & \Sexpr{pval[3]} & \Sexpr{odds[3]}  \\
% $\alpha_3$  & \Sexpr{ests[4]} & \Sexpr{se[4]} & \Sexpr{pval[4]} & \Sexpr{odds[4]} \\
% $\alpha_4$ & \Sexpr{ests[5]} & \Sexpr{se[5]} & \Sexpr{pval[5]} & \Sexpr{odds[5]}  \\
% $\alpha_5$  & \Sexpr{ests[6]} & \Sexpr{se[6]} & \Sexpr{pval[6]} & \Sexpr{odds[6]} \\
% $\theta$  & \Sexpr{ests[7]} & \Sexpr{se[7]} & \Sexpr{pval[7]} & \Sexpr{odds[7]} \\
% $\gamma$  & \Sexpr{ests[8]} & \Sexpr{se[8]} & \Sexpr{pval[8]} & \Sexpr{odds[8]} \\ 
% $(\alpha\theta)_1$  & \Sexpr{ests[9]} & \Sexpr{se[9]} & \Sexpr{pval[9]} & \Sexpr{odds[9]} \\
% $(\alpha\theta)_2$  & \Sexpr{ests[10]} & \Sexpr{se[10]} & \Sexpr{pval[10]} & \Sexpr{odds[10]} \\
% $(\alpha\theta)_3$  & \Sexpr{ests[11]} & \Sexpr{se[11]} & \Sexpr{pval[11]} & \Sexpr{odds[11]} \\
% $(\alpha\theta)_4$  & \Sexpr{ests[12]} & \Sexpr{se[12]} & \Sexpr{pval[12]} & \Sexpr{odds[12]} \\
% $(\alpha\theta)_5$  & \Sexpr{ests[13]} & \Sexpr{se[13]} & \Sexpr{pval[13]} & \Sexpr{odds[13]} \\
% $(\alpha\gamma)_1$  & \Sexpr{ests[14]} & \Sexpr{se[14]} & \Sexpr{pval[14]} & \Sexpr{odds[14]} \\
% $(\alpha\gamma)_2$  & \Sexpr{ests[15]} & \Sexpr{se[15]} & \Sexpr{pval[15]} & \Sexpr{odds[15]} \\
% $(\alpha\gamma)_3$  & \Sexpr{ests[16]} & \Sexpr{se[16]} & \Sexpr{pval[16]} & \Sexpr{odds[16]} \\
% $(\alpha\gamma)_4$  & \Sexpr{ests[17]} & \Sexpr{se[17]} & \Sexpr{pval[17]} & \Sexpr{odds[17]} \\
% $(\alpha\gamma)_5$  & \Sexpr{ests[18]} & \Sexpr{se[18]} & \Sexpr{pval[18]} & \Sexpr{odds[18]} \\
% $\theta\gamma$  & \Sexpr{ests[19]} & \Sexpr{se[19]} & \Sexpr{pval[19]} & \Sexpr{odds[19]} \\
% $(\alpha\theta\gamma)_1$ & \Sexpr{ests[20]} & \Sexpr{se[20]} & \Sexpr{pval[20]} & \Sexpr{odds[20]}  \\
% $(\alpha\theta\gamma)_2$  & \Sexpr{ests[21]} & \Sexpr{se[21]} & \Sexpr{pval[21]} & \Sexpr{odds[21]} \\
% $(\alpha\theta\gamma)_3$  & \Sexpr{ests[22]} & \Sexpr{se[22]} & \Sexpr{pval[22]} & \Sexpr{odds[22]} \\
% $(\alpha\theta\gamma)_4$  & \Sexpr{ests[23]} & \Sexpr{se[23]} & \Sexpr{pval[23]} & \Sexpr{odds[23]} \\
% $(\alpha\theta\gamma)_5$  & \Sexpr{ests[24]} & \Sexpr{se[24]} & \Sexpr{pval[24]} & \Sexpr{odds[24]} \\ 
% $\sigma^2_{\delta}$ & 0.5638 \\
% $\sigma^2_{\epsilon}$ & 0.3416
% \end{tabular}
% \caption{\label{tab:glmmests}Summary of the results from fitting the model given in Equation~\ref{eq:glmm}}
% \end{table}

<<predictglmm, fig.height=6, fig.cap='Predictions from our generalized linear mixed effects model given in Equation~\ref{eq:glmm}. The lines show the expected probability of detecting the alternative data in a lineup of size 6 for new observers of new lineups is plotted on the $y$-axis, and the size of the parameter of interest is on the $x$-axis. The proportions detected by our Turk participants for each lineup group are shown by the points, with the probability of picking out the data plot at random shown by a horizontal line at 1/6. The lineup marked as ``outlier" was removed from modeling. The panel for the reciprocity parameter, $\\beta_2$ is also presented in Figure~\\ref{fig:recipzoom} in more detail.'>>=
newdata <- read_csv("data/newdata_pred_glmm.csv")
newdata$param <- newdata$test_param
newdata$param <- as.factor(newdata$test_param)
levels(newdata$param) <-  c(1,3,4,2,6,5) 
newdata$param <- as.integer(as.character(newdata$param))
plotdata <- turk22 %>% mutate(type2 = ifelse(sign == 0, sign(initialEst), sign),
                       type2 = ifelse(type == "one", type2, -1)) %>% 
  group_by(pic_id, size, test_param, sign, type2,alt_model) %>% summarize(
    datapick = mean(datapick)
)
plotdata$param <- plotdata$test_param
plotdata$param <- as.factor(plotdata$param)
levels(plotdata$param) <- c(1,3,4,2,6,5) 
plotdata$param <- as.integer(as.character(plotdata$param))
labdat <- data_frame(test_param = "jttp", param = 3, type2 = -1, x = -5, y = .9, label = "jttp_neg_hard_2")

ggplot() + 
  geom_line(data = newdata, aes(x = size, y = predictfinal, color = as.factor(type2))) + 
  geom_point(data = plotdata, aes(x = size, y = datapick, color = as.factor(type2))) + 
  #geom_text(data = plotdata, aes(x = size, y = datapick, color = as.factor(type2), label = pic_id)) +
  geom_text(data = labdat, aes(x = x, y=y, label = label, color = as.factor(type2)),
            show.legend = F) + 
  geom_hline(yintercept = 1/6, linetype='dashed') +
  scale_color_brewer(palette = "Dark2", name = "Lineup Type") + 
  facet_wrap(~param, scales = 'free_x', 
             labeller = label_bquote(beta [.(param)])) + 
  labs(x = "Parameter value", y = "Expected probability of detection in a lineup of size 6") + 
  ThemeNoNet + 
  theme(legend.position = 'bottom')


#vals <- predict(model1, type = "response")
#modelData_sig$prediction <- vals
#ggplot(data = modelData_sig) + 
#  geom_point(aes(x = size, y = prediction, color = as.factor(type2))) + 
#  facet_wrap(~test_param, scales = "free")
@

In Figure~\ref{fig:predictglmm}, we see a clear trend in all parameters except $\beta_1$ and $\beta_2$ that as the parameter value approaches zero from either side, the probability of picking the data plot in a lineup of size six descreases. For $\beta_3$ and $\beta_5$, the slope of the fitted line is \textit{much} steeper for positive values of the parameter than for negative values, meaning that our participants perceived differences more often for postitive parameter values than for negative parameter values. This finding is similar to that of \citet{corrviz}, who found that people detect positive correlations sooner and better than negative correlations. 

<<morepredicts>>=
alphas <- (coef(mod)[[1]][,-1] %>% unique())[1:12] 
betas <- (coef(mod)[[1]][,-1] %>% unique())[13:24] 
names(betas) <- str_replace(str_replace(names(betas), "lps_param", ""), ":centersize", "")
names(betas) <- str_replace(names(betas), ".-1", "neg")
names(betas) <- str_replace(names(betas), ".1", "pos")
names(alphas) <- str_replace(names(alphas), "lps_param", "")
names(alphas) <- str_replace(names(alphas), ".-1", "neg")
names(alphas) <- str_replace(names(alphas), ".1", "pos")
source("code/newpredictions.R")
newdata <- newdata3
newdata$param <- newdata$test_param
newdata$param <- as.factor(newdata$test_param)
#levels(newdata$param) <-  c(1,3,4,2,6,5) 
#newdata$param <- as.integer(as.character(newdata$param))
plotdata <- turk22 %>% mutate(type2 = ifelse(sign == 0, sign(initialEst), sign),
                       type2 = ifelse(type == "one", type2, -1)) %>% 
  group_by(pic_id, size, test_param, sign, type2,alt_model) %>% summarize(
    datapick = mean(datapick)
)
plotdata$param <- plotdata$test_param
plotdata$param <- as.factor(plotdata$param)
#levels(plotdata$param) <- c(1,3,4,2,6,5) 
#plotdata$param <- as.integer(as.character(plotdata$param))
labdat <- data_frame(test_param = "jttp", param = 3, type2 = -1, x = -5, y = .9, label = "jttp_neg_hard_2")
@

<<recipzoom, fig.height=3, fig.cap='The top middle panel of Figure~\\ref{fig:predictglmm} expanded to show greater detail. The square root of the parameter value is shown on the $x$-axis. For this parameter, as its value approaches zero, the probability of identifying the alternate data model decreases, then increases, which is noticeably different from the pattern exhibited by the others. Again, a horizontal line is drawn at 1/6, the chance of selecting the data plot at random.'>>=
ggplot() + 
  geom_line(data = newdata %>% filter(test_param == "recip", category == "inside"), aes(x = sqrt(size), y = predictfinal, color = as.factor(type2)), size = 1.25) + 
  geom_point(data = plotdata %>% filter(test_param=="recip"), aes(x = sqrt(size), y = datapick, color = as.factor(type2)), size = 2) + 
  geom_line(data = newdata %>% filter(test_param == "recip", category == "outside"), aes(x = sqrt(size), y = predictfinal, group = as.factor(type2)), color = "gray60", alpha = .7) +
  geom_line(data = newdata %>% filter(test_param == "recip", category == "outside2"), aes(x = sqrt(size), y = predictfinal, group = as.factor(type2)), color = "gray60", alpha = .7) +
  #geom_text(data = plotdata, aes(x = size, y = datapick, color = as.factor(type2), label = pic_id)) +
  #geom_text(data = labdat, aes(x = x, y=y, label = label, color = as.factor(type2)),
  #          show.legend = F) + 
  geom_vline(xintercept = sqrt(4.893)) + 
  geom_hline(yintercept = 1/6, linetype='dotted') +
  scale_color_brewer(palette = "Dark2", name = "Lineup Type", labels = c("-", "+")) +  
  facet_grid(~param) + 
  labs(x = "Square root of parameter value", y = "Expected probability of detection\nin a lineup of size 6") + 
  ThemeNoNet + 
  theme(legend.position = 'bottom', strip.text = element_text(size = 10), text = element_text(size = rel(3)), legend.text = element_text(size = rel(3)), legend.key.size = unit(.25, "inches"))
@

<<beta5zoom, fig.height=3, fig.cap="The bottom middle panel of Figure~\\ref{fig:predictglmm} expanded to show greater detail. The parameter value is shown on the $x$-axis. This parameter most closely follows our hypothesis shown in Figure~\\ref{fig:hypothesis}. However, the result is not symmetric. According to the model, people will detect the effect at lower values and with greater frequency as the value increases when it is positive instead of negative.">>=
ggplot() + 
  geom_line(data = newdata %>% filter(test_param == "simttb", category == "inside"), aes(x = size, y = predictfinal, color = as.factor(type2)), size = 1.25) + 
  geom_point(data = plotdata %>% filter(test_param=="simttb"), aes(x = size, y = datapick, color = as.factor(type2)), size = 2) + 
  geom_line(data = newdata %>% filter(test_param == "simttb", category == "outside"), aes(x = size, y = predictfinal, group = as.factor(type2)), color = "gray60", alpha = .7) +
  geom_line(data = newdata %>% filter(test_param == "simttb", category == "outside2"), aes(x = size, y = predictfinal, group = as.factor(type2)), color = "gray60",  alpha = .7) +
  #geom_text(data = plotdata, aes(x = size, y = datapick, color = as.factor(type2), label = pic_id)) +
  #geom_text(data = labdat, aes(x = x, y=y, label = label, color = as.factor(type2)),
  #          show.legend = F) + 
  geom_vline(xintercept = 10.090829) + 
  geom_hline(yintercept = 1/6, linetype='dotted') +
  scale_color_brewer(palette = "Dark2", name = "Lineup Type", labels = c("-", "+")) +
  facet_grid(~param) + 
  labs(x = "Parameter value", y = "Expected probability of detection in a lineup of size 6") + 
  ThemeNoNet + 
  theme(legend.position = 'bottom', strip.text = element_text(size = 10), text = element_text(size = rel(3)), legend.text = element_text(size = rel(3)), legend.key.size = unit(.25, "inches"))
@

<<beta4zoom, fig.height=3, fig.cap='The bottom left panel of Figure~\\ref{fig:predictglmm} expanded to show greater detail. The parameter value is shown on the $x$-axis. The ``reverse" lineup has a much flatter slope than the ``regular" lineup, which means the participants had a harder time detecting a more simple M1 structure among many more complex M4 structures. Reversing the lineup scenario was not symmetric as we hypothesized.'>>=
ggplot() + 
  geom_line(data = newdata %>% filter(test_param == "jtts", category == "inside"), aes(x = size, y = predictfinal, color = as.factor(type2)), size = 1.25) + 
  geom_point(data = plotdata %>% filter(test_param=="jtts"), aes(x = size, y = datapick, color = as.factor(type2)), size = 2) + 
  geom_line(data = newdata %>% filter(test_param == "jtts", category == "outside"), aes(x = size, y = predictfinal, group = as.factor(type2)), color = "gray60", alpha = .7) +
  geom_line(data = newdata %>% filter(test_param == "jtts", category == "outside2"), aes(x = size, y = predictfinal, group = as.factor(type2)), color = "gray60", alpha = .7) +
  #geom_text(data = plotdata, aes(x = size, y = datapick, color = as.factor(type2), label = pic_id)) +
  #geom_text(data = labdat, aes(x = x, y=y, label = label, color = as.factor(type2)),
  #          show.legend = F) + 
  geom_vline(xintercept = 3.340302) + 
  geom_hline(yintercept = 1/6, linetype='dotted') +
  scale_color_brewer(palette = "Dark2", name = "Lineup Condition", labels = c("reverse", "regular")) +
  facet_grid(~param) + 
  labs(x = "Parameter value", y = "Expected probability of detection in a lineup of size 6") + 
  ThemeNoNet + 
  theme(legend.position = 'bottom', strip.text = element_text(size = 10), text = element_text(size = rel(3)), legend.text = element_text(size = rel(3)), legend.key.size = unit(.25, "inches"))
@

\st{We expand portions of Figure~\ref{fig:predictglmm} in Figures~\ref{fig:recipzoom}-\ref{fig:beta4zoom}. These figures show the same prediction regions as in Figure~\ref{fig:predictglmm}, plus some additional predictions outside of the data range shown in gray. Again, the points represent the results from the experiment. In all three of these figures, the lack of symmetry is apparent. In the reverse lineup scenario shown in Figure~\ref{fig:beta4zoom}, the probability of prediction is consistently far less than the probability of prediction in the regular lineup scenario. This demonstrates that the visual signal of one plot from M4 among five plots from M1 is much stronger than that of one plot from M1 among five plots from M4. We posit that the latter is a more difficult task because it involves noticing a \textit{lack of structure} as opposed to the presence of more structure. We can see a similar effect in Figure~\ref{fig:beta5zoom}. At a value of $\beta_5 = 20$, the model predicts a probability of about 0.60 that a new viewer of a new lineup will identify the alternative data plot. At a value of $\beta_5 = -20$, however, the model predicts this same probability to be about 0.40. This again demonstrates that the presence of structure is detected sooner and more frequently than the absence of strucutre. }


<<beta41, fig.height = 4, fig.cap='In our experiment, 52.8\\% of viewers of this plot selected the plot from the alternative model, M4. The ``reverse" of this lineup is given in Figure~\\ref{fig:beta4neg1}, where 41.4\\% of viewers selected the plot from the alternative model, M1. Here, the alternative plot is $\\sqrt{25} - 3$.'>>=
dat <- read_csv("data/jtts_pos_easy_3.csv")
# "answer" is 2
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size = .5) + 
        ThemeNet + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

<<beta4neg1,fig.height=4, fig.cap='In our experiment, 41.4\\% of viewers of this plot selected the plot from the alternative model, M1. The ``reverse" of this lineup is given in Figure~\\ref{fig:beta41}, where 52.8\\% of viewers selected the plot from the alternative model, M1. Here, the alternative plot is $\\sqrt{25} - 1$.' >>=
dat <- read_csv("data/jtts_neg_med_2.csv")
# 'answer' is 4
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size = .5) + 
        ThemeNet + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

For $\beta_4$ and $\beta_6$, where one plot simulated from M1 was placed among five plots from the corresponding model, we see that the predictions for the reverse lineup type (-1), are less than the standard lineup type (1) for all values of the parameter that we have. This contradicts our hypothesis for this scenario, which was that these two scenarios would perform similarly. One of the lineups for the $\beta_4 = 6.681$, lineup type 1 scenario is given in Figure~\ref{fig:beta41}, and a corresponding lineup for the lineup type -1 scenario is given in Figure~\ref{fig:beta4neg1}. For identical values of the parameter, viewers had a harder time identifying the different plot when they were selected the most ``simple" structure, detecting M1 in five plots from the more complicated model, than they did identifying the most ``complex" structure, the plot from the more complicated model, from the five plots from M1. \st{This result is also similar to that of \citet{corrviz} because it emphasizes the difficulty of picking out the absence of an effect relative to picking out the presence of an effect.}

\section{Discussion}\label{sec:concl}

% what did we do? what did we learn from it? 
By using visual inference methods, we have developed new ways to perform significance and goodness-of-fit testing for a complicated and intractable set of statistical models for social network data. \st{We have also developed a way to determine the power of these new visual tests.} Our methods can be used to supplement traditional methods and check our assumptions about network models. The traditional methods only look at one piece or derived measure of a network model, whereas our methods look at the models holistically for a broader sense of what it means for a parameter to be significant or a model to be a good fit. By looking at an entire network simulated from a SAOM side-by-side with other instances of networks simulated from another model, instead of singular %one-dimensional derived 
features, we develop an idea of the model in terms of the \textit{data} itself, instead of in terms of statistical summaries of the data. \st{These methods place the model in the data space, instead of summarizing or compressing the data to place it in the model space.}

Furthermore, we have found the visual power of some effects in the object function of a SAOM for this particular senate data example,  and we have shown that, for the same effects, there is a lot of variability in results from significance and goodness of fit tests. Because the visual tests we performed show a great deal of variability, we can see that the decisions with respect to the significance of a parameter or the goodness of fit of a model to data are not as cut-and-dried as the \st{more} traditional methods would have us believe. 

% what are the limitations of these findings 

These results do not come without limitations. In visual inference, the null plots are supposed to play the role of good representatives of the null model. Here, the number of null plots is reduced to five, which increases the variability seen in a single lineup dramatically, and can unfortunately lead to very different conclusions for the same lineup scenario. Furthermore, these results do not generalize to all SAOMs or to some subset of SAOMs. The lineups shown are made for only one set of data, and it is not clear whether the power results transfer, nor is it clear to what degree if they do transfer, to other situations with different number of actors, different edge densities, or different layout algorithm of the node-link diagram. \st{We can make some generalizations about what participants are picking up on in the lineups based on their feedback and previous research, but we cannot apply our hierarchical model directly to lineups constructed for new data or new models or parameters.}  

% what is the future of this research? 

We hope to apply these methods further for different types of network data and different types of network models. We accept the limitations of this type of network data visualization, in that even in small instances, the cognitive load of looking at a lineup is very high for the average observer. We would therefore like to explore larger datasets, different layout algorithms, and different ways of visualizing network data, such adjacency matrix visualizations, using visual inference to see if similar patterns emerge. 

%\hh{Best results: visual detection/ power analysis, a lot of variability in goodness of fit and significance tests. BUT: visualizations show that a decision of significance/ goodness of fit are not as clear cut as the traditional tests want us to believe.

%Limitations: 
%\begin{itemize}
%\item number of null plots: null plots are representatives of the null %distribution of all possible node-link diagrams of data sampled from the %null model. Here, the number of null plots is  reduced to five, which %increases the variability seen from a single lineup dramatically and %unfortunately leads to different conclusions.
%\item ability to generalize: the lineups are made for only one test case %- it is not clear, whether and how far, power results transfer to other %situations with different number of actors and different edge density.
%\end{itemize}}

% Talk about visual inference
%We propose to attack some of the aforementioned difficulties with SAOMs by using a technique known as \textit{visual inference}. This technique was created by \citet{Bujaetal} to provide well-defined, statistical rigor to the usual exploratory data analyses and model diagnostics that are typically performed by visualizing the data as opposed to looking at it raw or gathering numerical summaries of it. In visual inference there are \textit{null plots} and \textit{data plots}: the null plots are visualizations of data simulated from the model according to the null hypothesis, while the data plots are visualizations of data simulated from the model according to an alternative hypothesis. These two data sources are visualized side-by-side using small multiples using what Buja et al called the \textit{lineup protocol}. The idea behind the lineup protocol is the police lineup, where witnesses to crimes are brought to the police station to observe a group of people, one of whom is suspected to have committed the crime, and are asked to identify the perpetrator of the crime. If the witness identifies the suspect in the lineup, that is taken as evidence the suspect is guilty, whereas if the witness does not identify the suspect, that is taken as evidence the suspect is not guilty. In the visual inference lineup protocol, the \textit{suspect} is the data plot among $M-1$ null plots. If the "witness" can pick the data plot out of the lineup, that is taken as evidence that the null model should be rejected, whereas failing to pick out the data plot is taken as evidence that the null model should not be rejected. In our application of the lineup protocol to SAOMs, we include various effects of varying sizes in the SAOMs we fit to some data and try to \textit{see} those effects in node-link diagram corresponding to simulations from the various models.

%-Talk about political networks-

%-Talk about goodness-of-fit-

%-Talk about significance testings- 

