\documentclass[12pt]{article}
% \usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
% \geometry{letterpaper}                   % ... or a4paper or a5paper or ...
%\usepackage{graphicx}
\usepackage[font=small,skip=5pt]{caption}
\usepackage{subcaption}
%\usepackage{animate}
\usepackage{afterpage}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amsfonts}
%\usepackage{color}
\usepackage{multirow}
\usepackage{rotating}
\usepackage[dvipsnames,svgnames,table]{xcolor}
\usepackage{xr}
\usepackage{xr-hyper}
\usepackage{hyperref}
\usepackage{numprint}
\usepackage{booktabs}
\usepackage{longtable}

%\usepackage{appendix}

\graphicspath{{sm-figure/}}
% \usepackage{endfloat} % Figures to the end of the document
\newcommand{\st}[1]{{\color{orange} #1}}
\newcommand{\hh}[1]{{\color{magenta} #1}}


\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}
%---------------------------------------------------
%                 Editing Commands
%\newcommand{\hh}[1]{{\color{magenta} #1}}
%\newcommand{\st}[1]{{\color{orange} #1}}

%---------------------------------------------------
%                 Placing Figures
\renewcommand{\topfraction}{0.99}	% max fraction of floats at top
\renewcommand{\bottomfraction}{0.8}	% max fraction of floats at bottom
\renewcommand{\textfraction}{0.05}	% allow minimal text w. figs

%---------------------------------------------------
% Define new environment
\newtheorem{theorem}{Theorem}[section]
\newtheorem{algorithm}[theorem]{Algorithm}
%---------------------------------------------------

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{1.3in}%
\addtolength{\topmargin}{-.8in}%

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}

\externaldocument{paper}

\begin{document}

\title{\bf Supplementary Materials: Visual Inference for a Social Network Model}
  \author{Samantha Tyner\thanks{
    Department of Statistics and Statistical Laboratory, Iowa State University\\
    and \\
    Heike Hofmann\\
    Department of Statistics and Statistical Laboratory, Iowa State University}
  \maketitle
  }
  
   \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Supplementary Materials: Visual Inference for a Social Network Model}
\end{center}
 
\spacingset{1.45} % DON'T change the spacing!
<<setup-ch-2, echo = FALSE, message = FALSE, warning = FALSE>>=
options(replace.assign=TRUE,width=70, digits=3)
require(knitr)
opts_chunk$set(fig.path='sm-figure/', cache.path='sm-cache/', fig.align='center', fig.pos='h', out.width='.99\\textwidth', par=TRUE, cache=FALSE, concordance=TRUE, autodep=TRUE, message=F, warning=F, echo = FALSE, dev="cairo_pdf", fig.width = 6, fig.height = 6)#, root.dir = "~/Desktop/Dissertation/SAOM-removing-blindfold/")
@

<<pkgs-sm>>=
library(tidyverse)
library(RSiena)
library(geomnet)
library(vinference)
turk22 <- read_csv("data/turk22-sig.csv")
newdata <- read_csv("data/newdata_pred_glmm.csv")
newdata$param <- newdata$test_param
newdata$param <- as.factor(newdata$test_param)
levels(newdata$param) <-  c(1,3,4,2,6,5) 
newdata$param <- as.integer(as.character(newdata$param))
plotdata <- turk22 %>% mutate(type2 = ifelse(sign == 0, sign(initialEst), sign),
                       type2 = ifelse(type == "one", type2, -1)) %>% 
  group_by(pic_id, size, test_param, sign, type2,alt_model) %>% summarize(
    datapick = mean(datapick)
)
plotdata$param <- plotdata$test_param
plotdata$param <- as.factor(plotdata$param)
levels(plotdata$param) <- c(1,3,4,2,6,5) 
plotdata$param <- as.integer(as.character(plotdata$param))
labdat <- data_frame(test_param = "jttp", param = 3, type2 = -1, x = -5, y = .9, label = "outlier")
betahat <- tibble(param = 1:6, size = c(-4.903, 4.893, -3.45, 3.34, 10.091, 1.329))
ThemeNoNet <- theme_bw() %+replace% 
            theme(plot.title = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0, 
                                            family="Times New Roman"), 
                  axis.title.x = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0,
                                            family="Times New Roman"),
                  axis.title.y = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 90,
                                            family="Times New Roman"),
                  axis.text.x = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0,
                                            family="Times New Roman"), 
                  # axis.text.x.top = element_text(size = 12,
                  #                           face = 'plain', 
                  #                           angle = 0,
                  #                           family="Times New Roman"), 
                  # axis.text.x.bottom = element_text(size = 12,
                  #                           face = 'plain', 
                  #                           angle = 0,
                  #                           family="Times New Roman"), 
                  axis.text.y = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0,
                                            family="Times New Roman"),
                  # axis.text.y.left = element_text(size = 12,
                  #                           face = 'plain', 
                  #                           angle = 0,
                  #                           family="Times New Roman"),
                  # axis.text.y.right = element_text(size = 12,
                  #                           face = 'plain', 
                  #                           angle = 0,
                  #                           family="Times New Roman"),
                  strip.text.x = element_text(size = 12,
                                            face = 'plain', 
                                            angle = 0, 
                                            family="Times New Roman",
                                            margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.text.y =element_text(size = 12,
                                            face = 'plain', 
                                            angle = 90,
                                            family="Times New Roman",
                                            margin = margin(t = 3, r = 3, b = 3, l = 3, unit = "pt")),
                  strip.background = element_rect(colour = "black", fill = "white")
                    )
ThemeNet <- theme_net() %+replace% 
            theme(plot.title = element_text(size = 12,
                                            face = 'plain', angle = 0, family = "Times New Roman"), 
                  strip.text.x = element_text(size = 12,
                                            face = 'plain', angle = 0, family = "Times New Roman", 
                                            margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  strip.text.y =element_text(size = 12,
                                            face = 'plain', angle = 0, family = "Times New Roman",margin = margin(t = 3, r = 0, b = 3, l = 0, unit = "pt")),
                  panel.border = element_rect(fill = NA, color = 'black'),
                  strip.background = element_rect(colour = "black", fill = "white"))
@


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% intro: talk about the sections of the appendix
Blah 

\appendix % to get sections with letters instead of numbers
\section{Model Details}\label{sec:sm-model}

The continuous-time Markov chain (CTMC) family of models we use were first introduced in \citet{saompaper}. We describe their basic structure here, and full detail can be found in \citet{saompaper, snijders:2010}. 

\noindent \textbf{Rate Function:}  The rate function dictates when network changes are made and which actor can make them. An actor, $i$, is chosen make a change in its ties one of the other nodes $j$. In general, the rate function can include structural and node covariate parameters into account so that each actor has a different rate of change. However, we choose a simple rate function that is constant over all nodes in a given time period, because we focus on interpreting the parameters of the objective function which directly impact the overall network structure. We denote the rate from $t_{m}$ to $t_{m+1}$ as $\alpha_m$ for $m = 1, \dots, M-1$, where $M$ is the number of time points at which the network was observed. Using this notation, the \textit{waiting time} to the next chance for actor $i$ to make a change is exponentially distributed with expected value $\alpha_m^{-1}$. Since the rate is the same for all actors, the waiting time for \textit{any} actor to get the opportunity to change its set of ties is also exponentially distributed with expected value $(n\alpha_m)^{-1}$.

\noindent \textbf{Objective Function:} After actor $i$ is selected to make a change, it randomly picks one of its current ties, $x_{ij}$, to change. %The probability that actor $i$ changes its current tie to actor $j$ is determined by the \textit{objective function} of the model and a random error term $U$ with log-Weibull distribution \citep{modelsSnijders}.  %\hh{XXX can you paraphrase the purpose of U in half a sentence?}
%The random component is assumed to have log-Weibull distribution with location parameter $\mu = 0$ and scale parameter $\sigma = 0$ (see \citet{modelsSnijders}), so the probability density function of $U$ is
%\begin{equation}\label{eq:logweibull}
%g(u) = \exp\{-(u + \exp\{-u\})\}.
%\end{equation}
%The objective function, $f_i$, is the driving force of a SAOM. 
Actor $i$ aims to maximize the objective function $f_i$ given the current state of the network, $x$ and the node-level covariates, $\mathbf{Z}$. This function is defined as:% which has the form
\begin{equation}\label{eq:objective}
f_i(x, \boldsymbol{\beta}, \mathbf{Z}) = \sum_{k = 1}^K \beta_k s_{ik}(x, \mathbf{Z}),
\end{equation}
where $\boldsymbol{\beta} = (\beta_1, \dots, \beta_K)$ are additional model parameters, each associated with some statistics, $s_{i1}(x, \mathbf{Z}), \dots, s_{iK}(x, \mathbf{Z})$, calculated for actor $i$ at the current network state $x$. %The statistics can be very simple, like outdegree of the node, $s_{i1}(x) = \sum_{i\neq j} x_{ij}$, or more complicated, like the {\it transitive triplets jumping to different covariate} statistic, $s_i(x, \mathbf{Z}) = \sum_{i \neq j \neq h} x_{ij}x_{ih}x_{hj} \cdot \mathbb{I}(z_i = z_h \neq z_j)$. 
At least two parameters must be included in the objective function: density and reciprocity \citep{RSienamanual}. We denote the density, or out-degree, parameter by $\beta_1$ and the associated statistic as $s_{i1}(x)=\sum_{j} x_{ij}$ and we denote the reciprocity parameter by $\beta_2$ and the associated statistic as $s_{i2}(x) = \sum_{j} x_{ij}x_{ji}$. We will refer to the very simple model with only these two parameters in the objective function as model M1. We define additional parameters and models of interest in Section~\ref{sec:models}  %, plus many more. At last count, in the software we use to fit these models to network data, 
Version 1.2-3 of \texttt{RSiena} \citep{RSiena}, the software we use to fit CTMC models to data, provides over 80 possible effects that can be included in the objective function. %We discuss these and other statistics in more detail in Section~\ref{sec:models}. 

The objective function $f_{i}(x, \boldsymbol{\beta}, \mathbf{Z})$ dictates the \textit{transition probability}, $p_{ij}$ of the network changing from its current state $x$ to the state $x(i\leadsto j)$, which is identical to $x$ except for $x_{ij}$: $x_{ij}(i \leadsto j) = 1 - x_{ij}$. The transition probability is
\begin{equation}\label{eq:transprob}
p_{ij} = \dfrac{\exp\{f_i(x(i \leadsto j), \boldsymbol{\beta}, \mathbf{Z})\}}{\sum_{h} \exp\{f_i(x(i \leadsto h), \boldsymbol{\beta}, \mathbf{Z})\}},
\end{equation}
dictating which edge node $i$ changes. Thus, the actor is more likely to make changes that increase the value of their objective function, and no change is most likely when any change decreases the value of the objective function.

\subsection{Model Effects}

The similarity measure is computed as: 
\begin{equation}\label{eq:similar}
sim^b_{ij} = \frac{\max_{hk}|b_h - b_k| - |b_i - b_j|}{\max_{hk}|b_h - b_k|} 
\end{equation}
where $\max_{hk}|b_h - b_k|$ is the range of number of bills authored by senators, and $b_i$, $b_j$ are the number of bills authored by senators $i,j$ repectively in the senate period. 

The fitted values from repeated converged simulations are 

<<modelestimates>>=
load("data/allModelMeans.RDS")
modelMeanEsts %>% unnest(ests) %>% mutate(param = "rate") -> modelmeans
modelmeans$param[c(1,6,12, 18,24, 30)] <- "alpha1"
modelmeans$param[c(1,6,12, 18,24, 30)+1] <- "alpha2"
modelmeans$param[c(1,6,12, 18,24, 30)+2] <- "alpha3"
modelmeans$param[c(1,6,12, 18,24, 30)+3] <- "beta1"
modelmeans$param[c(1,6,12, 18,24, 30)+4] <- "beta2"
modelmeans$param[modelmeans$param == "rate"] <- c("beta3", 'beta4', 'samep', 'beta6', 'beta5')
modelmeans %>% filter(model != "samep") %>% spread(param, ests) -> modelmeans
M6 <- c(2.4405048,2.4594403,2.2098176,-4.9232775,4.8916183,2.3743720,0.2047038,6.9661589)
@


\begin{table}
\centering
\begin{tabular}{|lccccccccc|}
\toprule
Model & $\hat{\alpha}_1$ & $\hat{\alpha}_2$ & $\hat{\alpha}_3$ & $\hat{\beta}_1$ & $\hat{\beta}_2$ & $\hat{\beta}_3$ & $\hat{\beta}_4$ & $\hat{\beta}_5$ & $\hat{\beta}_6$ \\
\midrule
M1 & \Sexpr{round(modelmeans$alpha1[1], 3)} & \Sexpr{round(modelmeans$alpha2[1], 3)} & \Sexpr{round(modelmeans$alpha3[1], 3)} & \Sexpr{round(modelmeans$beta1[1], 3)} & \Sexpr{round(modelmeans$beta2[1], 3)} & -- & -- & -- & --\\
M3 & \Sexpr{round(modelmeans$alpha1[2], 3)} & \Sexpr{round(modelmeans$alpha2[2], 3)} & \Sexpr{round(modelmeans$alpha3[2], 3)} & \Sexpr{round(modelmeans$beta1[2], 3)} & \Sexpr{round(modelmeans$beta2[2], 3)} & \Sexpr{round(modelmeans$beta3[2], 3)} & -- & -- & --\\ 
M4 & \Sexpr{round(modelmeans$alpha1[3], 3)} & \Sexpr{round(modelmeans$alpha2[3], 3)} & \Sexpr{round(modelmeans$alpha3[3], 3)} & \Sexpr{round(modelmeans$beta1[3], 3)} & \Sexpr{round(modelmeans$beta2[3], 3)} & -- & \Sexpr{round(modelmeans$beta4[3], 3)} & -- & --\\
M5 & \Sexpr{round(modelmeans$alpha1[5], 3)} & \Sexpr{round(modelmeans$alpha2[5], 3)} & \Sexpr{round(modelmeans$alpha3[5], 3)} & \Sexpr{round(modelmeans$beta1[5], 3)}& \Sexpr{round(modelmeans$beta2[5], 3)} & -- & -- & \Sexpr{round(modelmeans$beta5[5], 3)} & -- \\
M6 & \Sexpr{round(modelmeans$alpha1[4], 3)} & \Sexpr{round(modelmeans$alpha2[4], 3)} & \Sexpr{round(modelmeans$alpha3[4], 3)} & \Sexpr{round(modelmeans$beta1[4], 3)} & \Sexpr{round(modelmeans$beta2[4], 3)} & -- & -- & -- & \Sexpr{round(modelmeans$beta6[4], 3)}\\
M7 & \Sexpr{round(M6[1], 3)} & \Sexpr{round(M6[2], 3)} & \Sexpr{round(M6[3], 3)} & \Sexpr{round(M6[4], 3)} & \Sexpr{round(M6[5], 3)} & -- & \Sexpr{round(M6[6], 3)} & \Sexpr{round(M6[8], 3)} & \Sexpr{round(M6[7], 3)} \\
\bottomrule
\end{tabular}
\caption{\label{tab:fittedvalues} The mean estimates from repeated fits of our models of interest. When simulating from these models, these are the estimates that we will use unless otherwise stated.}
\end{table}

\subsection{Goodness-of-Fit Testing}

The software \texttt{RSiena} contains methods for performing goodness-of-fit tests for the CTMC models. The \texttt{sienaGOF()} function performs goodness-of-fit testing as follows:
\begin{enumerate}
\item Auxiliary statistics, such as the cumulative outdegree distribution on the nodes, are computed on the observed data ($\mathbf{u}_d$) and on $N$ observations simulated from the model ($\mathbf{u}_1 \dots \mathbf{u}_N$). 
\item The mean $\overline{\mathbf{u}}$ and covariance matrix $\mathbf{S}$ are computed from the $N$ simulations, and the Mahalanobis distance, $d_M(\mathbf{u}_d)$ from the observed statistics to the distribution of the simulated statistics is computed:
\begin{equation}\label{eq:mahal}
d_M(\mathbf{u}_d) = \sqrt{(\mathbf{u}_d - \overline{\mathbf{u}})' \mathbf{S}^{-1} (\mathbf{u}_d - \overline{\mathbf{u}})}
\end{equation}
\item The Mahalanobis distance for each of the $N$ simulations is calculated and $d_M(\mathbf{u}_d)$ is compared to this distribution of distances.
\item An empirical $p$-value is found by computing the proportion of simulated distances found in step 4 that are as large or larger than $d_M(\mathbf{u}_d)$.% A model is thus considered a good fit to the data if $p$ is large.
\end{enumerate}
<<gofsiena, fig.height=2.5, fig.cap="An example of what a goodness-of-fit plot from \\texttt{RSiena} looks like. The overlaid boxplots and violin plots show the distribution of each of the outdegree count values on the simulated networks, and the red points and lines are the observed data values.", echo=FALSE>>=
library(RSiena)
library(tidyverse)
load("data/ansnullpaper.rda")
library(lattice)
#gof1 <- sienaGOF(ansnull, OutdegreeDistribution, varName ="friendship", join = FALSE)
gof1 <- readr::read_rds("data/gof-data.rds")
sims1 <- data.frame(gof1$`Period 1`$Simulations)
dat1 <- gof1$`Period 1`$Observations
dat1 %>% data.frame() %>% gather(outdegree, val) %>%
  mutate(outdegree = parse_number(outdegree)-1) -> dat1
p1 <- sims1 %>%
  mutate(sim = row_number()) %>%
  gather(outdegree, val, X1:X9) %>%
  mutate(outdegree = parse_number(outdegree) - 1) %>%
  filter(outdegree <= 6) %>%
  ggplot(aes(x = outdegree, y = val)) + 
  geom_boxplot(aes(group = outdegree), size = .5) + 
  geom_violin(aes(group = outdegree), bw = 2, fill = NA) + 
  geom_point(data = dat1, color = 'red', alpha = .5) + 
  geom_line(data= dat1, color = 'red', alpha=.5) + 
  geom_text(data = dat1, aes(label = val), hjust = -.5) + 
  labs(x = "Outdegree (p = 0.154)", y = "Statistic", 
       title = "Goodness-of-Fit: Outdegree distribution period 1") 
p1
@

\section{Data}\label{sec:sm-data}

 Details of how this data can be downloaded are provided by FranÃ§ois Briatte at \url{github.com/briatte/congress}. In the US Senate, senators often show support for a piece of legislation by co-sponsoring a bill authored by one of their colleagues. In a co-sponsorship network, ties are directed from senator $i$ to senator $j$ when senator $i$ signs on as a co-sponsor to the bill that senator $j$ authored. There are many hundreds of ties between senators when they are connected in this way, so we simplify the network by computing a single value for each senator-senator collaboration called the \textit{weighted propensity to co-sponsor} (WPC). This value is defined in \citet{senate} as 

\begin{equation}\label{eq:sen1}
    WPC_{ij} = \sum\limits_{b=1}^{B_j} \frac{Y_{ij(b)}}{c_{j(b)}}\left(\sum\limits_{b=1}^{B_j} \frac{1}{c_{j(b)}}\right)^{-1}
\end{equation}
where $B_j$ is the number of bills in a congressional session authored by senator $j$, $c_{j(b)}$ is the number of co-sponsors on senator $j$'s $b^{th}$ bill, where $b \in \{1,\dots, B_j\}$, and $Y_{ij(b)}$ is an indicator variable that senator $i$ co-sponsored senator $j$'s $b^{th}$ bill. This measure ranges in value from 0 to 1, where $WPC_{ij} = 1$ if senator $i$ is a co-sponsor on every one of senator $j$'s bills and $WPC_{ij} = 0$ if senator $i$ is never a co-sponsor any of senator $j$'s bills. To simplify the problem, we construct a network with binary edges as follows: 
 \begin{equation}\label{eq:edgewpc}
  x_{ij} =
\begin{cases}
                                   1 & WPC_{ij} > 0.25 \\
                                   0 & WPC_{ij} \leq 0.25
\end{cases}
\end{equation}
so that only strong ties between senators are in the network.

\section{Lineups}

<<lineup3132, fig.height=4, fig.cap="Lineup 3132, which led to rejection of the null hypothesis that $\\beta_3$ = 0. The network simulated from model M3 is found in panel $\\sqrt{16} - 1$, and the remaining panels show networks simulated from model M1.">>=
dat <- read_csv("data/jttp_pos_hard_2.csv")
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size =1) + 
        ThemeNet + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

<<lineup3133, fig.height=4, fig.cap="Lineup 3133, which resulted in failure to reject the null hypothesis that $\\beta_3$ = 0. The network simulated from model M3 is found in panel $\\sqrt{25} - 4$, and the remaining panels show networks simulated from model M1. 16 of 27 viewers picked plot two as most different.">>=
dat <- read_csv("data/jttp_pos_hard_3.csv")
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size =1) + 
        ThemeNet + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

<<failgof, fig.height=4, fig.cap="The goodness-of-fit lineup which resulted in failure to reject the null hypothesis. The null model for this lineup is M4. Only 7 of 20 viewers of this lineup selected the data plot as the most different from the others. The most commonly chosen panel was number four, which has a relatively simple structure compared to the other panels.">>=
load("data/se112adjmat.RDS")
wave2 <- data.frame(se112adj)
senators <- data.frame(id = colnames(wave2), number = 1:155)
wave2$from <- colnames(wave2)
wave2 <- wave2 %>% gather(to, val, Alan.Stuart.Franken:William.Cowan) %>% filter(val > 0)
wave2 <- merge(wave2, senators, by.x = "from", by.y = "id", all = T)
wave2$from <- senators$number[match(wave2$from, senators$id)]
wave2$to <- senators$number[match(wave2$to, senators$id)]
wave2$from <- paste0("V", wave2$from)
wave2$to <- ifelse(is.na(wave2$to), NA, paste0("V", wave2$to))
wave2 <- wave2 %>% mutate(sim = 1001, model = "data", wave = 1) %>% select(-c(val, number))
dat <- read_csv(paste0("data/jtts_gof_9_2.csv"))
datplot <- unique(dat$ord[dat$sim == 1001])
dat <- dat %>% filter(sim != 1001) %>% 
      bind_rows(wave2) %>% 
      mutate(ord = ifelse(is.na(ord), datplot, ord))
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size = .5) + 
        ThemeNet + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

<<mostsiggof, fig.height = 4, fig.cap="The lineup resulting in the smallest $p$-value rejecting the null hypothesis. Surprisingly, this another repetition for M5 as the null model. The data are in panel 6.">>=
dat <- read_csv(paste0("data/jtts_gof_9_3.csv"))
datplot <- unique(dat$ord[dat$sim == 1001])
dat <- dat %>% filter(sim != 1001) %>% 
      bind_rows(wave2) %>% 
      mutate(ord = ifelse(is.na(ord), datplot, ord))
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size = .5) + 
        ThemeNet + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

<<gofm71, fig.height=4, fig.cap="One repitition of a goodness-of-fit lineup testing model M7. The senate data are shown in panel two, and it is evident that none of the other five panels, which show data simulated from model M7, come close to creating the large connected component that is central to the structure of the senate data.">>=
dat <- read_csv(paste0("data/bigmod_gof_9_1.csv"))
datplot <- unique(dat$ord[dat$sim == 1001])
dat <- dat %>% filter(sim != 1001) %>% 
      bind_rows(wave2) %>% 
      mutate(ord = ifelse(is.na(ord), datplot, ord))
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size =1) + 
        ThemeNet + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

<<beta41, fig.height = 4, fig.cap='In our experiment, 52.8\\% of viewers of this plot selected the plot from the alternative model, M4. The ``reverse" of this lineup is given in Figure~\\ref{fig:beta4neg1}, where 41.4\\% of viewers selected the plot from the alternative model, M1. Here, the alternative plot is $\\sqrt{25} - 3$.'>>=
dat <- read_csv("data/jtts_pos_easy_3.csv")
# "answer" is 2
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size = .5) + 
        ThemeNet + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

<<beta4neg1,fig.height=4, fig.cap='In our experiment, 41.4\\% people viewing this lineup selected the plot from the alternative model, M1. Here, the alternative plot is $\\sqrt{25} - 1$. The other five plots were simulated from model M4.' >>=
dat <- read_csv("data/jtts_neg_med_2.csv")
# 'answer' is 4
ggplot(data = dat) + 
        geom_net(aes(from_id = from, to_id = to), 
                 arrow = arrow(type = 'open', length = unit(2, "points") ), 
                 linewidth = .25, singletons = T, fiteach = T, directed = T, 
                 color = 'black', arrowgap = .015, arrowsize = .3, size = .5) + 
        ThemeNet + 
        theme(panel.background = element_rect(color = 'black')) +
        facet_wrap(~ord)
@

\section{Results}

\subsection{Lineup Summaries}

<<res-table-lus, results = "asis">>=
# wrap_pvsim <- function(x, k, m = 6){
#   res <- vinference::pVsim(x = x, K = k, m = m, scenario = 3, target = 1)
#   res[2]
# }

# turk22 <- read_csv("data/turk22.csv")
# res_sim_pvals <- turk22 %>% group_by(pic_id) %>% summarise(n_datapick = sum(datapick), n_views = n()) %>% 
#   separate(pic_id, into = c("difficulty", "lu_type", "param", "rep"), sep = 1:3) %>% 
#   group_by(difficulty, lu_type, param) %>% 
#   mutate(n_datapick_c = sum(n_datapick), n_views_c = sum(n_views)) %>% 
#   mutate(single_pv =  map2_dbl(n_datapick, n_views, wrap_pvsim))
# res_sim_pvals2 <- res_sim_pvals %>% 
#   mutate(combined_pv =  map2_dbl(n_datapick_c, n_views_c, wrap_pvsim))

sig_marks <- function(p){
  if(p < .1 & p >= .05){
    return("^{*}$")
  }
   if(p < .05 & p >= .01){
    return("^{**}$")
   }
   if(p < .01 & p >= .001){
    return("^{\\dagger}$")
   }
   if(p < .001){
    return("^{\\ddagger}$")
   } else{
    return("$")
  }
}

lineup_pvals <- read_csv("data/vinf_pvals_res.csv")
pvals_table <- lineup_pvals %>% 
  mutate(sig_ind = map_chr(single_pv, sig_marks)) %>%
  mutate(res = str_c("$",n_datapick,"/", n_views, sig_ind, collapse = NULL)) %>% 
  group_by(difficulty, lu_type, param) %>% 
  mutate(combined_pv = round(mean(combined_pv), 3)) %>%
  select(difficulty:rep, combined_pv, res) %>% 
  spread(rep, res) %>% select(1:3, 5:7, 4) %>% 
  mutate(sig_ind_c = map_chr(combined_pv, sig_marks))
pvals_table$combined_pv[which(pvals_table$combined_pv < 0.0001)] <- "$< 10^{-4}$"
pvals_table$combined_pv <- paste0(pvals_table$combined_pv, "$", pvals_table$sig_ind_c)
pvals_table$combined_pv[which(pvals_table$sig_ind_c == "$")] <- str_remove_all(pvals_table$combined_pv[which(pvals_table$sig_ind_c == "$")], "\\$")

pvals_table %>% select(1:7) %>% 
  arrange(param, lu_type, difficulty) %>% ungroup %>%  
  mutate(lu_type = recode(lu_type, `1` = "1", `2` = "-1", `9` = "GoF"), 
         difficulty = recode(difficulty, `1` = "Easy", `2` = "Med.", `3` = "Hard", `9` = "GoF"),
         param = recode(param, `1`= "$\\beta_1$", `2`="$\\beta_2$", `3`="$\\beta_3$", `4`="$\\beta_4$", `5`="$\\beta_5$", `6`="$\\beta_6$", `7` = "M7")) -> pretty_table 

lus <- tibble(filename = list.files("img/lineup-pngs/"))

lus %>% mutate(filename2 = filename) %>% separate(filename2, into = c("pic", "id", "altplot", "png")) %>% 
  select(filename, id, altplot) %>% separate(altplot, into = c("plot", "alt_id"), sep = 4) %>% 
  select(filename, id, alt_id) %>% 
  separate(id, into = c("difficulty", "lu_type", "param", "rep"), sep = 1:3) %>% 
  arrange(param, lu_type, difficulty) -> lus

pretty_table <- mutate(pretty_table, pic_file = str_remove(lus$filename, ".png"))

insert_image <- pretty_table %>% glue::glue_data("\\includegraphics[width=3cm]{{img/lineup-pngs/{pic_file}}}")

pretty_table %>% mutate(pic_insert = insert_image) %>% select(9, 1:7) %>%
  knitr::kable(format = "latex", escape = F, col.names = c("Lineup", "Difficulty", "Type", "Param.", "Rep 1", "Rep 2", "Rep 3", "Global p-val."), longtable=T, caption = "Here is a caption. Put stuff here later", booktabs=T)


#pic_files <- turk22 %>% select(pic_id, pic_name) %>% distinct()

#magick::image_read_svg(path = paste0("/Users/sctyner/Desktop/Dissertation/lineups/experiments/turk22/plots/", pic_files$pic_name[1]))

@


<<res-table-lus-reas, results = "asis", eval = FALSE>>=
# ok, Heike was right. delving into the reasons is NOT worth the effort. 
turk22 <- read_csv("data/turk22.csv")
turk22 %>% group_by(pic_id, choice_reason) %>% 
  mutate(choice_reason2 = str_detect(choice_reason, "Other")) %>% 
  mutate(choice_reason3 = ifelse(choice_reason2, "Other", choice_reason)) %>% 
  group_by(pic_id, choice_reason3) %>% count()
mydates <- turk22$start_time
class(mydates) <- c('POSIXt','POSIXct')
turk22 %>%  mutate(time2 = mydates) %>% #why are there two???????
  filter(choice_reason == "Most complex overall structure" | choice_reason == "Most complicated overall structure" ) %>%
  ggplot() + 
  geom_histogram(aes(x = as.Date(time2), fill = choice_reason))

turk22 %>% group_by(pic_id, choice_reason) %>% 
  mutate(choice_reason2 = str_detect(choice_reason, "Other")) %>% 
  mutate(choice_reason3 = ifelse(choice_reason2, "Other", choice_reason)) %>% 
  ggplot(aes(x = start_time, y = end_time, color = choice_reason3)) + 
  geom_point() + 
  geom_abline() + 
  facet_wrap(~choice_reason3)
  
@

\subsection{Significance Testing}

<<sigtesting, results='asis'>>=
turk22_sig <- turk22 %>% 
  mutate(type2 = ifelse(sign == 0, sign(initialEst), sign),
         type2 = ifelse(type == "one", type2, -1)) %>% 
  filter(param_value == 1)
summ_sig <- turk22_sig %>% group_by(pic_id, test_param) %>% 
  summarize(npick = sum(datapick), n = n(), 
            pv = map2_dbl(npick, n, pV, m=6, scenario=3))
names(summ_sig) <- c("Lineup ID", "parameter", "\\# ID", "Total Views", "p-value")
summ_sig$parameter <- as.factor(summ_sig$parameter)
levels(summ_sig$parameter) <- c("$ \\beta_3 $", "$ \\beta_4$ ")
xtable::print.xtable(xtable::xtable(summ_sig, caption = "Experiment results for the two parameters for which we performed significance tests. \\# ID indicates the number of participants who identified the alternative data plot. There were three lineups for each parameter, so there are three results for each plot.", label = "tab:sigtesting", digits = c(0,0,0,0,0,5)), include.rownames = FALSE, sanitize.text.function=function(x){x})
@

\subsection{GoF}

<<goftab, cache = FALSE, results='asis'>>=
turk22_gof <- read_csv("data/turk22-gof.csv")
turk22_gof_stats <- turk22_gof %>% group_by(pic_id) %>% 
  summarize(npickdata = sum(datapick), total = n(), 
          pvinf = map2_dbl(npickdata, total, pV, m = 6, scenario = 3))
turk22_gof_stats %>% separate(pic_id, into = c("discard", "model", "rep"), sep = c(2,3)) -> print_gof_stats
print_gof_stats %>% select(-discard) %>% mutate(model = paste0("M", (as.integer(model))))-> print_gof_stats
names(print_gof_stats) <- c("Model", "Replicate", "Data Picks", "Total Viewers", "p-value")
print_gof_stats$Model[c(2:3,5:6,8:9,11:12)] <- ""
#print_gof_stats$Model[2] <- "jtt party"
#print_gof_stats$Model[5] <- "jtt sex"
#print_gof_stats$Model[8] <- "stt party"
print_gof_stats$`p-value` <- ifelse(print_gof_stats$`p-value` < .0001, "< 0.0001", sprintf("%.4f", print_gof_stats$`p-value`))
print(xtable::xtable(print_gof_stats, label = "tab:gofstats", caption = "An overview of the results from the 12 goodness-of-fit lineup tests.", digits = 4,
                      align="rlrrrr"), caption.placement = "bottom", include.rownames = FALSE, hline.after = c(-1,0,0,3,6,9,nrow(print_gof_stats)))
@

\subsection{Visual Power}\label{sec:sm-res-vp}

<<glmmres>>=
newdata <- read_csv("data/newdata_pred_glmm.csv")
newdata$param <- newdata$test_param
newdata$param <- as.factor(newdata$test_param)
levels(newdata$param) <-  c(1,3,4,2,6,5) 
newdata$param <- as.integer(as.character(newdata$param))
plotdata <- turk22 %>% mutate(type2 = ifelse(sign == 0, sign(initialEst), sign),
                       type2 = ifelse(type == "one", type2, -1)) %>% 
  group_by(pic_id, size, test_param, sign, type2,alt_model) %>% summarize(
    datapick = mean(datapick)
)
plotdata$param <- plotdata$test_param
plotdata$param <- as.factor(plotdata$param)
levels(plotdata$param) <- c(1,3,4,2,6,5) 
plotdata$param <- as.integer(as.character(plotdata$param))
labdat <- data_frame(test_param = "jttp", param = 3, type2 = -1, x = -5, y = .9, label = "outlier")
betahat <- tibble(param = 1:6, size = c(-4.903, 4.893, -3.45, 3.34, 10.091, 1.329))
load("data/finalglmm31-2.RDA")
mod <- model2randomscalesize
res <- summary(mod)[[10]]
ests <- as.numeric(round(res[,1], 2))
oddsidx <- which(exp(ests) < .0001)
oddsidx2 <- which(exp(ests) > 10000)
odds <- ifelse(exp(ests) > 1, round(exp(ests), 2), round(exp(ests), 3))
odds[oddsidx] <- "$<10^{-4}$"
odds[oddsidx2] <- "$>10^4$"
se <- as.numeric(round(res[,2],2))
pvalidx <- which(res[,4] < .0001)
pval <- sprintf("%.4f",round(res[,4],3))
pval[pvalidx] <- "$<10^{-4}$"
sigma_epsilon <- summary(model2randomscalesize)[[13]]$nick_name[[1]]
sigma_delta <- summary(model2randomscalesize)[[13]]$pic_id[[1]]
@


<<glmres_predict, eval = FALSE>>=
predict(model2randomscalesize, type = "link")
predict(model2randomscalesize, type = "response")

str(model2randomscalesize)

 @


\npdecimalsign{.}
%\nprounddigits{3}

\begin{table}
\centering
\begin{tabular}{cn{5}{2}n{5}{3}rr}\hline
Parameter & \multicolumn{1}{r}{Estimate} & \multicolumn{1}{r}{Std Error} & $p$-value & Odds Multiplier \\ \hline\hline
$\eta_{1+}$ & \Sexpr{ests[7]} & \Sexpr{se[7]} & \Sexpr{pval[7]}$^{\ddagger}$ & \Sexpr{odds[7]} \\
$\eta_{1-}$ & \Sexpr{ests[1]} & \Sexpr{se[1]} & \Sexpr{pval[1]}$^{\dagger}$ & \Sexpr{odds[1]}\\
$\gamma_{1+}$ & \Sexpr{ests[19]} & \Sexpr{se[19]} & \Sexpr{pval[19]}$^{\ddagger}$ & \Sexpr{odds[19]} \\
$\gamma_{1-}$  & \Sexpr{ests[13]} & \Sexpr{se[13]} & \Sexpr{pval[13]}$^{\dagger}$ & \Sexpr{odds[13]} \\ \hline
$\eta_{2+}$  & \Sexpr{ests[10]} & \Sexpr{se[10]} & \Sexpr{pval[10]} & \Sexpr{odds[10]} \\
$\eta_{2-}$  & \Sexpr{ests[4]} & \Sexpr{se[4]} & \Sexpr{pval[4]}$^{\ddagger}$ & \Sexpr{odds[4]} \\
$\gamma_{2+}$ & \Sexpr{ests[22]} & \Sexpr{se[22]} & \Sexpr{pval[22]}$^{*}$ & \Sexpr{odds[22]} \\
$\gamma_{2-}$  & \Sexpr{ests[16]} & \Sexpr{se[16]} & \Sexpr{pval[16]}$^{\ddagger}$ & \Sexpr{odds[16]} \\ \hline
$\eta_{3+}$  & \Sexpr{ests[8]} & \Sexpr{se[8]} & \Sexpr{pval[8]}$^{\dagger}$ & \Sexpr{odds[8]} \\ 
$\eta_{3-}$ & \Sexpr{ests[2]} & \Sexpr{se[2]} & \Sexpr{pval[2]}$^{\dagger}$ & \Sexpr{odds[2]} \\
$\gamma_{3+}$ & \Sexpr{ests[20]} & \Sexpr{se[20]} & \Sexpr{pval[20]}$^{\dagger}$ & \Sexpr{odds[20]}  \\
$\gamma_{3-}$   & \Sexpr{ests[14]} & \Sexpr{se[14]} & \Sexpr{pval[14]}$^{*}$ & \Sexpr{odds[14]} \\ \hline
$\eta_{4+}$ & \Sexpr{ests[9]} & \Sexpr{se[9]} & \Sexpr{pval[9]}$^{**}$ & \Sexpr{odds[9]} \\
$\eta_{4-}$ & \Sexpr{ests[3]} & \Sexpr{se[3]} & \Sexpr{pval[3]}$^{**}$ & \Sexpr{odds[3]}  \\
$\gamma_{4+}$ & \Sexpr{ests[21]} & \Sexpr{se[21]} & \Sexpr{pval[21]}$^{**}$ & \Sexpr{odds[21]} \\ 
$\gamma_{4-}$  & \Sexpr{ests[15]} & \Sexpr{se[15]} & \Sexpr{pval[15]} & \Sexpr{odds[15]} \\ \hline
$\eta_{5+}$  & \Sexpr{ests[12]} & \Sexpr{se[12]} & \Sexpr{pval[12]}$^{*}$ & \Sexpr{odds[12]} \\
$\eta_{5-}$  & \Sexpr{ests[6]} & \Sexpr{se[6]} & \Sexpr{pval[6]}$^{\ddagger}$ & \Sexpr{odds[6]} \\
$\gamma_{5+}$ & \Sexpr{ests[24]} & \Sexpr{se[24]} & \Sexpr{pval[24]}$^{*}$ & \Sexpr{odds[24]} \\ 
$\gamma_{5-}$ & \Sexpr{ests[18]} & \Sexpr{se[18]} & \Sexpr{pval[18]}$^{\ddagger}$ & \Sexpr{odds[18]} \\ \hline
$\eta_{6+}$  & \Sexpr{ests[11]} & \Sexpr{se[11]} & \Sexpr{pval[11]} & \Sexpr{odds[11]} \\
$\eta_{6-}$ & \Sexpr{ests[5]} & \Sexpr{se[5]} & \Sexpr{pval[5]}$^{\ddagger}$ & \Sexpr{odds[5]}  \\
$\gamma_{6+}$ & \Sexpr{ests[23]} & \Sexpr{se[23]} & \Sexpr{pval[23]} & \Sexpr{odds[23]} \\
$\gamma_{6-}$ & \Sexpr{ests[17]} & \Sexpr{se[17]} & \Sexpr{pval[17]}$^{\ddagger}$ & \Sexpr{odds[17]} \\ \hline
$\sigma^2_{\delta}$ & \Sexpr{round(sigma_delta, 4)} & \multicolumn{1}{r}{--} & -- & --\\
$\sigma^2_{\epsilon}$ &  \Sexpr{round(sigma_epsilon, 4)}  & \multicolumn{1}{r}{--} & -- & --\\
\hline 
\end{tabular}
\caption{\label{tab:glmmests}Summary of the results from fitting the model given in Equation~\ref{eq:glmm}. Significance levels: * - $< 0.10$; ** - $<0.05$; $\dagger$ - $<0.01$; $\ddagger$ - $<0.001$}
\end{table}

<<morepredicts>>=
alphas <- (coef(mod)[[1]][,-1] %>% unique())[1:12] 
betas <- (coef(mod)[[1]][,-1] %>% unique())[13:24] 
names(betas) <- str_replace(str_replace(names(betas), "lps_param", ""), ":centersize", "")
names(betas) <- str_replace(names(betas), ".-1", "neg")
names(betas) <- str_replace(names(betas), ".1", "pos")
names(alphas) <- str_replace(names(alphas), "lps_param", "")
names(alphas) <- str_replace(names(alphas), ".-1", "neg")
names(alphas) <- str_replace(names(alphas), ".1", "pos")
source("code/newpredictions.R")
newdata <- newdata3
newdata$param <- newdata$test_param
newdata$param <- as.factor(newdata$test_param)
#levels(newdata$param) <-  c(1,3,4,2,6,5) 
#newdata$param <- as.integer(as.character(newdata$param))
plotdata <- turk22 %>% mutate(type2 = ifelse(sign == 0, sign(initialEst), sign),
                       type2 = ifelse(type == "one", type2, -1)) %>% 
  group_by(pic_id, size, test_param, sign, type2,alt_model) %>% summarize(
    datapick = mean(datapick)
)
plotdata$param <- plotdata$test_param
plotdata$param <- as.factor(plotdata$param)
#levels(plotdata$param) <- c(1,3,4,2,6,5) 
#plotdata$param <- as.integer(as.character(plotdata$param))
labdat <- data_frame(test_param = "jttp", param = 3, type2 = -1, x = -5, y = .9, label = "jttp_neg_hard_2")
@

<<recipzoom, fig.height=3, fig.cap='The top middle panel of Figure~\\ref{fig:predictglmm} expanded to show greater detail. The square root of the parameter value is shown on the $x$-axis. For this parameter, as its value approaches zero, the probability of identifying the alternate data model decreases, then increases, which is noticeably different from the pattern exhibited by the others. Again, a horizontal line is drawn at 1/6, the chance of selecting the data plot at random.'>>=
library(dplyr)
newdatabeta2 <- newdata %>% filter(test_param == "recip", category == "inside") %>% mutate(param2 = "beta[2]", size2 = sqrt(size)) %>% as.tibble()
plotdatabeta2 <- plotdata %>% filter(test_param=="recip")%>% mutate(param2 = "beta[2]", size2 = sqrt(size)) %>% as.tibble()
newdatabeta22 <-  newdata %>% filter(test_param == "recip", category == "outside")%>% mutate(param2 = "beta[2]", size2 = sqrt(size)) %>% as.tibble()
newdatabeta23 <- newdata %>% filter(test_param == "recip", category == "outside2")%>% mutate(param2 = "beta[2]", size2 = sqrt(size)) %>% filter(!is.nan(size2)) %>% as.tibble()
interceptlabel <- tibble(x = sqrt(4.893), y = .05, label = "hat(beta)[2]")
ggplot() + 
  geom_line(data = newdatabeta2, aes(x = size2, y = predictfinal, color = as.factor(type2)), size = 1.25) + 
  geom_point(data = plotdatabeta2, aes(x = size2, y = datapick, color = as.factor(type2)), size = 2) + 
  geom_line(data =newdatabeta22, aes(x = size2, y = predictfinal, group = as.factor(type2)), color = "gray60", alpha = .7) +
  geom_line(data = newdatabeta23, aes(x = size2, y = predictfinal, group = as.factor(type2)), color = "gray60", alpha = .7) +
  #geom_text(data = plotdata, aes(x = size, y = datapick, color = as.factor(type2), label = pic_id)) +
  #geom_text(data = labdat, aes(x = x, y=y, label = label, color = as.factor(type2)),
  #          show.legend = F) + 
  geom_vline(xintercept = sqrt(4.893),  linetype = "dotted", color = "grey60") + 
  geom_text(data = interceptlabel, aes(x =x, y = y, label = label), parse = T, inherit.aes = F, hjust = -.2) + 
  geom_hline(yintercept = 1/6, linetype='dotted') +
  scale_color_brewer(palette = "Dark2", name = "Lineup Type", labels = c("-", "+")) +  
  facet_wrap(~param2, labeller = label_parsed) + 
  labs(x = "Square root of parameter value", y = "Expected probability of detection\nin a lineup of size 6", title = ) + 
  ThemeNoNet + 
  theme(legend.position = 'bottom')
@

<<beta5zoom, fig.height=3, fig.cap="The bottom middle panel of Figure~\\ref{fig:predictglmm} expanded to show greater detail. The parameter value is shown on the $x$-axis. This parameter most closely follows our hypothesis shown in Figure~\\ref{fig:hypothesis}. However, the result is not symmetric. According to the model, people will detect the effect at lower values and with greater frequency as the value increases when it is positive instead of negative.">>=
newdatabeta5 <- newdata %>% filter(test_param == "simttb", category == "inside") %>% mutate(param = "beta[5]")
plotdatabeta5 <- plotdata %>% filter(test_param=="simttb")%>% mutate(param = "beta[5]")
newdatabeta52 <- newdata %>% filter(test_param == "simttb", category == "outside") %>% mutate(param = "beta[5]")
newdatabeta53 <- newdata %>% filter(test_param == "simttb", category == "outside2") %>% mutate(param = "beta[5]")
interceptdata <- tibble(x = 10.090829, y= .05, label = "hat(beta)[5]")
ggplot() + 
  geom_line(data = newdatabeta5, aes(x = size, y = predictfinal, color = as.factor(type2)), size = 1.25) + 
  geom_point(data = plotdatabeta5, aes(x = size, y = datapick, color = as.factor(type2)), size = 2) + 
  geom_line(data = newdatabeta52, aes(x = size, y = predictfinal, group = as.factor(type2)), color = "gray60", alpha = .7) +
  geom_line(data = newdatabeta53, aes(x = size, y = predictfinal, group = as.factor(type2)), color = "gray60",  alpha = .7) +
  #geom_text(data = plotdata, aes(x = size, y = datapick, color = as.factor(type2), label = pic_id)) +
  #geom_text(data = labdat, aes(x = x, y=y, label = label, color = as.factor(type2)),
  #          show.legend = F) + 
  geom_vline(xintercept = 10.090829, linetype = "dotted", color = "grey60") + 
  geom_text(data = interceptdata, aes(x=x,y=y,label=label), parse = T, hjust = -.2) + 
  geom_hline(yintercept = 1/6, linetype='dotted') +
  scale_color_brewer(palette = "Dark2", name = "Lineup Type", labels = c("-", "+")) +
  facet_grid(~param, labeller = label_parsed) + 
  labs(x = "Parameter value", y = "Expected probability of detection\nin a lineup of size 6") + 
  ThemeNoNet + 
  theme(legend.position = 'bottom', strip.text = element_text(size = 10), text = element_text(size = rel(3)), legend.text = element_text(size = rel(3)), legend.key.size = unit(.25, "inches"))
@

<<beta4zoom, fig.height=3, fig.cap='The bottom right panel of Figure~\\ref{fig:predictglmm} expanded to show greater detail. The parameter value is shown on the $x$-axis. The ``reverse" lineup has a much flatter slope than the ``regular" lineup, which means the participants had a harder time detecting a more simple M1 structure among many more complex M4 structures. Reversing the lineup scenario was not symmetric as we hypothesized.'>>=
newdatabeta6 <- newdata %>% filter(test_param == "jtts", category == "inside") %>% mutate(param = "beta[6]")
plotdatabeta6 <- plotdata %>% filter(test_param=="jtts") %>% mutate(param = "beta[6]")
newdatabeta62 <- newdata %>% filter(test_param == "jtts", category == "outside")%>% mutate(param = "beta[6]")
newdatabeta63 <- newdata %>% filter(test_param == "jtts", category == "outside2") %>% mutate(param = "beta[6]")
labellerdata <- tibble(x = 3.340302, y = .8, label = "hat(beta)[6]")
ggplot() + 
  geom_vline(xintercept = 3.340302, linetype = "dotted", color = "grey60") + 
  geom_text(data = labellerdata, aes(x = x, y = y, label = label), parse =T, hjust = 1) + 
  geom_line(data = newdatabeta6, aes(x = size, y = predictfinal, color = as.factor(type2)), size = 1.25) + 
  geom_point(data = plotdatabeta6, aes(x = size, y = datapick, color = as.factor(type2)), size = 2) + 
  geom_line(data = newdatabeta62, aes(x = size, y = predictfinal, group = as.factor(type2)), color = "gray60", alpha = .7) +
  geom_line(data = newdatabeta63, aes(x = size, y = predictfinal, group = as.factor(type2)), color = "gray60", alpha = .7) +
  #geom_text(data = plotdata, aes(x = size, y = datapick, color = as.factor(type2), label = pic_id)) +
  #geom_text(data = labdat, aes(x = x, y=y, label = label, color = as.factor(type2)),
  #          show.legend = F) + 
  geom_hline(yintercept = 1/6, linetype='dotted') +
  scale_color_brewer(palette = "Dark2", name = "Lineup Condition", labels = c("reverse", "regular")) +
  facet_grid(~param, labeller = label_parsed) + 
  labs(x = "Parameter value", y = "Expected probability of detection\nin a lineup of size 6") + 
  ThemeNoNet + 
  theme(legend.position = 'bottom', strip.text = element_text(size = 10), text = element_text(size = rel(3)), legend.text = element_text(size = rel(3)), legend.key.size = unit(.25, "inches"))
@

We expand portions of Figure~\ref{fig:predictglmm} in Figures~\ref{fig:recipzoom}-\ref{fig:beta4zoom}. These figures show the same prediction regions as in Figure~\ref{fig:predictglmm}, plus some additional predictions outside of the data range shown in light gray. Again, the points represent the results from the experiment. In all three of these figures, the lack of symmetry is apparent. In the reverse lineup scenario shown in Figure~\ref{fig:beta4zoom}, the probability of prediction is consistently far less than the probability of prediction in the regular lineup scenario. This demonstrates that the visual signal of one plot from M4 among five plots from M1 is much stronger than that of one plot from M1 among five plots from M4. We posit that the latter is a more difficult task because it involves noticing a \textit{lack of structure} as opposed to the presence of \textit{more} structure. We can see a similar effect in Figure~\ref{fig:beta5zoom}. At a value of $\beta_5 = 20$, the model predicts a probability of about 0.60 that a new viewer of a new lineup will identify the alternative data plot. At a value of $\beta_5 = -20$, however, the model predicts this same probability to be about 0.40. This again demonstrates that the presence of structure is detected  more frequently and at smaller values than the absence of structure. 

%%%%%%%%%%%%%%%%%%%%%% References %%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{asa}
\bibliography{bibliography}

\end{document}